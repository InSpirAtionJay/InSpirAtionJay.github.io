<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[IMU预积分]]></title>
    <url>%2F2019%2F07%2F01%2FIMU%E9%A2%84%E7%A7%AF%E5%88%86%2F</url>
    <content type="text"></content>
      <categories>
        <category>IMU</category>
      </categories>
      <tags>
        <tag>Camera</tag>
        <tag>IMU</tag>
        <tag>Preintegration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[旋转向量和李代数]]></title>
    <url>%2F2019%2F06%2F25%2F%E6%97%8B%E8%BD%AC%E5%90%91%E9%87%8F%E5%92%8CIMU%2F</url>
    <content type="text"><![CDATA[1. 向量和坐标系如果我们确定一个坐标系$\vec{w_1}=\begin{bmatrix} e_1 \ e_2 \ e_3\end{bmatrix}$，也就是一个线性空间的基$e_1,e_2,e_3$那就可以谈论向量$a$在这组基下的坐标了，假设向量$a$在这个坐标系的坐标为$x_1 =\begin{bmatrix} a_1 \ a_2 \ a_3\end{bmatrix} $: \vec{a} = \begin{bmatrix} e_1 & e_2 & e_3\end{bmatrix} \begin{bmatrix}a_1\\ a_2 \\ a_3\end{bmatrix} = a_1e_1+a_2e_2+a_3e_3对于另一个坐标系$\vec{w_2}=\begin{bmatrix} e_1’ \ e_2’ \ e_3’\end{bmatrix}$，坐标为$x_2 =\begin{bmatrix} a_1’ \ a_2’ \ a_3’\end{bmatrix} $也有 \vec{a} = \begin{bmatrix} e_1' & e_2' & e_3'\end{bmatrix} \begin{bmatrix}b_1\\ b_2 \\ b_3\end{bmatrix} = a_1'e_1'+b_2e_2'+b_3e_3'于是有原点相同的两个参考系$\vec{w_1}$和$\vec{w_2}$ \vec{a} = \vec{w_1}x_1 = \vec{w_2}x_2继续推导可以得到 x_2 = R_{21}x_1这就定义了$R_{21} = \vec{w_2}\vec{w_1}^T$我们称矩阵$R_{21}$为旋转矩阵，有时也称之为方向余弦矩阵，因为两个单位向量的点积结果就是他们之间夹角的余弦。于是两个坐标系中的单位向量可以关联起来 \vec{w_1} = \vec{w_2}R_{21}旋转矩阵的性质： x_1 = R_{21}^{-1}x_2 = R_{12}x_2于是有： R_{21} = R_{12}^{-1} = R_{12}^T2. 向量的内积和外积设向量$\vec {a} = \begin{bmatrix} a_1 \ a_2 \ a_3\end{bmatrix}$，向量$\vec {b} = \begin{bmatrix} b_1 \ b_2 \ b_3\end{bmatrix}$ 内积： \vec{a}\vec{b} = \vec{a}^T\vec{b} = \sum_{i=1}^3{a_ib_i}=|\vec{a}||\vec{b}|\cos\langle a,b\rangle 外积： \vec{a} \times \vec{b} = \begin{bmatrix} i & j & k \\ a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \end{bmatrix}= \begin{bmatrix} a_2b_3 - a_3b_2 \\ a_3b_1 - a_1b_3 \\ a_1b_2 - a_2b_1 \end{bmatrix}= \begin{bmatrix} 0 & -a_3 &a_2 \\ a_3 & 0 & -a_1 \\ a_2& a_1 & 0 \end{bmatrix}\vec{b} =[\vec{a}]_x\vec{b} hat运算： [\vec{a}]\hat{}=\begin{bmatrix} a_1 \\ a_2 \\ a_3\end{bmatrix}\hat{} = \begin{bmatrix} 0 & -a_3 &a_2 \\ a_3 & 0 & -a_1 \\ a_2& a_1 & 0 \end{bmatrix} -a\hat{}b=-b\hat{}a3. 基本旋转矩阵与欧拉角绕坐标系$z$轴旋转$\theta_3$角度的矩阵： R_3 = \begin{bmatrix} \cos\theta_3 & \sin\theta_3 & 0 \\ -\sin\theta_3 & \cos\theta_3 & 0 \\ 0 & 0 & 1 \end{bmatrix}绕坐标系$y$轴旋转$\theta_2$角度的矩阵： R_2 = \begin{bmatrix} \cos\theta_2 & 0 & -\sin\theta_2 \\ 0 & 1 & 0 \\ \sin\theta_2 & 0 & \cos\theta_2 \\ \end{bmatrix}绕坐标系$x$轴旋转$\theta_1$角度的矩阵： R_1 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \cos\theta_1 & \sin\theta_1 \\ 0 & -\sin\theta_1 & \cos\theta_1 \end{bmatrix}RPY(翻滚角-俯仰角-偏航角)表示旋转 沿着原始坐标系的主轴1（x轴）旋转$\theta_1$d度（翻滚角）； 沿着原始坐标系的主轴2（y轴）旋转$\theta_2$d度（俯仰角）； 沿着原始坐标系的主轴3（z轴）旋转$\theta_3$d度（偏航角）；这种情况下从参考系1到参考系2的旋转矩阵为： R_{21}(\theta_3,\theta_2,\theta_1) = R_3(\theta_3)R_2(\theta_2)R_1(\theta_1) = \begin{bmatrix} c_2c_3 & c_1s_3+s_1s_2c_3 & s_1s_3-c_1s_2c_3 \\ -c_2s_3 & c_1c_3 -s_1s_2s_3 & s_1c_3 + c_1s_2s_3 \\ s_2 & -s_1c_2 &c_1c_2 \end{bmatrix} 4. SO(n) 特殊正交群 与 SE(n) 特殊欧式群旋转矩阵是一个行列式为1的正交矩阵，反之，行列式为1的正交矩阵也是一个旋转矩阵。所以可以把旋转矩阵的集合定义如下： SO(3) = \{R \in \Bbb{R}^{n \times n}| RR^T=I,\det(R)=1\} R^{-1}=R^T在欧氏变换中,除了旋转之外还有一个平移。关于变换矩阵$T$，它具有比较特别的结构：左上角为旋转矩阵，右侧为平移向量，左下角为0向量，右下角为1。这种矩阵又称为特殊欧氏群： SE(3) = \{T=\begin{bmatrix}R &t \\ 0^T & 1\end{bmatrix} \in \Bbb {R}^{4 \times 4}| R \in SO(3),t \in \Bbb {R}^3\} T^{-1} = \begin{bmatrix}R^T & -R^Tt \\ 0^T & 1\end{bmatrix}5. 旋转向量对于坐标系的旋转，我们知道，任意旋转都可以用一个旋转轴和一个旋转角来刻画。于是，我们可以使用一个向量，其方向与旋转轴一致，而长度等于旋转角。这种向量，称为旋转向量(或轴角，Axis-Angle)。这种表示法只需一个三维向量即可描述旋转。同样，对于变换矩阵，我们使用一个旋转向量和一个平移向量即可表达一次变换。这时的维数正好是六维。假设有一个旋转轴为$\alpha$，角度为$\theta$的旋转，显然，它对应的旋转向量为$\theta \alpha$。由旋转向量到旋转矩阵的过程由罗德里格斯公式表明： R = \cos \theta I + (1-\cos\theta)\alpha\alpha^T + \sin \theta \alpha\hat{}.符号$\hat{}$是向量到反对称的转换符。反之，我们也可以计算从一个旋转矩阵到旋转向量的转换。对于转角$\theta$，有： tr(R) = \cos\theta tr(I) + (1-\cos \theta)tr(\alpha\alpha^T)+\sin\theta tr(\alpha\hat{})=3\cos\theta + (1-\cos\theta)= 1+2cos\theta因此，有 \theta = \arccos (\frac{tr(R)-1}{2})关于转轴$\alpha$，由于旋转轴上的向量在旋转后不发生改变，说明 R\alpha=\alpha.因此，转轴$\alpha$是矩阵$R$特征值1对应的特征向量。求解此方程，再归一化，就得到了旋转轴。 6. 李代数$SO(3)$对应的李代数$so(3)$是定义在$\Bbb {R}^3$的向量，我们记作$\phi$。根据前面的推导，每个$\phi$都可以生成一个反对称矩阵: \Phi = \phi\hat{}= \begin{bmatrix} 0 & -\phi_3 &\phi_2 \\ \phi_3 & 0 & -\phi_1 \\ -\phi_2 & -\phi_1 & 0 \end{bmatrix} \in \Bbb {R}^{3 \times 3}由于$\phi$与反对称矩阵关系很紧密,在不引起歧义的情况下,就说$so(3)$的元素是3维向量或者3维反对称矩阵,不加区别: so(3) = \{\phi \in \Bbb {R}^3,\Phi = \phi\hat{} \in \Bbb {R}^{3\times3}\}至此，我们已清楚了$so(3)$的内容。它们是一个由三维向量组成的集合，每个向量对应到一个反对称矩阵，可以表达旋转矩阵的导数。它与$SO(0)$的关系由指数映射给定: R = \exp(\phi\hat{})对于$SE(3)$，它也有对应的李代数$se(3)$，$se(3)$位于$\Bbb {R}^6$空间中： se(3)=\{ \xi = \begin{bmatrix} \rho \\ \phi \end{bmatrix} \in \Bbb {R}^3, \phi \in so(3), \xi\hat{} = \begin{bmatrix} \phi\hat{} & \rho \\ 0^T & 0 \end{bmatrix} \in \Bbb {R}^{4 \times4} \}我们把每个$se(3)$元素记作$\xi$，它是一个六维向量。前三维为平移，记作$\rho$；后三维为旋转，记作$\phi$，实质上是$so(3)$元素。同时，我们拓展了$\hat{}$符号的含义。在$se(3)$中，同样使用符号$\hat{}$将一个六维向量转换成四维矩阵，但这里不再表示反对称： \xi\hat{} = \begin{bmatrix} \phi\hat{} & \rho \\ 0^T & 0 \end{bmatrix} \in \Bbb {R}^{4\times4}7. 指数与对数映射任意矩阵的指数映射可以写成一个泰勒展开，但是只有在收敛的情况下才会有结果，其结果仍是一个矩阵。 \exp(A) = \sum_{n=0}^{\infty}{\frac{1}{n!}A^n}同样地,对$SO(3)$中任意一元素$\phi$，我们亦可按此方式定义它的指数映射: \exp(\phi\hat{}) = \sum_{n=0}^{\infty}{\frac{1}{n!}(\phi\hat{})^n}我们来仔细推导一下这个定义。由于$\phi$是三维向量，我们可以定义它的模长和它的方向，分别记作$\theta$和$\alpha$,于是有$\phi = \theta\alpha$。这里$\alpha$是一个长度为1的方向向量。首先，对于$a\hat{}$，有以下两条性质: a\hat{}a\hat{} = aa^T-I,a\hat{}a\hat{}a\hat{} = -a\hat{}于是，指数映射有：\begin{align}\exp(\phi\hat{}) &amp;= \exp(\theta\alpha\hat{})=\sum_{n=0}^{\infty}{\frac{1}{n!}(\theta\alpha\hat{})^n} \\&amp;=I + \theta\alpha\hat{} +\frac{1}{2!}\theta^2\alpha\hat{}\alpha\hat{}+\frac{1}{3!}\theta^3\alpha\hat{}\alpha\hat{}\alpha\hat{}+\frac{1}{4!}\theta^4(\alpha\hat{})^4 +\cdots\\&amp;=\alpha\alpha^T-\alpha\hat{}\alpha\hat{}+\theta\alpha\hat{}+\frac{1}{2!}\theta^2\alpha\hat{}\alpha\hat{}-\frac{1}{3!}\theta^3\alpha\hat{}-\frac{1}{4!}\theta^4(\alpha\hat{})^2+\cdots \\&amp;= \alpha\alpha^T + (\theta -\frac{1}{3!}\theta^3+\frac{1}{5!}\theta^5-\cdots)\alpha\hat{} - (1-\frac{1}{2!}\theta^2+\frac{1}{4!}\theta^4-\cdots)\alpha\hat{}\alpha\hat{} \\&amp;=\alpha\hat{}\alpha\hat{} + I +\sin\theta\alpha\hat{} -\cos\theta\alpha\hat{}\alpha\hat{} \\&amp;=(1-\cos\theta)\alpha\hat{}\alpha\hat{} + I + \sin\theta\alpha\hat{} \\&amp;=\cos\theta I +(1-\cos\theta)\alpha\alpha^T+\sin\theta\alpha\hat{}\end{align}最后，得到罗德里格斯公式： \exp(\phi\hat{}) =\cos\theta I +(1-\cos\theta)\alpha\alpha^T+\sin\theta\alpha\hat{}通过$\phi = \theta\alpha$转化为原旋转矩阵有： \exp(\phi\hat{}) = I + \frac{\sin(\Vert\phi\Vert)}{\Vert\phi\Vert}\phi\hat{}+\frac{1-\cos(\Vert\phi\Vert)}{\Vert\phi\Vert}(\phi\hat{})^2如果$\phi$比较小，有一阶近似如下： exp(\phi\hat{})\approx I + \phi\hat{}则有对数映射如下： \theta =\arccos (\frac{tr(R)-1}{2}) \phi = log(R) = \frac {\theta (R-R^T)}{2\sin(\theta)} R\alpha = \alphaSE(3)上的指数映射 \exp(\xi\hat{})= \begin{bmatrix} \sum_{n=0}^{\infty}{\frac{1}{n!}(\phi\hat{})^n} & \sum_{n=0}^{\infty}{\frac{1}{(n+1)!}(\phi\hat{})^n\rho} \\ 0^T & 1 \end{bmatrix} = \begin{bmatrix} R & J\rho \\ 0^T & 1 \end{bmatrix} = T J =J_l = \frac{\sin\theta}{\theta}I+(1-\frac{\sin\theta}{\theta})\alpha\alpha^T+\frac{1-\cos\theta}{\theta}\alpha\hat{}平移向量t，可以用下式推导： t = J\rho 8. Baker-Campbell-Hausdorff 近似公式 \log(\exp(\phi_1\hat{}\exp(\phi_2\hat{}) \approx \begin{cases} J_l(\phi_2)^{-1}\phi_1+\phi_2 & \text{if $\phi_1$ is small} \\[2ex] J_r(\phi_1)^{-1}\phi_2+\phi_1 & \text{if $\phi_2$ is small} \end{cases} J =J_l = \frac{\sin\theta}{\theta}I+(1-\frac{\sin\theta}{\theta})\alpha\alpha^T+\frac{1-\cos\theta}{\theta}\alpha\hat{}它的逆为： J_l^{-1} = \frac{\theta}{2} \cot(\frac{\theta}{2}) I + ( 1 -\frac{\theta}{2} \cot(\frac{\theta}{2}) ) \alpha\alpha^T - \frac{\theta}{2}\alpha\hat{}而右乘雅可比仅需要对自变量取负号即可： J_r^{\phi} = J_l(-\phi)加法近似： \exp((\phi+\delta\phi)\hat{}) = \exp((J_l\delta\phi)\hat{})\exp(\phi\hat{})=\exp(\phi\hat{})\exp((J_r\delta\phi)\hat{})对于原$\phi$来说，Jacobian 式如下： J_r(\phi) = I - \frac{1-\cos(\Vert\phi\Vert)}{\Vert \phi \Vert^2} \phi\hat{} + \frac{\Vert\phi\Vert-\sin(\Vert\phi\Vert)}{\Vert \phi ^3 \Vert}(\phi\hat{})^2 J_r(\phi)^{-1} = I + \frac{1}{2}\phi\hat{}+ \left(\frac{1}{\Vert \phi \Vert^2} + \frac{1 + \cos(\Vert\phi\Vert)}{2\Vert \phi \Vert \sin(\Vert\phi\Vert)}\right)(\phi\hat{})^29. adjoint property R\exp(\phi)R^T = \exp(R\phi\hat{}R^T) = \exp{R\phi} \exp(\phi)R = R\exp(R^T\phi)]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析十一]]></title>
    <url>%2F2019%2F06%2F19%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%E4%B8%80%2F</url>
    <content type="text"><![CDATA[LocalMapping 类说明1. LocalMapping 成员变量1.1 単目Monocular标记，标记是否为単目地图跟踪1bool mbMonocular; 1.2 reset重置地图请求标记1bool mbResetRequested; 1.3 地图完成请求标记，以及完成标记12bool mbFinishRequested;bool mbFinished; 1.4 优化的地图指针1Map* mpMap; 1.5 回环检测指针以及跟踪指针12LoopClosing* mpLoopCloser;Tracking* mpTracker; 1.6 Tracking线程向LocalMapping中插入关键帧是先插入到该队列中,等待处理的关键帧列表12// Tracking线程向LocalMapping中插入关键帧是先插入到该队列中 std::list&lt;KeyFrame*&gt; mlNewKeyFrames; ///&lt; 等待处理的关键帧列表 1.7 当前处理的关键帧指针1KeyFrame* mpCurrentKeyFrame; 1.8 最近新加进来的地图点1std::list&lt;MapPoint*&gt; mlpRecentAddedMapPoints; 1.9 是否要BA标记1bool mbAbortBA; 1.10 进程终止标记、请求进程终止标记、进程不可终止标记123bool mbStopped;bool mbStopRequested;bool mbNotStop; 1.11 是否插入关键帧标记1bool mbAcceptKeyFrames; 2. LocalMapping 成员函数说明2.1 构造函数，通过传入的地图指针以及是否为単目地图跟踪来初始化LocalMapping,此时可接受关键帧并且认为上一次LocalMapping已经结束12345LocalMapping::LocalMapping(Map *pMap, const float bMonocular): mbMonocular(bMonocular), mbResetRequested(false), mbFinishRequested(false), mbFinished(true), mpMap(pMap), mbAbortBA(false), mbStopped(false), mbStopRequested(false), mbNotStop(false), mbAcceptKeyFrames(true)&#123;&#125; 2.2 设置LoopCloser以及Tracker指针123456789void LocalMapping::SetLoopCloser(LoopClosing* pLoopCloser)&#123; mpLoopCloser = pLoopCloser;&#125;void LocalMapping::SetTracker(Tracking *pTracker)&#123; mpTracker=pTracker;&#125; 2.3 将关键帧插入到列表中进行等待1234567891011121314/** * @brief 插入关键帧 * * 将关键帧插入到地图中，以便将来进行局部地图优化 * 这里仅仅是将关键帧插入到列表中进行等待 * @param pKF KeyFrame */void LocalMapping::InsertKeyFrame(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lock(mMutexNewKFs); // 将关键帧插入到列表中 mlNewKeyFrames.push_back(pKF); mbAbortBA=true;&#125; 2.4 查看列表中是否有等待被插入的关键帧123456789/** * @brief 查看列表中是否有等待被插入的关键帧 * @return 如果存在，返回true */bool LocalMapping::CheckNewKeyFrames()&#123; unique_lock&lt;mutex&gt; lock(mMutexNewKFs); return(!mlNewKeyFrames.empty());&#125; 2.5 处理列表中的关键帧,函数用于计算关键帧特征点的BoW映射，将关键帧插入地图。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * @brief 处理列表中的关键帧 * * - 计算Bow，加速三角化新的MapPoints * - 关联当前关键帧至MapPoints，并更新MapPoints的平均观测方向和观测距离范围 * - 插入关键帧，更新Covisibility图和Essential图 * @see VI-A keyframe insertion */void LocalMapping::ProcessNewKeyFrame()&#123; // 步骤1：从缓冲队列中取出一帧关键帧 // Tracking线程向LocalMapping中插入关键帧存在该队列中 &#123; unique_lock&lt;mutex&gt; lock(mMutexNewKFs); // 从列表中获得一个等待被插入的关键帧 mpCurrentKeyFrame = mlNewKeyFrames.front(); mlNewKeyFrames.pop_front(); &#125; // Compute Bags of Words structures // 步骤2：计算该关键帧特征点的Bow映射关系 mpCurrentKeyFrame-&gt;ComputeBoW(); // Associate MapPoints to the new keyframe and update normal and descriptor // 步骤3：跟踪局部地图过程中新匹配上的MapPoints和当前关键帧绑定 // 在TrackLocalMap函数中将局部地图中的MapPoints与当前帧进行了匹配， // 但没有对这些匹配上的MapPoints与当前帧进行关联 const vector&lt;MapPoint*&gt; vpMapPointMatches = mpCurrentKeyFrame-&gt;GetMapPointMatches(); for(size_t i=0; i&lt;vpMapPointMatches.size(); i++) &#123; MapPoint* pMP = vpMapPointMatches[i]; if(pMP) &#123; if(!pMP-&gt;isBad()) &#123; // 非当前帧生成的MapPoints // 为当前帧在tracking过程跟踪到的MapPoints更新属性 if(!pMP-&gt;IsInKeyFrame(mpCurrentKeyFrame)) &#123; // 添加观测 pMP-&gt;AddObservation(mpCurrentKeyFrame, i); // 获得该点的平均观测方向和观测距离范围 pMP-&gt;UpdateNormalAndDepth(); // 加入关键帧后，更新3d点的最佳描述子 pMP-&gt;ComputeDistinctiveDescriptors(); &#125; else // this can only happen for new stereo points inserted by the Tracking &#123; // 当前帧生成的MapPoints // 将双目或RGBD跟踪过程中新插入的MapPoints放入mlpRecentAddedMapPoints，等待检查 // CreateNewMapPoints函数中通过三角化也会生成MapPoints // 这些MapPoints都会经过MapPointCulling函数的检验 mlpRecentAddedMapPoints.push_back(pMP); &#125; &#125; &#125; &#125; // Update links in the Covisibility Graph // 步骤4：更新关键帧间的连接关系，Covisibility图和Essential图(tree)，UpdateConnections()是根据observations进行更新关键帧之间的联系 mpCurrentKeyFrame-&gt;UpdateConnections(); // Insert Keyframe in Map // 步骤5：将该关键帧插入到地图中 mpMap-&gt;AddKeyFrame(mpCurrentKeyFrame);&#125; 2.6 剔除ProcessNewKeyFrame函数中引入的质量不好的MapPoints设立观测阈值，単目为2，双目为3遍历所有待检测的点进行剔除，剔除点的规则： 1.该点已经是坏点的MapPoints直接从检查链表中删除（该点已经被删除） 2.跟踪到该MapPoint的Frame数（即3D点可以找到对应的特征点）相比预计可观测到该MapPoint的Frame数（Observations可以观测到的）的比例需大于25% // IncreaseFound / IncreaseVisible &lt; 25%，注意不一定是关键帧，设定badflag，并从列表中删除该点 3.从该点建立开始，到现在已经过了不小于2个关键帧，但是观测到该点的关键帧数却不超过cnThObs（単目=2）帧，那么该点检验不合格，设定badflag，并从列表中删除该点 从建立该点开始，已经过了3个关键帧而没有被剔除，则认为是质量高的点，因此没有SetBadFlag()，仅从队列中删除，放弃继续对该MapPoint的检测123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * @brief 剔除ProcessNewKeyFrame和CreateNewMapPoints函数中引入的质量不好的MapPoints * @see VI-B recent map points culling */void LocalMapping::MapPointCulling()&#123; // Check Recent Added MapPoints list&lt;MapPoint*&gt;::iterator lit = mlpRecentAddedMapPoints.begin(); const unsigned long int nCurrentKFid = mpCurrentKeyFrame-&gt;mnId; int nThObs; if(mbMonocular) nThObs = 2; else nThObs = 3; const int cnThObs = nThObs; // 遍历等待检查的MapPoints while(lit!=mlpRecentAddedMapPoints.end()) &#123; MapPoint* pMP = *lit; if(pMP-&gt;isBad()) &#123; // 步骤1：已经是坏点的MapPoints直接从检查链表中删除 lit = mlpRecentAddedMapPoints.erase(lit); &#125; else if(pMP-&gt;GetFoundRatio()&lt;0.25f) &#123; // 步骤2：将不满足VI-B条件的MapPoint剔除 // VI-B 条件1： // 跟踪到该MapPoint的Frame数相比预计可观测到该MapPoint的Frame数的比例需大于25% // IncreaseFound / IncreaseVisible &lt; 25%，注意不一定是关键帧。 pMP-&gt;SetBadFlag(); lit = mlpRecentAddedMapPoints.erase(lit); &#125; else if(((int)nCurrentKFid-(int)pMP-&gt;mnFirstKFid)&gt;=2 &amp;&amp; pMP-&gt;Observations()&lt;=cnThObs) &#123; // 步骤3：将不满足VI-B条件的MapPoint剔除 // VI-B 条件2：从该点建立开始，到现在已经过了不小于2个关键帧 // 但是观测到该点的关键帧数却不超过cnThObs帧，那么该点检验不合格 pMP-&gt;SetBadFlag(); lit = mlpRecentAddedMapPoints.erase(lit); &#125; else if(((int)nCurrentKFid-(int)pMP-&gt;mnFirstKFid)&gt;=3) // 步骤4：从建立该点开始，已经过了3个关键帧而没有被剔除，则认为是质量高的点 // 因此没有SetBadFlag()，仅从队列中删除，放弃继续对该MapPoint的检测 lit = mlpRecentAddedMapPoints.erase(lit); else lit++; &#125;&#125; 2.7 CreateNewMapPoints() 生成新的地图点 步骤一：在当前关键帧的共视关键帧中找到共视程度最高的nn帧相邻帧vpNeighKFs 123456// Retrieve neighbor keyframes in covisibility graphint nn = 10;if(mbMonocular) nn=20;// 步骤1：在当前关键帧的共视关键帧中找到共视程度最高的nn帧相邻帧vpNeighKFsconst vector&lt;KeyFrame*&gt; vpNeighKFs = mpCurrentKeyFrame-&gt;GetBestCovisibilityKeyFrames(nn); 步骤二：得到当前帧的位姿等相机信息，得到光心坐标，焦距等相机参数。 123456789101112131415161718cv::Mat Rcw1 = mpCurrentKeyFrame-&gt;GetRotation();cv::Mat Rwc1 = Rcw1.t();cv::Mat tcw1 = mpCurrentKeyFrame-&gt;GetTranslation();cv::Mat Tcw1(3,4,CV_32F);Rcw1.copyTo(Tcw1.colRange(0,3));tcw1.copyTo(Tcw1.col(3));// 得到当前关键帧在世界坐标系中的坐标cv::Mat Ow1 = mpCurrentKeyFrame-&gt;GetCameraCenter();const float &amp;fx1 = mpCurrentKeyFrame-&gt;fx;const float &amp;fy1 = mpCurrentKeyFrame-&gt;fy;const float &amp;cx1 = mpCurrentKeyFrame-&gt;cx;const float &amp;cy1 = mpCurrentKeyFrame-&gt;cy;const float &amp;invfx1 = mpCurrentKeyFrame-&gt;invfx;const float &amp;invfy1 = mpCurrentKeyFrame-&gt;invfy;const float ratioFactor = 1.5f*mpCurrentKeyFrame-&gt;mfScaleFactor; 步骤三：得到临近关键帧相机光心坐标，和当前帧的基线长度，然后和景深作比较，如果景深过深或者基线太短就放弃这一帧 123456789101112131415161718192021222324252627KeyFrame* pKF2 = vpNeighKFs[i];// Check first that baseline is not too short// 邻接的关键帧在世界坐标系中的坐标cv::Mat Ow2 = pKF2-&gt;GetCameraCenter();// 基线向量，两个关键帧间的相机位移cv::Mat vBaseline = Ow2-Ow1;// 基线长度const float baseline = cv::norm(vBaseline);// 步骤3：判断相机运动的基线是不是足够长if(!mbMonocular)&#123; // 如果是立体相机，关键帧间距太小时不生成3D点 if(baseline&lt;pKF2-&gt;mb) continue;&#125;else&#123; // 邻接关键帧的场景深度中值 const float medianDepthKF2 = pKF2-&gt;ComputeSceneMedianDepth(2); // baseline与景深的比例 const float ratioBaselineDepth = baseline/medianDepthKF2; // 如果特别远(比例特别小)，那么不考虑当前邻接的关键帧，不生成3D点 if(ratioBaselineDepth&lt;0.01) continue;&#125; 步骤四：根据两个关键帧的位姿计算它们之间的基本矩阵 123/ Compute Fundamental Matrix // 步骤4：根据两个关键帧的位姿计算它们之间的基本矩阵 cv::Mat F12 = ComputeF12(mpCurrentKeyFrame,pKF2); 步骤五：根据这两帧计算F矩阵，根据矩阵计算匹配点索引，存在vMatchedIndices里，通过极线约束限制匹配时的搜索范围，进行特征点匹配，vector]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析十]]></title>
    <url>%2F2019%2F06%2F14%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%8D%81%2F</url>
    <content type="text"><![CDATA[KeyFrame 类说明1234/* KeyFrame * 关键帧，和普通的Frame不一样，但是可以由Frame来构造 * 许多数据会被三个线程同时访问，所以用锁的地方很普遍 */ 1. KeyFrame 类成员变量说明1.1 关键帧id和下一个关键帧id,nNextId用于计数1234// nNextID名字改为nLastID更合适，表示上一个KeyFrame的ID号static long unsigned int nNextId;// 在nNextID的基础上加1就得到了mnID，为当前KeyFrame的ID号long unsigned int mnId; 1.2 mnFrameId记录了该KeyFrame是由哪个Frame初始化的123// 每个KeyFrame基本属性是它是一个Frame，KeyFrame初始化的时候需要Frame，// mnFrameId记录了该KeyFrame是由哪个Frame初始化的const long unsigned int mnFrameId; 1.3 该关键帧的时间戳1const double mTimeStamp; 1.4 网格信息，和Frame中定义的一样，用于加速特征匹配123456// Grid (to speed up feature matching)// 和Frame类中的定义相同const int mnGridCols;const int mnGridRows;const float mfGridElementWidthInv;const float mfGridElementHeightInv; 1.5 用于Tracking的变量123 // Variables used by the trackinglong unsigned int mnTrackReferenceForFrame;long unsigned int mnFuseTargetForKF; 1.6 用于local mapping的变量123// Variables used by the local mappinglong unsigned int mnBALocalForKF;long unsigned int mnBAFixedForKF; 1.7 用于关键帧数据库的变量1234567// Variables used by the keyframe databaselong unsigned int mnLoopQuery;int mnLoopWords;float mLoopScore;long unsigned int mnRelocQuery;int mnRelocWords;float mRelocScore; 1.8 用于回环检测的变量1234// Variables used by loop closing cv::Mat mTcwGBA; cv::Mat mTcwBefGBA; long unsigned int mnBAGlobalForKF; 1.9 相机参数12// Calibration parametersconst float fx, fy, cx, cy, invfx, invfy, mbf, mb, mThDepth; 1.10 特征点数目12// Number of KeyPointsconst int N; 1.11 Bow信息123//BoWDBoW2::BowVector mBowVec; ///&lt; Vector of words to represent imagesDBoW2::FeatureVector mFeatVec; ///&lt; Vector of nodes with indexes of local features 1.12 相对于父关键帧的位姿12// Pose relative to parent (this is computed when bad flag is activated) cv::Mat mTcp; 1.13 尺度信息1234567// Scaleconst int mnScaleLevels;const float mfScaleFactor;const float mfLogScaleFactor;const std::vector&lt;float&gt; mvScaleFactors;// 尺度因子，scale^n，scale=1.2，n为层数const std::vector&lt;float&gt; mvLevelSigma2;// 尺度因子的平方const std::vector&lt;float&gt; mvInvLevelSigma2; 1.14 图像边界1234const int mnMinX;const int mnMinY;const int mnMaxX;const int mnMaxY; 1.15 相机内参1const cv::Mat mK; 1.16 相机位姿12345// SE3 Pose and camera centercv::Mat Tcw;cv::Mat Twc;cv::Mat Ow;cv::Mat Cw; // Stereo middel point. Only for visualization 1.17 关键帧地图点12// MapPoints associated to keypoints std::vector&lt;MapPoint*&gt; mvpMapPoints; 1.18 Bow123// BoWKeyFrameDatabase* mpKeyFrameDB;ORBVocabulary* mpORBvocabulary; 1.19 网格划分12// Grid over the image to speed up feature matching std::vector&lt; std::vector &lt;std::vector&lt;size_t&gt; &gt; &gt; mGrid; 1.20 共视图信息1234// Covisibility Graphstd::map&lt;KeyFrame*,int&gt; mConnectedKeyFrameWeights; ///&lt; 与该关键帧连接的关键帧与权重std::vector&lt;KeyFrame*&gt; mvpOrderedConnectedKeyFrames; ///&lt; 排序后的关键帧std::vector&lt;int&gt; mvOrderedWeights; ///&lt; 排序后的权重(从大到小) 1.21 生成树信息123456// Spanning Tree and Loop Edges // std::set是集合，相比vector，进行插入数据这样的操作时会自动排序 bool mbFirstConnection; KeyFrame* mpParent; std::set&lt;KeyFrame*&gt; mspChildrens; std::set&lt;KeyFrame*&gt; mspLoopEdges; 1.22 Bad flags1234// Bad flagsbool mbNotErase;bool mbToBeErased;bool mbBad; 1.23 mHalfBaseline1float mHalfBaseline; // Only for visualization 1.24 map指针1Map* mpMap; 2. KeyFrame 成员函数说明2.1 构造函数传入图像帧，地图指针，以及关键帧数据库指针来建立关键帧，用传入的图像帧来赋值关键帧1KeyFrame(Frame &amp;F, Map *pMap, KeyFrameDatabase *pKFDB); 2.2 计算Bow12345678910111213141516/** * @brief Bag of Words Representation * * 计算mBowVec，并且将描述子分散在第4层上，即mFeatVec记录了属于第i个node的ni个描述子 * @see ProcessNewKeyFrame() */void KeyFrame::ComputeBoW()&#123; if(mBowVec.empty() || mFeatVec.empty()) &#123; vector&lt;cv::Mat&gt; vCurrentDesc = Converter::toDescriptorVector(mDescriptors); // Feature vector associate features with nodes in the 4th level (from leaves up) // We assume the vocabulary tree has 6 levels, change the 4 otherwise mpORBvocabulary-&gt;transform(vCurrentDesc,mBowVec,mFeatVec,4); &#125;&#125; 2.3 设置相机参数，计算$T_{cw},R_{cw},t_{cw},T_{wc},R_{wc},t_{wc}$12345678910111213141516171819void KeyFrame::SetPose(const cv::Mat &amp;Tcw_)&#123; unique_lock&lt;mutex&gt; lock(mMutexPose); Tcw_.copyTo(Tcw); cv::Mat Rcw = Tcw.rowRange(0,3).colRange(0,3); cv::Mat tcw = Tcw.rowRange(0,3).col(3); cv::Mat Rwc = Rcw.t(); Ow = -Rwc*tcw; Twc = cv::Mat::eye(4,4,Tcw.type()); Rwc.copyTo(Twc.rowRange(0,3).colRange(0,3)); Ow.copyTo(Twc.rowRange(0,3).col(3)); // center为相机坐标系（左目）下，立体相机中心的坐标 // 立体相机中心点坐标与左目相机坐标之间只是在x轴上相差mHalfBaseline, // 因此可以看出，立体相机中两个摄像头的连线为x轴，正方向为左目相机指向右目相机 cv::Mat center = (cv::Mat_&lt;float&gt;(4,1) &lt;&lt; mHalfBaseline, 0 , 0, 1); // 世界坐标系下，左目相机中心到立体相机中心的向量，方向由左目相机指向立体相机中心 Cw = Twc*center;&#125; 2.4 GetPose12345cv::Mat KeyFrame::GetPose()&#123; unique_lock&lt;mutex&gt; lock(mMutexPose); return Tcw.clone();&#125; 2.5 GetPoseInverse12345cv::Mat KeyFrame::GetPoseInverse()&#123; unique_lock&lt;mutex&gt; lock(mMutexPose); return Twc.clone();&#125; 2.6 GetCameraCenter12345cv::Mat KeyFrame::GetCameraCenter()&#123; unique_lock&lt;mutex&gt; lock(mMutexPose); return Ow.clone();&#125; 2.7 GetStereoCenter12345cv::Mat KeyFrame::GetStereoCenter()&#123; unique_lock&lt;mutex&gt; lock(mMutexPose); return Cw.clone();&#125; 2.8 GetRotation12345cv::Mat KeyFrame::GetRotation()&#123; unique_lock&lt;mutex&gt; lock(mMutexPose); return Tcw.rowRange(0,3).colRange(0,3).clone();&#125; 2.9 GetTranslation12345cv::Mat KeyFrame::GetTranslation()&#123; unique_lock&lt;mutex&gt; lock(mMutexPose); return Tcw.rowRange(0,3).col(3).clone();&#125; 2.10 为关键帧之间添加连接12345678910111213141516171819202122/** * @brief 为关键帧之间添加连接 * * 更新了mConnectedKeyFrameWeights * @param pKF 关键帧 * @param weight 权重，该关键帧与pKF共同观测到的3d点数量 */void KeyFrame::AddConnection(KeyFrame *pKF, const int &amp;weight)&#123; &#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); // std::map::count函数只可能返回0或1两种情况 if(!mConnectedKeyFrameWeights.count(pKF)) // count函数返回0，mConnectedKeyFrameWeights中没有pKF，之前没有连接 mConnectedKeyFrameWeights[pKF]=weight; else if(mConnectedKeyFrameWeights[pKF]!=weight) // 之前连接的权重不一样 mConnectedKeyFrameWeights[pKF]=weight; else return; &#125; UpdateBestCovisibles();&#125; 2.11 按权重对关键帧进行排序,从大到小关键帧放在mvpOrderedConnectedKeyFrames中，权重值存放在mvOrderedWeights中1234567891011121314151617181920212223242526272829/** * @brief 按照权重对连接的关键帧进行排序 * * 更新后的变量存储在mvpOrderedConnectedKeyFrames和mvOrderedWeights中 */void KeyFrame::UpdateBestCovisibles()&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); // http://stackoverflow.com/questions/3389648/difference-between-stdliststdpair-and-stdmap-in-c-stl vector&lt;pair&lt;int,KeyFrame*&gt; &gt; vPairs; vPairs.reserve(mConnectedKeyFrameWeights.size()); // 取出所有连接的关键帧，mConnectedKeyFrameWeights的类型为std::map&lt;KeyFrame*,int&gt;，而vPairs变量将共视的3D点数放在前面，利于排序 for(map&lt;KeyFrame*,int&gt;::iterator mit=mConnectedKeyFrameWeights.begin(), mend=mConnectedKeyFrameWeights.end(); mit!=mend; mit++) vPairs.push_back(make_pair(mit-&gt;second,mit-&gt;first)); // 按照权重进行排序 sort(vPairs.begin(),vPairs.end()); list&lt;KeyFrame*&gt; lKFs; // keyframe list&lt;int&gt; lWs; // weight for(size_t i=0, iend=vPairs.size(); i&lt;iend;i++) &#123; lKFs.push_front(vPairs[i].second); lWs.push_front(vPairs[i].first); &#125; // 权重从大到小 mvpOrderedConnectedKeyFrames = vector&lt;KeyFrame*&gt;(lKFs.begin(),lKFs.end()); mvOrderedWeights = vector&lt;int&gt;(lWs.begin(), lWs.end());&#125; 2.12 得到与该关键帧连接的关键帧，返回的是是关键帧set123456789101112/** * @brief 得到与该关键帧连接的关键帧 * @return 连接的关键帧 */set&lt;KeyFrame*&gt; KeyFrame::GetConnectedKeyFrames()&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); set&lt;KeyFrame*&gt; s; for(map&lt;KeyFrame*,int&gt;::iterator mit=mConnectedKeyFrameWeights.begin();mit!=mConnectedKeyFrameWeights.end();mit++) s.insert(mit-&gt;first); return s;&#125; 2.13 得到与该关键帧连接的关键帧(已按权值排序),返回mvpOrderedConnectedKeyFrames，是一个vector123456789/** * @brief 得到与该关键帧连接的关键帧(已按权值排序) * @return 连接的关键帧 */vector&lt;KeyFrame*&gt; KeyFrame::GetVectorCovisibleKeyFrames()&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); return mvpOrderedConnectedKeyFrames;&#125; 2.14 得到与该关键帧连接的前N个关键帧(已按权值排序)123456789101112131415/** * @brief 得到与该关键帧连接的前N个关键帧(已按权值排序) * * 如果连接的关键帧少于N，则返回所有连接的关键帧 * @param N 前N个 * @return 连接的关键帧 */vector&lt;KeyFrame*&gt; KeyFrame::GetBestCovisibilityKeyFrames(const int &amp;N)&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); if((int)mvpOrderedConnectedKeyFrames.size()&lt;N) return mvpOrderedConnectedKeyFrames; else return vector&lt;KeyFrame*&gt;(mvpOrderedConnectedKeyFrames.begin(),mvpOrderedConnectedKeyFrames.begin()+N);&#125; 2.15 得到与该关键帧连接的权重大于等于w的关键帧，返回的是一个vector，已排序12345678910111213141516171819202122232425/** * @brief 得到与该关键帧连接的权重大于等于w的关键帧 * @param w 权重 * @return 连接的关键帧 */vector&lt;KeyFrame*&gt; KeyFrame::GetCovisiblesByWeight(const int &amp;w)&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); if(mvpOrderedConnectedKeyFrames.empty()) return vector&lt;KeyFrame*&gt;(); // http://www.cplusplus.com/reference/algorithm/upper_bound/ // 从mvOrderedWeights找出第一个大于w的那个迭代器 // 这里应该使用lower_bound，因为lower_bound是返回小于等于，而upper_bound只能返回第一个大于的 // 自注释：这里没问题，因为自定义的比较函数weightComp是按大于排序的 vector&lt;int&gt;::iterator it = upper_bound(mvOrderedWeights.begin(),mvOrderedWeights.end(),w,KeyFrame::weightComp); if(it==mvOrderedWeights.end() &amp;&amp; *mvOrderedWeights.rbegin()&lt;w) return vector&lt;KeyFrame*&gt;(); else &#123; int n = it-mvOrderedWeights.begin(); return vector&lt;KeyFrame*&gt;(mvpOrderedConnectedKeyFrames.begin(), mvpOrderedConnectedKeyFrames.begin()+n); &#125;&#125; 2.16 得到该关键帧与pKF的权重12345678910111213/** * @brief 得到该关键帧与pKF的权重 * @param pKF 关键帧 * @return 权重 */int KeyFrame::GetWeight(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); if(mConnectedKeyFrameWeights.count(pKF)) return mConnectedKeyFrameWeights[pKF]; else return 0;&#125; 2.17 在关键帧中添加地图点，需要指定地图点的序号，也就是对应的特征点序号12345678910/** * @brief Add MapPoint to KeyFrame * @param pMP MapPoint * @param idx MapPoint在KeyFrame中的索引 */void KeyFrame::AddMapPoint(MapPoint *pMP, const size_t &amp;idx)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); mvpMapPoints[idx]=pMP;&#125; 2.18 传入地图点序号，清除地图点12345void KeyFrame::EraseMapPointMatch(const size_t &amp;idx)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); mvpMapPoints[idx]=static_cast&lt;MapPoint*&gt;(NULL);&#125; 2.19 传入地图点指针，清除地图点123456void KeyFrame::EraseMapPointMatch(MapPoint* pMP)&#123; int idx = pMP-&gt;GetIndexInKeyFrame(this); if(idx&gt;=0) mvpMapPoints[idx]=static_cast&lt;MapPoint*&gt;(NULL);&#125; 2.20 传入地图点序号和新的地图点指针，替换原来的地图点1234void KeyFrame::ReplaceMapPointMatch(const size_t &amp;idx, MapPoint* pMP)&#123; mvpMapPoints[idx]=pMP;&#125; 2.21 返回地图点set集合，判断每一个地图点是否为空，该地图点是否是坏点1234567891011121314set&lt;MapPoint*&gt; KeyFrame::GetMapPoints()&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); set&lt;MapPoint*&gt; s; for(size_t i=0, iend=mvpMapPoints.size(); i&lt;iend; i++) &#123; if(!mvpMapPoints[i]) continue; MapPoint* pMP = mvpMapPoints[i]; if(!pMP-&gt;isBad()) s.insert(pMP); &#125; return s;&#125; 2.22 关键帧中，大于等于minObs的MapPoints的数量，判断每一个地图点是否为空，该地图点是否是坏点，该地图点的观测关键帧是否大于阈值123456789101112131415161718192021222324252627282930313233/** * @brief 关键帧中，大于等于minObs的MapPoints的数量 * minObs就是一个阈值，大于minObs就表示该MapPoint是一个高质量的MapPoint * 一个高质量的MapPoint会被多个KeyFrame观测到， * @param minObs 最小观测 */int KeyFrame::TrackedMapPoints(const int &amp;minObs)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); int nPoints=0; const bool bCheckObs = minObs&gt;0; for(int i=0; i&lt;N; i++) &#123; MapPoint* pMP = mvpMapPoints[i]; if(pMP) &#123; if(!pMP-&gt;isBad()) &#123; if(bCheckObs) &#123; // 该MapPoint是一个高质量的MapPoint if(mvpMapPoints[i]-&gt;Observations()&gt;=minObs) nPoints++; &#125; else nPoints++; &#125; &#125; &#125; return nPoints;&#125; 2.23 获得该关键帧所有的MapPoints12345678910/** * @brief Get MapPoint Matches * * 获取该关键帧的MapPoints */vector&lt;MapPoint*&gt; KeyFrame::GetMapPointMatches()&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); return mvpMapPoints;&#125; 2.24 通过地图点的序号来获得关键帧地图点12345MapPoint* KeyFrame::GetMapPoint(const size_t &amp;idx)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); return mvpMapPoints[idx];&#125; 2.25 新建一个关键帧后，更新共视图的连接123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125/** * @brief 更新图的连接 * * 1. 首先获得该关键帧的所有MapPoint点，统计观测到这些3d点的每个关键帧与该帧图像的共视程度， * 对每一个找到的关键帧，建立一条边，边的权重是该关键帧与当前关键帧公共3d点的个数。 * 2. 并且该权重必须大于一个阈值，如果没有超过该阈值的权重，那么就只保留权重最大的边（与其它关键帧的共视程度比较高） * 3. 对这些连接按照权重从大到小进行排序，以方便将来的处理 * 更新完covisibility图之后，如果没有初始化过，则初始化为连接权重最大的边（与其它关键帧共视程度最高的那个关键帧），类似于最大生成树 */void KeyFrame::UpdateConnections()&#123; // 在没有执行这个函数前，关键帧只和MapPoints之间有连接关系，这个函数可以更新关键帧之间的连接关系 //===============1================================== map&lt;KeyFrame*,int&gt; KFcounter; // 关键帧-权重，权重为其它关键帧与当前关键帧共视3d点的个数 vector&lt;MapPoint*&gt; vpMP; &#123; // 获得该关键帧的所有3D点 unique_lock&lt;mutex&gt; lockMPs(mMutexFeatures); vpMP = mvpMapPoints; &#125; //For all map points in keyframe check in which other keyframes are they seen //Increase counter for those keyframes // 通过3D点间接统计可以观测到这些3D点的所有关键帧之间的共视程度 // 即统计每一个关键帧都有多少关键帧与它存在共视关系，统计结果放在KFcounter for(vector&lt;MapPoint*&gt;::iterator vit=vpMP.begin(), vend=vpMP.end(); vit!=vend; vit++) &#123; MapPoint* pMP = *vit; if(!pMP) continue; if(pMP-&gt;isBad()) continue; // 对于每一个MapPoint点，observations记录了可以观测到该MapPoint的所有关键帧 map&lt;KeyFrame*,size_t&gt; observations = pMP-&gt;GetObservations(); for(map&lt;KeyFrame*,size_t&gt;::iterator mit=observations.begin(), mend=observations.end(); mit!=mend; mit++) &#123; // 除去自身，自己与自己不算共视 if(mit-&gt;first-&gt;mnId==mnId) continue; KFcounter[mit-&gt;first]++; &#125; &#125; // This should not happen if(KFcounter.empty()) return; //===============2================================== // If the counter is greater than threshold add connection // In case no keyframe counter is over threshold add the one with maximum counter int nmax=0; KeyFrame* pKFmax=NULL; int th = 15; // vPairs记录与其它关键帧共视帧数大于th的关键帧 // pair&lt;int,KeyFrame*&gt;将关键帧的权重写在前面，关键帧写在后面方便后面排序 vector&lt;pair&lt;int,KeyFrame*&gt; &gt; vPairs; vPairs.reserve(KFcounter.size()); for(map&lt;KeyFrame*,int&gt;::iterator mit=KFcounter.begin(), mend=KFcounter.end(); mit!=mend; mit++) &#123; if(mit-&gt;second&gt;nmax) &#123; nmax=mit-&gt;second; // 找到对应权重最大的关键帧（共视程度最高的关键帧） pKFmax=mit-&gt;first; &#125; if(mit-&gt;second&gt;=th) &#123; // 对应权重需要大于阈值，对这些关键帧建立连接 vPairs.push_back(make_pair(mit-&gt;second,mit-&gt;first)); // 更新KFcounter中该关键帧的mConnectedKeyFrameWeights // 更新其它KeyFrame的mConnectedKeyFrameWeights，更新其它关键帧与当前帧的连接权重 (mit-&gt;first)-&gt;AddConnection(this,mit-&gt;second); &#125; &#125; // 如果没有超过阈值的权重，则对权重最大的关键帧建立连接 if(vPairs.empty()) &#123; // 如果每个关键帧与它共视的关键帧的个数都少于th， // 那就只更新与其它关键帧共视程度最高的关键帧的mConnectedKeyFrameWeights // 这是对之前th这个阈值可能过高的一个补丁 vPairs.push_back(make_pair(nmax,pKFmax)); pKFmax-&gt;AddConnection(this,nmax); &#125; // vPairs里存的都是相互共视程度比较高的关键帧和共视权重，由小到大 sort(vPairs.begin(),vPairs.end()); // lKFs 和 lWs 排序都是从大到小 list&lt;KeyFrame*&gt; lKFs; list&lt;int&gt; lWs; for(size_t i=0; i&lt;vPairs.size();i++) &#123; lKFs.push_front(vPairs[i].second); lWs.push_front(vPairs[i].first); &#125; //===============3================================== &#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); // mspConnectedKeyFrames = spConnectedKeyFrames; // 更新图的连接(权重) mConnectedKeyFrameWeights = KFcounter;//更新该KeyFrame的mConnectedKeyFrameWeights，更新当前帧与其它关键帧的连接权重 mvpOrderedConnectedKeyFrames = vector&lt;KeyFrame*&gt;(lKFs.begin(),lKFs.end()); mvOrderedWeights = vector&lt;int&gt;(lWs.begin(), lWs.end()); // 更新生成树的连接 if(mbFirstConnection &amp;&amp; mnId!=0) &#123; // 初始化该关键帧的父关键帧为共视程度最高的那个关键帧 mpParent = mvpOrderedConnectedKeyFrames.front(); // 建立双向连接关系 mpParent-&gt;AddChild(this); mbFirstConnection = false; &#125; &#125;&#125; 2.26 增加子树节点12345void KeyFrame::AddChild(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); mspChildrens.insert(pKF);&#125; 2.27 删除子树节点12345void KeyFrame::EraseChild(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); mspChildrens.erase(pKF);&#125; 2.28 改变父节点123456void KeyFrame::ChangeParent(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); mpParent = pKF; pKF-&gt;AddChild(this);&#125; 2.29 获得所有子树节点set12345set&lt;KeyFrame*&gt; KeyFrame::GetChilds()&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); return mspChildrens;&#125; 2.30 获得该帧的父节点12345KeyFrame* KeyFrame::GetParent()&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); return mpParent;&#125; 2.31 判断某一帧是否含有某一个关键帧子节点12345bool KeyFrame::hasChild(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); return mspChildrens.count(pKF);&#125; 2.32 将某一关键帧加入回环边集合123456void KeyFrame::AddLoopEdge(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); mbNotErase = true; mspLoopEdges.insert(pKF);&#125; 2.33 获得回环关键帧集合12345set&lt;KeyFrame*&gt; KeyFrame::GetLoopEdges()&#123; unique_lock&lt;mutex&gt; lockCon(mMutexConnections); return mspLoopEdges;&#125; 2.34 设置某一关键帧不可擦除12345void KeyFrame::SetNotErase()&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); mbNotErase = true;&#125; 2.35 删除关键帧，修改父子连接关系123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111void KeyFrame::SetBadFlag()&#123; &#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); if(mnId==0) return; else if(mbNotErase)// mbNotErase表示不应该擦除该KeyFrame，于是把mbToBeErased置为true，表示已经擦除了，其实没有擦除 &#123; mbToBeErased = true; return; &#125; &#125; for(map&lt;KeyFrame*,int&gt;::iterator mit = mConnectedKeyFrameWeights.begin(), mend=mConnectedKeyFrameWeights.end(); mit!=mend; mit++) mit-&gt;first-&gt;EraseConnection(this);// 让其它的KeyFrame删除与自己的联系 for(size_t i=0; i&lt;mvpMapPoints.size(); i++) if(mvpMapPoints[i]) mvpMapPoints[i]-&gt;EraseObservation(this);// 让与自己有联系的MapPoint删除与自己的联系 &#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); unique_lock&lt;mutex&gt; lock1(mMutexFeatures); //清空自己与其它关键帧之间的联系 mConnectedKeyFrameWeights.clear(); mvpOrderedConnectedKeyFrames.clear(); // Update Spanning Tree set&lt;KeyFrame*&gt; sParentCandidates; sParentCandidates.insert(mpParent); // Assign at each iteration one children with a parent (the pair with highest covisibility weight) // Include that children as new parent candidate for the rest // 如果这个关键帧有自己的孩子关键帧，告诉这些子关键帧，它们的父关键帧不行了，赶紧找新的父关键帧 while(!mspChildrens.empty()) &#123; bool bContinue = false; int max = -1; KeyFrame* pC; KeyFrame* pP; // 遍历每一个子关键帧，让它们更新它们指向的父关键帧 for(set&lt;KeyFrame*&gt;::iterator sit=mspChildrens.begin(), send=mspChildrens.end(); sit!=send; sit++) &#123; KeyFrame* pKF = *sit; if(pKF-&gt;isBad()) continue; // Check if a parent candidate is connected to the keyframe // 子关键帧遍历每一个与它相连的关键帧（共视关键帧） vector&lt;KeyFrame*&gt; vpConnected = pKF-&gt;GetVectorCovisibleKeyFrames(); for(size_t i=0, iend=vpConnected.size(); i&lt;iend; i++) &#123; for(set&lt;KeyFrame*&gt;::iterator spcit=sParentCandidates.begin(), spcend=sParentCandidates.end(); spcit!=spcend; spcit++) &#123; // 如果该帧的子节点和父节点（祖孙节点）之间存在连接关系（共视） // 举例：B--&gt;A（B的父节点是A） C--&gt;B（C的父节点是B） D--C（D与C相连） E--C（E与C相连） F--C（F与C相连） D--&gt;A（D的父节点是A） E--&gt;A（E的父节点是A） // 现在B挂了，于是C在与自己相连的D、E、F节点中找到父节点指向A的D // 此过程就是为了找到可以替换B的那个节点。 // 上面例子中，B为当前要设置为SetBadFlag的关键帧 // A为spcit，也即sParentCandidates // C为pKF,pC，也即mspChildrens中的一个 // D、E、F为vpConnected中的变量，由于C与D间的权重 比 C与E间的权重大，因此D为pP if(vpConnected[i]-&gt;mnId == (*spcit)-&gt;mnId) &#123; int w = pKF-&gt;GetWeight(vpConnected[i]); if(w&gt;max) &#123; pC = pKF; pP = vpConnected[i]; max = w; bContinue = true; &#125; &#125; &#125; &#125; &#125; if(bContinue) &#123; // 因为父节点死了，并且子节点找到了新的父节点，子节点更新自己的父节点 pC-&gt;ChangeParent(pP); // 因为子节点找到了新的父节点并更新了父节点，那么该子节点升级，作为其它子节点的备选父节点 sParentCandidates.insert(pC); // 该子节点处理完毕 mspChildrens.erase(pC); &#125; else break; &#125; // If a children has no covisibility links with any parent candidate, assign to the original parent of this KF // 如果还有子节点没有找到新的父节点 if(!mspChildrens.empty()) for(set&lt;KeyFrame*&gt;::iterator sit=mspChildrens.begin(); sit!=mspChildrens.end(); sit++) &#123; // 直接把父节点的父节点作为自己的父节点 (*sit)-&gt;ChangeParent(mpParent); &#125; mpParent-&gt;EraseChild(this); mTcp = Tcw*mpParent-&gt;GetPoseInverse(); mbBad = true; &#125; mpMap-&gt;EraseKeyFrame(this); mpKeyFrameDB-&gt;erase(this);&#125; 2.36 判断一帧图像是否是坏帧，即是否已经被删除12345bool KeyFrame::isBad()&#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); return mbBad;&#125; 2.37 删除与某一帧的联系，并没有删除该关键帧123456789101112131415void KeyFrame::EraseConnection(KeyFrame* pKF)&#123; bool bUpdate = false; &#123; unique_lock&lt;mutex&gt; lock(mMutexConnections); if(mConnectedKeyFrameWeights.count(pKF)) &#123; mConnectedKeyFrameWeights.erase(pKF); bUpdate=true; &#125; &#125; if(bUpdate) UpdateBestCovisibles();&#125; 2.38 找到在以x,y为中心，边长为2r的方形内特征点，先遍历该圆域区间所属的边长为2r的格子区间内的所有特征点，再判断该点的尺度信息是否满足要求，再判断该特征点离中心点的距离是否满足半径要求，返回该区间的所有的特征点，参考普通帧的GetFeaturesInArea函数12345678910111213141516171819202122232425262728293031323334353637383940414243// r为边长（半径）vector&lt;size_t&gt; KeyFrame::GetFeaturesInArea(const float &amp;x, const float &amp;y, const float &amp;r) const&#123; vector&lt;size_t&gt; vIndices; vIndices.reserve(N); // floor向下取整，mfGridElementWidthInv为每个像素占多少个格子 const int nMinCellX = max(0,(int)floor((x-mnMinX-r)*mfGridElementWidthInv)); if(nMinCellX&gt;=mnGridCols) return vIndices; // ceil向上取整 const int nMaxCellX = min((int)mnGridCols-1,(int)ceil((x-mnMinX+r)*mfGridElementWidthInv)); if(nMaxCellX&lt;0) return vIndices; const int nMinCellY = max(0,(int)floor((y-mnMinY-r)*mfGridElementHeightInv)); if(nMinCellY&gt;=mnGridRows) return vIndices; const int nMaxCellY = min((int)mnGridRows-1,(int)ceil((y-mnMinY+r)*mfGridElementHeightInv)); if(nMaxCellY&lt;0) return vIndices; for(int ix = nMinCellX; ix&lt;=nMaxCellX; ix++) &#123; for(int iy = nMinCellY; iy&lt;=nMaxCellY; iy++) &#123; const vector&lt;size_t&gt; vCell = mGrid[ix][iy]; for(size_t j=0, jend=vCell.size(); j&lt;jend; j++) &#123; const cv::KeyPoint &amp;kpUn = mvKeysUn[vCell[j]]; const float distx = kpUn.pt.x-x; const float disty = kpUn.pt.y-y; if(fabs(distx)&lt;r &amp;&amp; fabs(disty)&lt;r) vIndices.push_back(vCell[j]); &#125; &#125; &#125; return vIndices;&#125; 2.39 判断某个坐标点是否在图像内1234bool KeyFrame::IsInImage(const float &amp;x, const float &amp;y) const&#123; return (x&gt;=mnMinX &amp;&amp; x&lt;mnMaxX &amp;&amp; y&gt;=mnMinY &amp;&amp; y&lt;mnMaxY);&#125; 2.40 双目立体投影123456789101112131415161718192021222324252627282930/** * @brief Backprojects a keypoint (if stereo/depth info available) into 3D world coordinates. * @param i 第i个keypoint * @return 3D点（相对于世界坐标系） */cv::Mat KeyFrame::UnprojectStereo(int i)&#123; const float z = mvDepth[i]; if(z&gt;0) &#123; // 由2维图像反投影到相机坐标系 // mvDepth是在ComputeStereoMatches函数中求取的 // mvDepth对应的校正前的特征点，因此这里对校正前特征点反投影 // 可在Frame::UnprojectStereo中却是对校正后的特征点mvKeysUn反投影 // 在ComputeStereoMatches函数中应该对校正后的特征点求深度？？ (wubo???) const float u = mvKeys[i].pt.x; const float v = mvKeys[i].pt.y; const float x = (u-cx)*z*invfx; const float y = (v-cy)*z*invfy; cv::Mat x3Dc = (cv::Mat_&lt;float&gt;(3,1) &lt;&lt; x, y, z); unique_lock&lt;mutex&gt; lock(mMutexPose); // 由相机坐标系转换到世界坐标系 // Twc为相机坐标系到世界坐标系的变换矩阵 // Twc.rosRange(0,3).colRange(0,3)取Twc矩阵的前3行与前3列 return Twc.rowRange(0,3).colRange(0,3)*x3Dc+Twc.rowRange(0,3).col(3); &#125; else return cv::Mat();&#125; 2.41 计算并返回所有地图点的深度中值123456789101112131415161718192021222324252627282930313233343536/** * @brief 评估当前关键帧场景深度，q=2表示中值 * @param q q=2 * @return Median Depth */float KeyFrame::ComputeSceneMedianDepth(const int q)&#123; vector&lt;MapPoint*&gt; vpMapPoints; cv::Mat Tcw_; &#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); unique_lock&lt;mutex&gt; lock2(mMutexPose); vpMapPoints = mvpMapPoints; Tcw_ = Tcw.clone(); &#125; vector&lt;float&gt; vDepths; vDepths.reserve(N); cv::Mat Rcw2 = Tcw_.row(2).colRange(0,3); Rcw2 = Rcw2.t(); float zcw = Tcw_.at&lt;float&gt;(2,3); for(int i=0; i&lt;N; i++) &#123; if(mvpMapPoints[i]) &#123; MapPoint* pMP = mvpMapPoints[i]; cv::Mat x3Dw = pMP-&gt;GetWorldPos(); float z = Rcw2.dot(x3Dw)+zcw; // (R*x3Dw+t)的第三行，即z vDepths.push_back(z); &#125; &#125; sort(vDepths.begin(),vDepths.end()); return vDepths[(vDepths.size()-1)/q];&#125;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析九]]></title>
    <url>%2F2019%2F06%2F14%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%9D%2F</url>
    <content type="text"><![CDATA[Frame 类说明1. Frame 类成员变量说明1.1 用于重定位的词典12// Vocabulary used for relocalization.ORBVocabulary* mpORBvocabulary; 1.2 当前帧的特征提取器12// Feature extractor. The right is used only in the stereo case. ORBextractor* mpORBextractorLeft, *mpORBextractorRight; 1.3 当前帧的时间戳12// Frame timestamp. double mTimeStamp; 1.3 当前帧的时间戳12// Frame timestamp. double mTimeStamp; 1.3 当前帧的时间戳12// Frame timestamp. double mTimeStamp; 1.4 相机参数123456789101112131415// Calibration matrix and OpenCV distortion parameters. cv::Mat mK; static float fx; static float fy; static float cx; static float cy; static float invfx; static float invfy; cv::Mat mDistCoef; // Stereo baseline multiplied by fx. float mbf; // Stereo baseline in meters. float mb; 1.4 截断深度123// Threshold close/far points. Close points are inserted from 1 view. // Far points are inserted as in the monocular case from 2 views. float mThDepth; 1.5 特征点的数目12// Number of KeyPoints. int N; ///&lt; KeyPoints数量 1.6 特征点Vector12345678// Vector of keypoints (original for visualization) and undistorted (actually used by the system).// In the stereo case, mvKeysUn is redundant as images must be rectified.// In the RGB-D case, RGB images can be distorted.// mvKeys:原始左图像提取出的特征点（未校正）// mvKeysRight:原始右图像提取出的特征点（未校正）// mvKeysUn:校正mvKeys后的特征点，对于双目摄像头，一般得到的图像都是校正好的，再校正一次有点多余std::vector&lt;cv::KeyPoint&gt; mvKeys, mvKeysRight;std::vector&lt;cv::KeyPoint&gt; mvKeysUn; 1.7 特征点的双目信息1234567// Corresponding stereo coordinate and depth for each keypoint. // "Monocular" keypoints have a negative value. // 对于双目，mvuRight存储了左目像素点在右目中的对应点的横坐标 // mvDepth对应的深度 // 单目摄像头，这两个容器中存的都是-1 std::vector&lt;float&gt; mvuRight; std::vector&lt;float&gt; mvDepth; 1.8 当前帧的Bow以及特征点的Bow123// Bag of Words Vector structures. DBoW2::BowVector mBowVec; DBoW2::FeatureVector mFeatVec; 1.9 当前帧的描述子123// ORB descriptor, each row associated to a keypoint. // 左目摄像头和右目摄像头特征点对应的描述子 cv::Mat mDescriptors, mDescriptorsRight; 1.10 每个特征点对应的MapPoints123// MapPoints associated to keypoints, NULL pointer if no association. // 每个特征点对应的MapPoint std::vector&lt;MapPoint*&gt; mvpMapPoints; 1.11 特征点是否是Outlier的flag123// Flag to identify outlier associations. // 观测不到Map中的3D点 std::vector&lt;bool&gt; mvbOutlier; 1.12 Frame 网格信息划分 mGrid[FRAME_GRID_COLS][FRAME_GRID_ROWS]记录每一个网格包含的特征点的序号vector12345678// Keypoints are assigned to cells in a grid to reduce matching complexity when projecting MapPoints. // 坐标乘以mfGridElementWidthInv和mfGridElementHeightInv就可以确定在哪个格子 static float mfGridElementWidthInv; static float mfGridElementHeightInv; // 每个格子分配的特征点数，将图像分成格子，保证提取的特征点比较均匀 // FRAME_GRID_ROWS 48 // FRAME_GRID_COLS 64 std::vector&lt;std::size_t&gt; mGrid[FRAME_GRID_COLS][FRAME_GRID_ROWS]; 1.13 相机位姿12// Camera pose. cv::Mat mTcw; ///&lt; 相机姿态 世界坐标系到相机坐标坐标系的变换矩阵 1.14 当前帧和下一帧的id123// Current and Next Frame id.static long unsigned int nNextId; ///&lt; Next Frame id.long unsigned int mnId; ///&lt; Current Frame id. 1.15 当前帧的参考关键帧12// Reference Keyframe. KeyFrame* mpReferenceKF;//指针，指向参考关键帧 1.16 图像金字塔信息12345678// Scale pyramid info. int mnScaleLevels;//图像提金字塔的层数 float mfScaleFactor;//图像提金字塔的尺度因子 float mfLogScaleFactor;// vector&lt;float&gt; mvScaleFactors; vector&lt;float&gt; mvInvScaleFactors; vector&lt;float&gt; mvLevelSigma2; vector&lt;float&gt; mvInvLevelSigma2; 1.17 用于确定画格子时的边界123456// Undistorted Image Bounds (computed once).// 用于确定画格子时的边界static float mnMinX;static float mnMaxX;static float mnMinY;static float mnMaxY; 1.18 mbInitialComputations初始计算Flag1static bool mbInitialComputations; 构造第一帧的时候，确定相机内参等，避免重复初始化计算 1.19 相机位姿的几个变换1234cv::Mat mRcw; ///&lt; Rotation from world to cameracv::Mat mtcw; ///&lt; Translation from world to cameracv::Mat mRwc; ///&lt; Rotation from camera to worldcv::Mat mOw; ///&lt; mtwc,Translation from camera to world 1.20 图像帧划分格子数目12#define FRAME_GRID_ROWS 48#define FRAME_GRID_COLS 64 2. Frame 类成员函数说明2.1 构造函数 默认构造函数，建立一个空的Frame 12Frame::Frame()&#123;&#125; 复制构造函数，mLastFrame = Frame(mCurrentFrame)，完全复制当前帧的成员变量 1234567891011121314151617181920Frame::Frame(const Frame &amp;frame) :mpORBvocabulary(frame.mpORBvocabulary), mpORBextractorLeft(frame.mpORBextractorLeft), mpORBextractorRight(frame.mpORBextractorRight), mTimeStamp(frame.mTimeStamp), mK(frame.mK.clone()), mDistCoef(frame.mDistCoef.clone()), mbf(frame.mbf), mb(frame.mb), mThDepth(frame.mThDepth), N(frame.N), mvKeys(frame.mvKeys), mvKeysRight(frame.mvKeysRight), mvKeysUn(frame.mvKeysUn), mvuRight(frame.mvuRight), mvDepth(frame.mvDepth), mBowVec(frame.mBowVec), mFeatVec(frame.mFeatVec), mDescriptors(frame.mDescriptors.clone()), mDescriptorsRight(frame.mDescriptorsRight.clone()), mvpMapPoints(frame.mvpMapPoints), mvbOutlier(frame.mvbOutlier), mnId(frame.mnId), mpReferenceKF(frame.mpReferenceKF), mnScaleLevels(frame.mnScaleLevels), mfScaleFactor(frame.mfScaleFactor), mfLogScaleFactor(frame.mfLogScaleFactor), mvScaleFactors(frame.mvScaleFactors), mvInvScaleFactors(frame.mvInvScaleFactors), mvLevelSigma2(frame.mvLevelSigma2), mvInvLevelSigma2(frame.mvInvLevelSigma2)&#123; for(int i=0;i&lt;FRAME_GRID_COLS;i++) for(int j=0; j&lt;FRAME_GRID_ROWS; j++) mGrid[i][j]=frame.mGrid[i][j]; if(!frame.mTcw.empty()) SetPose(frame.mTcw);&#125; RGB-D 构造函数 Stereo 构造函数 Monocular 构造函数，利用灰度图，时间戳，特征提取器，词典，相机内参，相机矫正参数，基线，深度截断值来构造一帧图像123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 单目初始化Frame::Frame(const cv::Mat &amp;imGray, const double &amp;timeStamp, ORBextractor* extractor,ORBVocabulary* voc, cv::Mat &amp;K, cv::Mat &amp;distCoef, const float &amp;bf, const float &amp;thDepth) :mpORBvocabulary(voc),mpORBextractorLeft(extractor),mpORBextractorRight(static_cast&lt;ORBextractor*&gt;(NULL)), mTimeStamp(timeStamp), mK(K.clone()),mDistCoef(distCoef.clone()), mbf(bf), mThDepth(thDepth)&#123; // Frame ID mnId=nNextId++; // Scale Level Info ，设置金字塔信息 mnScaleLevels = mpORBextractorLeft-&gt;GetLevels(); mfScaleFactor = mpORBextractorLeft-&gt;GetScaleFactor(); mfLogScaleFactor = log(mfScaleFactor); mvScaleFactors = mpORBextractorLeft-&gt;GetScaleFactors(); mvInvScaleFactors = mpORBextractorLeft-&gt;GetInverseScaleFactors(); mvLevelSigma2 = mpORBextractorLeft-&gt;GetScaleSigmaSquares(); mvInvLevelSigma2 = mpORBextractorLeft-&gt;GetInverseScaleSigmaSquares(); // ORB extraction ExtractORB(0,imGray); // 0表示単目，进行特征点的提取，提取的特征存在mvKeys里面 N = mvKeys.size(); if(mvKeys.empty()) return; // 调用OpenCV的矫正函数矫正orb提取的特征点 UndistortKeyPoints(); // Set no stereo information mvuRight = vector&lt;float&gt;(N,-1); mvDepth = vector&lt;float&gt;(N,-1); mvpMapPoints = vector&lt;MapPoint*&gt;(N,static_cast&lt;MapPoint*&gt;(NULL)); // 对特征点列表构造空的地图点列表 mvbOutlier = vector&lt;bool&gt;(N,false); // This is done only for the first Frame (or after a change in the calibration) if(mbInitialComputations) &#123; ComputeImageBounds(imGray); mfGridElementWidthInv=static_cast&lt;float&gt;(FRAME_GRID_COLS)/static_cast&lt;float&gt;(mnMaxX-mnMinX); mfGridElementHeightInv=static_cast&lt;float&gt;(FRAME_GRID_ROWS)/static_cast&lt;float&gt;(mnMaxY-mnMinY); fx = K.at&lt;float&gt;(0,0); fy = K.at&lt;float&gt;(1,1); cx = K.at&lt;float&gt;(0,2); cy = K.at&lt;float&gt;(1,2); invfx = 1.0f/fx; invfy = 1.0f/fy; mbInitialComputations=false; &#125; mb = mbf/fx; AssignFeaturesToGrid();&#125; 2.2 获取特征点对应的格子坐标，并判断该特征点是否在原图像中1234567891011bool Frame::PosInGrid(const cv::KeyPoint &amp;kp, int &amp;posX, int &amp;posY)&#123; posX = round((kp.pt.x-mnMinX)*mfGridElementWidthInv); // 将实际坐标分散到各个网格中 posY = round((kp.pt.y-mnMinY)*mfGridElementHeightInv); //Keypoint's coordinates are undistorted, which could cause to go out of the image if(posX&lt;0 || posX&gt;=FRAME_GRID_COLS || posY&lt;0 || posY&gt;=FRAME_GRID_ROWS) return false; return true;&#125; 2.3 将矫正过的特征点分配到各个网格中，判断矫正过的特征点在哪个网格中，并将其加入到对应的表格vector中1234567891011121314151617void Frame::AssignFeaturesToGrid()&#123; int nReserve = 0.5f*N/(FRAME_GRID_COLS*FRAME_GRID_ROWS); for(unsigned int i=0; i&lt;FRAME_GRID_COLS;i++) for (unsigned int j=0; j&lt;FRAME_GRID_ROWS;j++) mGrid[i][j].reserve(nReserve); // 在mGrid中记录了各特征点 for(int i=0;i&lt;N;i++) &#123; const cv::KeyPoint &amp;kp = mvKeysUn[i]; int nGridPosX, nGridPosY; if(PosInGrid(kp,nGridPosX,nGridPosY)) mGrid[nGridPosX][nGridPosY].push_back(i); &#125;&#125; 2.4 特征提取函数ExtractORB，调用初始化的特征提取器进行特征提取，flag = 0単目提取，flag = 1，双目提取1234567void Frame::ExtractORB(int flag, const cv::Mat &amp;im)&#123; if(flag==0) (*mpORBextractorLeft)(im,cv::Mat(),mvKeys,mDescriptors); else (*mpORBextractorRight)(im,cv::Mat(),mvKeysRight,mDescriptorsRight);&#125; 2.5 设置相机姿态，随后会调用 UpdatePoseMatrices() 来改变mRcw,mRwc等变量的值1234567891011/** * @brief Set the camera pose. * * 设置相机姿态，随后会调用 UpdatePoseMatrices() 来改变mRcw,mRwc等变量的值 * @param Tcw Transformation from world to camera */void Frame::SetPose(cv::Mat Tcw)&#123; mTcw = Tcw.clone(); UpdatePoseMatrices();&#125; 2.6 根据Tcw计算mRcw、mtcw和mRwc、mOw（Twc）1234567891011121314151617/** * @brief Computes rotation, translation and camera center matrices from the camera pose. * * 根据Tcw计算mRcw、mtcw和mRwc、mOw */void Frame::UpdatePoseMatrices()&#123; // [x_camera 1] = [R|t]*[x_world 1]，坐标为齐次形式 // x_camera = R*x_world + t mRcw = mTcw.rowRange(0,3).colRange(0,3); mRwc = mRcw.t(); mtcw = mTcw.rowRange(0,3).col(3); // mtcw, 即相机坐标系下相机坐标系到世界坐标系间的向量, 向量方向由相机坐标系指向世界坐标系 // mOw, 即世界坐标系下世界坐标系到相机坐标系间的向量, 向量方向由世界坐标系指向相机坐标系 mOw = -mRcw.t()*mtcw;&#125; 2.7 判断一个点是否在当前帧的视野内，将该地图点投影到图像中然后计算观测方向夹角，预测在当前帧的尺度12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * @brief 判断一个点是否在视野内 * * 计算了重投影坐标，观测方向夹角，预测在当前帧的尺度 * @param pMP MapPoint * @param viewingCosLimit 视角和平均视角的方向阈值,0.5 ,cos 60 * @return true if is in view * @see SearchLocalPoints() */bool Frame::isInFrustum(MapPoint *pMP, float viewingCosLimit)&#123; pMP-&gt;mbTrackInView = false; // 3D in absolute coordinates cv::Mat P = pMP-&gt;GetWorldPos(); // 3D in camera coordinates // 3D点P在相机坐标系下的坐标 const cv::Mat Pc = mRcw*P+mtcw; // 这里的Rt是经过初步的优化后的 const float &amp;PcX = Pc.at&lt;float&gt;(0); const float &amp;PcY = Pc.at&lt;float&gt;(1); const float &amp;PcZ = Pc.at&lt;float&gt;(2); // Check positive depth if(PcZ&lt;0.0f) return false; // Project in image and check it is not outside // V-D 1) 将MapPoint投影到当前帧, 并判断是否在图像内 const float invz = 1.0f/PcZ; const float u=fx*PcX*invz+cx; const float v=fy*PcY*invz+cy; if(u&lt;mnMinX || u&gt;mnMaxX) return false; if(v&lt;mnMinY || v&gt;mnMaxY) return false; // Check distance is in the scale invariance region of the MapPoint // V-D 3) 计算MapPoint到相机中心的距离, 并判断是否在尺度变化的距离内 const float maxDistance = pMP-&gt;GetMaxDistanceInvariance(); const float minDistance = pMP-&gt;GetMinDistanceInvariance(); // 世界坐标系下，相机到3D点P的向量, 向量方向由相机指向3D点P const cv::Mat PO = P-mOw; const float dist = cv::norm(PO); if(dist&lt;minDistance || dist&gt;maxDistance) return false; // Check viewing angle // V-D 2) 计算当前视角和平均视角夹角的余弦值, 若小于cos(60), 即夹角大于60度则返回 cv::Mat Pn = pMP-&gt;GetNormal(); const float viewCos = PO.dot(Pn)/dist; if(viewCos&lt;viewingCosLimit) // viewingCosLimit = 0.5 return false; // Predict scale in the image // V-D 4) 根据深度预测尺度（对应特征点在一层） const int nPredictedLevel = pMP-&gt;PredictScale(dist,this); // Data used by the tracking // 标记该点将来要被投影 pMP-&gt;mbTrackInView = true; pMP-&gt;mTrackProjX = u; pMP-&gt;mTrackProjXR = u - mbf*invz; //该3D点投影到双目右侧相机上的横坐标 pMP-&gt;mTrackProjY = v; pMP-&gt;mnTrackScaleLevel = nPredictedLevel; pMP-&gt;mTrackViewCos = viewCos; return true;&#125; 2.8 找到在以x,y为中心，边长为2r的方形内且在[minLevel, maxLevel]的特征点，先遍历该圆域区间所属的边长为2r的格子区间内的所有特征点，再判断该点的尺度信息是否满足要求，再判断该特征点离中心点的距离是否满足半径要求，返回该区间的所有的特征点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * @brief 找到在 以x,y为中心,边长为2r的方形内且在[minLevel, maxLevel]的特征点 * @param x 图像坐标u * @param y 图像坐标v * @param r 边长 * @param minLevel 最小尺度 * @param maxLevel 最大尺度 * @return 满足条件的特征点的序号 */ vector&lt;size_t&gt; Frame::GetFeaturesInArea(const float &amp;x, const float &amp;y, const float &amp;r, const int minLevel, const int maxLevel) const&#123; vector&lt;size_t&gt; vIndices; vIndices.reserve(N); // 预留所有的特征点 // 判断最小最大距离是否越界，并计算坐标点所在的格子坐标，先计算出圆所在的格子区域区间 const int nMinCellX = max(0,(int)floor((x-mnMinX-r)*mfGridElementWidthInv)); if(nMinCellX&gt;=FRAME_GRID_COLS) return vIndices; const int nMaxCellX = min((int)FRAME_GRID_COLS-1,(int)ceil((x-mnMinX+r)*mfGridElementWidthInv)); if(nMaxCellX&lt;0) return vIndices; const int nMinCellY = max(0,(int)floor((y-mnMinY-r)*mfGridElementHeightInv)); if(nMinCellY&gt;=FRAME_GRID_ROWS) return vIndices; const int nMaxCellY = min((int)FRAME_GRID_ROWS-1,(int)ceil((y-mnMinY+r)*mfGridElementHeightInv)); if(nMaxCellY&lt;0) return vIndices; const bool bCheckLevels = (minLevel&gt;0) || (maxLevel&gt;=0); for(int ix = nMinCellX; ix&lt;=nMaxCellX; ix++) &#123; for(int iy = nMinCellY; iy&lt;=nMaxCellY; iy++) &#123; const vector&lt;size_t&gt; vCell = mGrid[ix][iy]; // vCell 存的是该格子区域的所有特征点对应的MvKeysUn序号 if(vCell.empty()) continue; for(size_t j=0, jend=vCell.size(); j&lt;jend; j++) &#123; const cv::KeyPoint &amp;kpUn = mvKeysUn[vCell[j]]; if(bCheckLevels) &#123; if(kpUn.octave&lt;minLevel) // 判断该特征点的金字塔level是否满足要求 continue; if(maxLevel&gt;=0) if(kpUn.octave&gt;maxLevel) continue; &#125; const float distx = kpUn.pt.x-x; const float disty = kpUn.pt.y-y; if(fabs(distx)&lt;r &amp;&amp; fabs(disty)&lt;r) vIndices.push_back(vCell[j]); &#125; &#125; &#125; return vIndices;&#125; 2.9 计算当前帧的词包词典12345678void Frame::ComputeBoW()&#123; if(mBowVec.empty()) &#123; vector&lt;cv::Mat&gt; vCurrentDesc = Converter::toDescriptorVector(mDescriptors); mpORBvocabulary-&gt;transform(vCurrentDesc,mBowVec,mFeatVec,4); &#125;&#125; 2.10 调用OpenCV的矫正函数矫正orb提取的特征点1234567891011121314151617181920212223242526272829303132333435void Frame::UndistortKeyPoints()&#123; // 如果没有图像是矫正过的，没有失真 if(mDistCoef.at&lt;float&gt;(0)==0.0) &#123; mvKeysUn=mvKeys; return; &#125; // Fill matrix with points // N为提取的特征点数量，将N个特征点保存在N*2的mat中 cv::Mat mat(N,2,CV_32F); for(int i=0; i&lt;N; i++) &#123; mat.at&lt;float&gt;(i,0)=mvKeys[i].pt.x; mat.at&lt;float&gt;(i,1)=mvKeys[i].pt.y; &#125; // Undistort points // 调整mat的通道为2，矩阵的行列形状不变 mat=mat.reshape(2); cv::undistortPoints(mat,mat,mK,mDistCoef,cv::Mat(),mK); // 用cv的函数进行失真校正 mat=mat.reshape(1); // Fill undistorted keypoint vector // 存储校正后的特征点 mvKeysUn.resize(N); for(int i=0; i&lt;N; i++) &#123; cv::KeyPoint kp = mvKeys[i]; kp.pt.x=mat.at&lt;float&gt;(i,0); kp.pt.y=mat.at&lt;float&gt;(i,1); mvKeysUn[i]=kp; &#125;&#125; 2.11 计算图像边界123456789101112131415161718192021222324252627282930313233void Frame::ComputeImageBounds(const cv::Mat &amp;imLeft)&#123; if(mDistCoef.at&lt;float&gt;(0)!=0.0) &#123; // 矫正前四个边界点：(0,0) (cols,0) (0,rows) (cols,rows) cv::Mat mat(4,2,CV_32F); mat.at&lt;float&gt;(0,0)=0.0; //左上 mat.at&lt;float&gt;(0,1)=0.0; mat.at&lt;float&gt;(1,0)=imLeft.cols; //右上 mat.at&lt;float&gt;(1,1)=0.0; mat.at&lt;float&gt;(2,0)=0.0; //左下 mat.at&lt;float&gt;(2,1)=imLeft.rows; mat.at&lt;float&gt;(3,0)=imLeft.cols; //右下 mat.at&lt;float&gt;(3,1)=imLeft.rows; // Undistort corners mat=mat.reshape(2); cv::undistortPoints(mat,mat,mK,mDistCoef,cv::Mat(),mK); mat=mat.reshape(1); mnMinX = min(mat.at&lt;float&gt;(0,0),mat.at&lt;float&gt;(2,0));//左上和左下横坐标最小的 mnMaxX = max(mat.at&lt;float&gt;(1,0),mat.at&lt;float&gt;(3,0));//右上和右下横坐标最大的 mnMinY = min(mat.at&lt;float&gt;(0,1),mat.at&lt;float&gt;(1,1));//左上和右上纵坐标最小的 mnMaxY = max(mat.at&lt;float&gt;(2,1),mat.at&lt;float&gt;(3,1));//左下和右下纵坐标最小的 &#125; else &#123; mnMinX = 0.0f; mnMaxX = imLeft.cols; mnMinY = 0.0f; mnMaxY = imLeft.rows; &#125;&#125;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析八]]></title>
    <url>%2F2019%2F06%2F14%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%AB%2F</url>
    <content type="text"><![CDATA[MapPoint地图点1. MapPoint 成员变量1.1 MapPointId 地图点的ID，全局唯一，nNextId 静态计数12long unsigned int mnId; ///&lt; Global ID for MapPointstatic long unsigned int nNextId; 1.2 记录第一次创建该地图点的关键帧ID，以及帧ID12const long int mnFirstKFid; ///&lt; 创建该MapPoint的关键帧IDconst long int mnFirstFrame; ///&lt; 创建该MapPoint的帧ID（即每一关键帧有一个帧ID） 1.3 表示该地图点被观测的次数 nObs1int nObs; 1.4 该地图点被重投影到某一阵帧时的X坐标，Y坐标，以及在右视图中的坐标XR(双目匹配)，以及该地图点所对应的特征点被预测的尺度，以及该地图点在该帧下的观测向量与平均向量mNormalVector的夹角信息。123456// Variables used by the tracking float mTrackProjX; float mTrackProjY; float mTrackProjXR; int mnTrackScaleLevel; float mTrackViewCos; 1.5 该地图点是否需要进行重投影的标记变量，mbTrackInView = true表示需要进行重投影123456// TrackLocalMap - SearchByProjection中决定是否对该点进行投影的变量// mbTrackInView==false的点有几种：// a 已经和当前帧经过匹配（TrackReferenceKeyFrame，TrackWithMotionModel）但在优化过程中认为是外点// b 已经和当前帧经过匹配且为内点，这类点也不需要再进行投影// c 不在当前相机视野中的点（即未通过isInFrustum判断）bool mbTrackInView; 1.6 TrackLocalMap - UpdateLocalPoints中防止将MapPoints重复添加至mvpLocalMapPoints的参考帧标记避免重复添加局部地图点到更新的局部地图12// TrackLocalMap - UpdateLocalPoints中防止将MapPoints重复添加至mvpLocalMapPoints的标记 long unsigned int mnTrackReferenceForFrame; 1.7 TrackLocalMap - SearchLocalPoints中决定是否进行isInFrustum判断的变量，判断该点是否需要进行视野判断的变量12345// TrackLocalMap - SearchLocalPoints中决定是否进行isInFrustum判断的变量// mnLastFrameSeen==mCurrentFrame.mnId的点有几种：// a 已经和当前帧经过匹配（TrackReferenceKeyFrame，TrackWithMotionModel）但在优化过程中认为是外点// b 已经和当前帧经过匹配且为内点，这类点也不需要再进行投影long unsigned int mnLastFrameSeen; 1.8123// Variables used by local mapping long unsigned int mnBALocalForKF; long unsigned int mnFuseCandidateForKF; 1.9123456// Variables used by loop closinglong unsigned int mnLoopPointForKF;long unsigned int mnCorrectedByKF;long unsigned int mnCorrectedReference;cv::Mat mPosGBA;long unsigned int mnBAGlobalForKF; 1.10 MapPoint在世界坐标系下的坐标,地图点的坐标都是相对于世界坐标系，和当前帧无关，并没有存储局部坐标信息12// Position in absolute coordinatescv::Mat mWorldPos; ///&lt; MapPoint在世界坐标系下的坐标 1.11 观测到该MapPoint的KF和该MapPoint在KF中的索引12// Keyframes observing the point and associated index in keyframe std::map&lt;KeyFrame*,size_t&gt; mObservations; ///&lt; 观测到该MapPoint的KF和该MapPoint在KF中的索引 1.12 该MapPoint平均观测方向123// Mean viewing direction// 该MapPoint平均观测方向cv::Mat mNormalVector; 1.13 3D点的特征描述子12345// Best descriptor to fast matching// 每个3D点也有一个descriptor// 如果MapPoint与很多帧图像特征点对应（由keyframe来构造时），那么距离其它描述子的平均距离最小的描述子是最佳描述子// MapPoint只与一帧的图像特征点对应（由frame来构造时），那么这个特征点的描述子就是该3D点的描述子cv::Mat mDescriptor; ///&lt; 通过 ComputeDistinctiveDescriptors() 得到的最优描述子 1.14 参考关键帧12// Reference KeyFrame KeyFrame* mpRefKF; 1.15 跟踪计数，将局部地图关键帧和地图点重新跟踪投影时的计数信息123// Tracking counters int mnVisible; int mnFound; 1.16 坏的地图点标记，并没有显式的从内存中删除地图点123// Bad flag (we do not currently erase MapPoint from memory) bool mbBad; MapPoint* mpReplaced; 1.17 追踪时的地图点尺度信息123// Scale invariance distances float mfMinDistance; float mfMaxDistance; 1.18 该地图点所属于的地图1Map* mpMap; 2. 成员函数2.1 构造函数 MapPoint::MapPoint(const cv::Mat &amp;Pos, KeyFrame pRefKF, Map pMap)123//单目：CreateInitialMapMonocular()，LocalMapping::CreateNewMapPoints()// 通过世界坐标，关键帧，以及地图指针来构建新的地图点，初始化所有的变量，赋默认值，并令计算器自增，这种情况下是不知道地图点所对应的关键帧特征点，需要下一步重投影来计算MapPoint::MapPoint(const cv::Mat &amp;Pos, KeyFrame *pRefKF, Map* pMap); 2.2 构造函数 MapPoint::MapPoint(const cv::Mat &amp;Pos, Map pMap, Frame pFrame, const int &amp;idxF)12// 通过世界坐标，地图指针，图像帧，以及该地图点所对应的图像中的特征点idx来构建新的地图点，已经有了匹配信息，可以计算出相关变量的值MapPoint::MapPoint(const cv::Mat &amp;Pos, Map* pMap, Frame* pFrame, const int &amp;idxF) 123456// 计算这个点在当前帧的最大最小尺度距离mfMaxDistance = dist*levelScaleFactor;mfMinDistance = mfMaxDistance/pFrame-&gt;mvScaleFactors[nLevels-1];// 见mDescriptor在MapPoint.h中的注释，计算这个地图点在当前帧的描述符pFrame-&gt;mDescriptors.row(idxF).copyTo(mDescriptor); 2.3 构造函数 MapPoint::MapPoint(const cv::Mat &amp;Pos, Map pMap, Frame pFrame, const int &amp;idxF)12// 通过世界坐标，地图指针，图像帧，以及该地图点所对应的图像中的特征点idx来构建新的地图点，已经有了匹配信息，可以计算出相关变量的值MapPoint::MapPoint(const cv::Mat &amp;Pos, Map* pMap, Frame* pFrame, const int &amp;idxF) 2.4 set和get世界坐标12void SetWorldPos(const cv::Mat &amp;Pos);cv::Mat GetWorldPos(); 2.5 get平均观测向量12345cv::Mat MapPoint::GetNormal()&#123; unique_lock&lt;mutex&gt; lock(mMutexPos); return mNormalVector.clone();&#125; 2.6 get参考关键帧12345KeyFrame* MapPoint::GetReferenceKeyFrame()&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); return mpRefKF;&#125; 2.7 添加观测信息到 mObservations 这个 map12345678910111213void MapPoint::AddObservation(KeyFrame* pKF, size_t idx)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); if(mObservations.count(pKF)) // 如果键值已存在，直接返回 return; // 记录下能观测到该MapPoint的KF和该MapPoint在KF中的索引 mObservations[pKF]=idx; if(pKF-&gt;mvuRight[idx]&gt;=0) nObs+=2; // 双目或者grbd else nObs++; // 单目，让该点的观测计数加1&#125; 2.8 从mObservations 这个 map 删除观测信息 当删除观测信息时，会检测该点是否变为坏点，会重新设置badFlag1234567891011121314151617181920212223242526272829void MapPoint::EraseObservation(KeyFrame* pKF)&#123; bool bBad=false; // 默认该点不是坏点 &#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); if(mObservations.count(pKF)) &#123; int idx = mObservations[pKF]; if(pKF-&gt;mvuRight[idx]&gt;=0) nObs-=2; else nObs--; // 让观测计数建减1 mObservations.erase(pKF); // 从mObservations 这个 map删除该关键帧 // 如果该keyFrame是参考帧，该Frame被删除后重新指定RefFrame if(mpRefKF==pKF) mpRefKF=mObservations.begin()-&gt;first; // If only 2 observations or less, discard point // 当观测到该点的相机数目少于2时，丢弃该点 if(nObs&lt;=2) bBad=true; &#125; &#125; if(bBad) SetBadFlag();&#125; 2.9 GetObservations 返回观测信息 map12345map&lt;KeyFrame*, size_t&gt; MapPoint::GetObservations()&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); return mObservations;&#125; 2.10 返回观测计数 obs1int MapPoint::Observations() 2.11 SetBadFlag 告知可以观测到该MapPoint的Frame，该MapPoint已被删除12345678910111213141516171819// 告知可以观测到该MapPoint的Frame，该MapPoint已被删除void MapPoint::SetBadFlag()&#123; map&lt;KeyFrame*,size_t&gt; obs; &#123; unique_lock&lt;mutex&gt; lock1(mMutexFeatures); unique_lock&lt;mutex&gt; lock2(mMutexPos); mbBad=true; obs = mObservations;// 把mObservations转存到obs，obs和mObservations里存的是指针，赋值过程为浅拷贝 mObservations.clear();// 把mObservations指向的内存释放，obs作为局部变量之后自动删除 &#125; for(map&lt;KeyFrame*,size_t&gt;::iterator mit=obs.begin(), mend=obs.end(); mit!=mend; mit++) &#123; KeyFrame* pKF = mit-&gt;first; pKF-&gt;EraseMapPointMatch(mit-&gt;second);// 告诉可以观测到该MapPoint的KeyFrame，该MapPoint被删了 &#125; mpMap-&gt;EraseMapPoint(this);// 擦除该MapPoint申请的内存&#125; 2.12 返回被替换的地图点123456MapPoint* MapPoint::GetReplaced()&#123; unique_lock&lt;mutex&gt; lock1(mMutexFeatures); unique_lock&lt;mutex&gt; lock2(mMutexPos); return mpReplaced;&#125; 2.13 闭环更新12345678910111213141516171819202122232425262728293031323334353637383940414243// 在形成闭环的时候，会更新KeyFrame与MapPoint之间的关系void MapPoint::Replace(MapPoint* pMP)&#123; if(pMP-&gt;mnId==this-&gt;mnId) return; int nvisible, nfound; map&lt;KeyFrame*,size_t&gt; obs;// 这一段和SetBadFlag函数相同 &#123; unique_lock&lt;mutex&gt; lock1(mMutexFeatures); unique_lock&lt;mutex&gt; lock2(mMutexPos); obs=mObservations; mObservations.clear(); mbBad=true; nvisible = mnVisible; nfound = mnFound; mpReplaced = pMP; &#125; // 所有能观测到该MapPoint的keyframe都要替换 for(map&lt;KeyFrame*,size_t&gt;::iterator mit=obs.begin(), mend=obs.end(); mit!=mend; mit++) &#123; // Replace measurement in keyframe KeyFrame* pKF = mit-&gt;first; if(!pMP-&gt;IsInKeyFrame(pKF)) &#123; pKF-&gt;ReplaceMapPointMatch(mit-&gt;second, pMP);// 让KeyFrame用pMP替换掉原来的MapPoint pMP-&gt;AddObservation(pKF,mit-&gt;second);// 让MapPoint替换掉对应的KeyFrame &#125; else &#123; // 产生冲突，即pKF中有两个特征点a,b（这两个特征点的描述子是近似相同的），这两个特征点对应两个MapPoint为this,pMP // 然而在fuse的过程中pMP的观测更多，需要替换this，因此保留b与pMP的联系，去掉a与this的联系 pKF-&gt;EraseMapPointMatch(mit-&gt;second); &#125; &#125; pMP-&gt;IncreaseFound(nfound); pMP-&gt;IncreaseVisible(nvisible); pMP-&gt;ComputeDistinctiveDescriptors(); mpMap-&gt;EraseMapPoint(this);&#125; 2.14 检测该点是否是一个坏点123456bool MapPoint::isBad()&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); unique_lock&lt;mutex&gt; lock2(mMutexPos); return mbBad;&#125; 2.15 增加该地图点的可视性，可以观察到，但不一定能够匹配到特征点1234567891011121314/** * @brief Increase Visible * * Visible表示： * 1. 该MapPoint在某些帧的视野范围内，通过Frame::isInFrustum()函数判断 * 2. 该MapPoint被这些帧观测到，但并不一定能和这些帧的特征点匹配上 * 例如：有一个MapPoint（记为M），在某一帧F的视野范围内， * 但并不表明该点M可以和F这一帧的某个特征点能匹配上 */void MapPoint::IncreaseVisible(int n)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); mnVisible+=n;&#125; 2.16 能找到该地图点的帧数加1，可以观察到该地图点，同时可以匹配到某一帧的特征点1234567891011/** * @brief Increase Found * * 能找到该点的帧数+n，n默认为1 * @see Tracking::TrackLocalMap() */void MapPoint::IncreaseFound(int n)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); mnFound+=n;&#125; 2.17 增加该地图点的可视性1234567891011121314/** * @brief Increase Visible * * Visible表示： * 1. 该MapPoint在某些帧的视野范围内，通过Frame::isInFrustum()函数判断 * 2. 该MapPoint被这些帧观测到，但并不一定能和这些帧的特征点匹配上 * 例如：有一个MapPoint（记为M），在某一帧F的视野范围内， * 但并不表明该点M可以和F这一帧的某个特征点能匹配上 */void MapPoint::IncreaseVisible(int n)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); mnVisible+=n;&#125; 2.18 返回匹配性与观测性的比例12345float MapPoint::GetFoundRatio()&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); return static_cast&lt;float&gt;(mnFound)/mnVisible;&#125; 2.19 计算具有代表的描述子，然后计算描述子之间的两两距离，最好的描述子与其他描述子应该具有最小的距离中值1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/** * @brief 计算具有代表的描述子 * * 由于一个MapPoint会被许多相机观测到，因此在插入关键帧后，需要判断是否更新当前点的最适合的描述子 \n * 先获得当前点的所有描述子，然后计算描述子之间的两两距离，最好的描述子与其他描述子应该具有最小的距离中值 * @see III - C3.3 */void MapPoint::ComputeDistinctiveDescriptors()&#123; // Retrieve all observed descriptors vector&lt;cv::Mat&gt; vDescriptors; map&lt;KeyFrame*,size_t&gt; observations; &#123; unique_lock&lt;mutex&gt; lock1(mMutexFeatures); if(mbBad) return; observations=mObservations; &#125; if(observations.empty()) return; vDescriptors.reserve(observations.size()); // 遍历观测到3d点的所有关键帧，获得orb描述子，并插入到vDescriptors中 for(map&lt;KeyFrame*,size_t&gt;::iterator mit=observations.begin(), mend=observations.end(); mit!=mend; mit++) &#123; KeyFrame* pKF = mit-&gt;first; if(!pKF-&gt;isBad()) vDescriptors.push_back(pKF-&gt;mDescriptors.row(mit-&gt;second)); &#125; if(vDescriptors.empty()) return; // Compute distances between them // 获得这些描述子两两之间的距离 const size_t N = vDescriptors.size(); //float Distances[N][N]; std::vector&lt;std::vector&lt;float&gt; &gt; Distances; Distances.resize(N, vector&lt;float&gt;(N, 0)); for (size_t i = 0; i&lt;N; i++) &#123; Distances[i][i]=0; for(size_t j=i+1;j&lt;N;j++) &#123; int distij = ORBmatcher::DescriptorDistance(vDescriptors[i],vDescriptors[j]); Distances[i][j]=distij; Distances[j][i]=distij; &#125; &#125; // Take the descriptor with least median distance to the rest int BestMedian = INT_MAX; int BestIdx = 0; for(size_t i=0;i&lt;N;i++) &#123; // 第i个描述子到其它所有所有描述子之间的距离 //vector&lt;int&gt; vDists(Distances[i],Distances[i]+N); vector&lt;int&gt; vDists(Distances[i].begin(), Distances[i].end()); sort(vDists.begin(), vDists.end()); // 获得中值 int median = vDists[0.5*(N-1)]; // 寻找最小的中值 if(median&lt;BestMedian) &#123; BestMedian = median; BestIdx = i; &#125; &#125; &#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); // 最好的描述子，该描述子相对于其他描述子有最小的距离中值 // 简化来讲，中值代表了这个描述子到其它描述子的平均距离 // 最好的描述子就是和其它描述子的平均距离最小 mDescriptor = vDescriptors[BestIdx].clone(); &#125;&#125; 2.20 GetDescriptor()获得mapPoint的描述符12345cv::Mat MapPoint::GetDescriptor()&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); return mDescriptor.clone();&#125; 2.21 返回该mapPoint所对应的特征点在指定关键帧的idx12345678int MapPoint::GetIndexInKeyFrame(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); if(mObservations.count(pKF)) return mObservations[pKF]; else return -1;&#125; 2.22 检查该地图点是否是某一帧图像的地图点12345bool MapPoint::IsInKeyFrame(KeyFrame *pKF)&#123; unique_lock&lt;mutex&gt; lock(mMutexFeatures); return (mObservations.count(pKF));&#125; 2.23 更新平均观测方向以及观测距离范围1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @brief 更新平均观测方向以及观测距离范围 * * 由于一个MapPoint会被许多相机观测到，因此在插入关键帧后，需要更新相应变量 * @see III - C2.2 c2.4 */void MapPoint::UpdateNormalAndDepth()&#123; map&lt;KeyFrame*,size_t&gt; observations; KeyFrame* pRefKF; cv::Mat Pos; &#123; unique_lock&lt;mutex&gt; lock1(mMutexFeatures); unique_lock&lt;mutex&gt; lock2(mMutexPos); if(mbBad) return; observations=mObservations; // 获得观测到该3d点的所有关键帧 pRefKF=mpRefKF; // 观测到该点的参考关键帧 Pos = mWorldPos.clone(); // 3d点在世界坐标系中的位置 &#125; if(observations.empty()) return; cv::Mat normal = cv::Mat::zeros(3,1,CV_32F); int n=0; for(map&lt;KeyFrame*,size_t&gt;::iterator mit=observations.begin(), mend=observations.end(); mit!=mend; mit++) &#123; KeyFrame* pKF = mit-&gt;first; cv::Mat Owi = pKF-&gt;GetCameraCenter(); cv::Mat normali = mWorldPos - Owi; normal = normal + normali/cv::norm(normali); // 对所有关键帧对该点的观测方向归一化为单位向量进行求和 n++; &#125; cv::Mat PC = Pos - pRefKF-&gt;GetCameraCenter(); // 参考关键帧相机指向3D点的向量（在世界坐标系下的表示） const float dist = cv::norm(PC); // 该点到参考关键帧相机的距离 const int level = pRefKF-&gt;mvKeysUn[observations[pRefKF]].octave; const float levelScaleFactor = pRefKF-&gt;mvScaleFactors[level]; const int nLevels = pRefKF-&gt;mnScaleLevels; // 金字塔层数 &#123; unique_lock&lt;mutex&gt; lock3(mMutexPos); // 另见PredictScale函数前的注释 mfMaxDistance = dist*levelScaleFactor; // 观测到该点的距离下限 mfMinDistance = mfMaxDistance/pRefKF-&gt;mvScaleFactors[nLevels-1]; // 观测到该点的距离上限 mNormalVector = normal/n; // 获得新的平均的观测方向 &#125;&#125; 2.24 获得最小最大距离的方差1234567891011float MapPoint::GetMinDistanceInvariance()&#123; unique_lock&lt;mutex&gt; lock(mMutexPos); return 0.8f*mfMinDistance;&#125;float MapPoint::GetMaxDistanceInvariance()&#123; unique_lock&lt;mutex&gt; lock(mMutexPos); return 1.2f*mfMaxDistance;&#125; 2.25 预测当前地图点相对于参考帧的金字塔尺度12345678910111213141516171819202122232425262728293031323334353637383940414243444546// ____// Nearer /____\ level:n-1 --&gt; dmin// /______\ d/dmin = 1.2^(n-1-m)// /________\ level:m --&gt; d// /__________\ dmax/d = 1.2^m// Farther /____________\ level:0 --&gt; dmax//// log(dmax/d)// m = ceil(------------)// log(1.2)int MapPoint::PredictScale(const float &amp;currentDist, KeyFrame* pKF)&#123; float ratio; &#123; unique_lock&lt;mutex&gt; lock(mMutexPos); // mfMaxDistance = ref_dist*levelScaleFactor为参考帧考虑上尺度后的距离 // ratio = mfMaxDistance/currentDist = ref_dist/cur_dist ratio = mfMaxDistance/currentDist; &#125; // 同时取log线性化 int nScale = ceil(log(ratio)/pKF-&gt;mfLogScaleFactor); if(nScale&lt;0) nScale = 0; else if(nScale&gt;=pKF-&gt;mnScaleLevels) nScale = pKF-&gt;mnScaleLevels-1; return nScale;&#125;int MapPoint::PredictScale(const float &amp;currentDist, Frame* pF)&#123; float ratio; &#123; unique_lock&lt;mutex&gt; lock(mMutexPos); ratio = mfMaxDistance/currentDist; &#125; int nScale = ceil(log(ratio)/pF-&gt;mfLogScaleFactor); if(nScale&lt;0) nScale = 0; else if(nScale&gt;=pF-&gt;mnScaleLevels) nScale = pF-&gt;mnScaleLevels-1; return nScale;&#125;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[相机矩阵的Tips]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%9B%B8%E6%9C%BA%E7%9F%A9%E9%98%B5%E7%9A%84tips%2F</url>
    <content type="text"><![CDATA[关于相机矩阵以及坐标系变换的Tips绕原点逆时针旋转$\theta$角度的旋转方程为 x' = \begin{bmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta \end{bmatrix} x设$T_{cw}$表示从世界坐标系到摄像机坐标系的变换矩阵，也就是说，对于齐次坐标： \begin{array}{c} \begin{bmatrix} P_c & 1 \end{bmatrix} = T_{cw} * { \begin{bmatrix} P_w & 1 \end{bmatrix} } \\ \begin{bmatrix} P_w & 1 \end{bmatrix} = T_{wc} * { \begin{bmatrix} P_c & 1 \end{bmatrix} } \end{array}对标准坐标来说，分解成旋转矩阵和平移向量也就是 \begin{array}{c} P_c = R_{cw} * P_w + t_{cw} \\ P_w = R_{wc} * P_c + t_{wc} \end{array}$R_{cw}$表示$T_{cw}$的左上角$3 \times 3$的矩阵，即1Rcw = Tcw.rowRange(0,3).colRange(0,3); $t_{cw}$表示$T_{cw}$的右上角$3 \times 1$的矩阵，即1tcw = Tcw.rowRange(0,3).col(3); 平移向量$T_{cw}$表示在相机坐标系中，相机坐标系到世界坐标系间的向量，向量方向由相机坐标系指向世界坐标系，即相机坐标系中，世界坐标系的原点坐标。旋转矩阵R中的角度表示平移完成后，从相机坐标系逆时针旋转至世界坐标系的角度。以二维坐标旋转为例，如下图所示 反过来，$R_{wc}$表示坐标从相机坐标系转换世界坐标系的旋转矩阵，$R_{wc}=R_{cw}^T=R_{cw}^{-1}$1Rwc = Rcw.t(); $t_{wc}$也记作$Ow$，表示世界坐标系下世界坐标系到相机坐标系间的向量, 向量方向由世界坐标系指向相机坐标系，即世界坐标系下，相机中心点的坐标。$t_{wc} = - R_{cw}^T \ast t_{cw}$ 1Ow = -Rcw.t()*tcw;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>Camera</tag>
        <tag>Matrix</tag>
        <tag>System Transform</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析七]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83%2F</url>
    <content type="text"><![CDATA[track()线程主函数详解此时track()函数已经完成了初始化，mstate = OK，mpInitializer已经初始化，track()函数进入else分支 1. 正常初始化 mState==OK1.1 检查并更新上一帧被替换的MapPoints，Local Mapping线程可能会修改最后一帧图像的地图点1234// Local Mapping might have changed some MapPoints tracked in last frame // 检查并更新上一帧被替换的MapPoints // 更新Fuse函数和SearchAndFuse函数替换的MapPoints CheckReplacedInLastFrame(); 1.2 根据参考关键帧进行跟踪 TrackReferenceKeyFrame()123456789101112// 运动模型是空的或刚完成重定位// mCurrentFrame.mnId&lt;mnLastRelocFrameId+2这个判断不应该有???// 源代码注释说不应该有后面这个判断，我认为有必要，如果刚进行重定位，此时mVelocity不为空// 应该只要mVelocity不为空，就优先选择TrackWithMotionModel// mnLastRelocFrameId上一次重定位的那一帧if(mVelocity.empty() || mCurrentFrame.mnId&lt;mnLastRelocFrameId+2)&#123; // 将上一帧的位姿作为当前帧的初始位姿 // 通过BoW的方式在参考帧中找当前帧特征点的匹配点 // 优化每个特征点都对应3D点重投影误差即可得到位姿 bOK = TrackReferenceKeyFrame();&#125; 1.3 根据恒速模型和最后一帧图像进行跟踪 TrackWithMotionModel()，如果恒速模型不成功，再根据参考关键帧进行跟踪1234567891011else &#123; // 根据恒速模型设定当前帧的初始位姿 // 通过投影的方式在参考帧中找当前帧特征点的匹配点 // 优化每个特征点所对应3D点的投影误差即可得到位姿 bOK = TrackWithMotionModel(); if(!bOK) // TrackReferenceKeyFrame是跟踪参考帧，不能根据固定运动速度模型预测当前帧的位姿态，通过bow加速匹配（SearchByBow） // 最后通过优化得到优化后的位姿 bOK = TrackReferenceKeyFrame(); &#125; 2. 初始化完成后，特征点跟踪丢失，mState！=OK,此时进行重定位12345else &#123; // BOW搜索，PnP求解位姿 bOK = Relocalization(); &#125; 3. 跟踪完当前帧后，需要进行更新信息3.1 更新当前帧的参考关键帧为当前参考关键帧12// 将最新的关键帧作为reference frame mCurrentFrame.mpReferenceKF = mpReferenceKF; 3.2 跟踪局部地图 详细分解参照源码分析二1234567891011// If we have an initial estimation of the camera pose and matching. Track the local map.// 步骤2.2：在帧间匹配得到初始的姿态后，现在对local map进行跟踪得到更多的匹配，并优化当前位姿// local map:当前帧、当前帧的MapPoints、当前关键帧与其它关键帧共视关系// 在步骤2.1中主要是两两跟踪（恒速模型跟踪上一帧、跟踪参考帧），这里搜索局部关键帧后搜集所有局部MapPoints，// 然后将局部MapPoints和当前帧进行投影匹配，得到更多匹配的MapPoints后进行Pose优化if(!mbOnlyTracking)&#123; if(bOK) bOK = TrackLocalMap();&#125; 3.3 跟踪局部地图完成，mstate = OK1234if(bOK) mState = OK;else mState=LOST; 3.4 更新Motion模型计算mVelocity，计算当前帧相对于最后一帧的相对位姿 \begin{array}{l} X_l = R_{lw}X_w+t_{lw}=T_{lw}X_w \\ X_c = R_{cw}X_w+t_{cw}=T_{cw}X_w = T_{cw}T_{lw}^{-1}X_l \end{array}1234567891011// Update motion model if(!mLastFrame.mTcw.empty()) &#123; // 步骤2.3：更新恒速运动模型TrackWithMotionModel中的mVelocity cv::Mat LastTwc = cv::Mat::eye(4,4,CV_32F); mLastFrame.GetRotationInverse().copyTo(LastTwc.rowRange(0,3).colRange(0,3)); mLastFrame.GetCameraCenter().copyTo(LastTwc.rowRange(0,3).col(3)); mVelocity = mCurrentFrame.mTcw*LastTwc; // Tcl &#125; else mVelocity = cv::Mat(); 3.5 清楚VO matches 清除UpdateLastFrame中为当前帧临时添加的MapPoints12345678910111213// Clean VO matches // 步骤2.4：清除UpdateLastFrame中为当前帧临时添加的MapPoints for(int i=0; i&lt;mCurrentFrame.N; i++) &#123; MapPoint* pMP = mCurrentFrame.mvpMapPoints[i]; if(pMP) // 排除UpdateLastFrame函数中为了跟踪增加的MapPoints if(pMP-&gt;Observations()&lt;1) &#123; mCurrentFrame.mvbOutlier[i] = false; mCurrentFrame.mvpMapPoints[i]=static_cast&lt;MapPoint*&gt;(NULL); &#125; &#125; 3.6 从地图中清除地图点1234567891011// Delete temporal MapPoints // 步骤2.5：清除临时的MapPoints，这些MapPoints在TrackWithMotionModel的UpdateLastFrame函数里生成（仅双目和rgbd） // 步骤2.4中只是在当前帧中将这些MapPoints剔除，这里从MapPoints数据库中删除 // 这里生成的仅仅是为了提高双目或rgbd摄像头的帧间跟踪效果，用完以后就扔了，没有添加到地图中 for(list&lt;MapPoint*&gt;::iterator lit = mlpTemporalPoints.begin(), lend = mlpTemporalPoints.end(); lit!=lend; lit++) &#123; MapPoint* pMP = *lit; delete pMP; &#125; // 这里不仅仅是清除mlpTemporalPoints，通过delete pMP还删除了指针指向的MapPoint mlpTemporalPoints.clear(); 3.7 检测是否需要插入关键帧1234// Check if we need to insert a new keyframe // 步骤2.6：检测并插入关键帧，对于双目会产生新的MapPoints if(NeedNewKeyFrame()) CreateNewKeyFrame(); 3.8 删除那些在bundle adjustment中检测为outlier的3D map点12345678910// We allow points with high innovation (considererd outliers by the Huber Function) // pass to the new keyframe, so that bundle adjustment will finally decide // if they are outliers or not. We don't want next frame to estimate its position // with those points so we discard them in the frame. // 删除那些在bundle adjustment中检测为outlier的3D map点 for(int i=0; i&lt;mCurrentFrame.N;i++) &#123; if(mCurrentFrame.mvpMapPoints[i] &amp;&amp; mCurrentFrame.mvbOutlier[i]) mCurrentFrame.mvpMapPoints[i]=static_cast&lt;MapPoint*&gt;(NULL); &#125; 3.9 判断跟踪状态，如果跟踪丢失，则reset1234567891011// Reset if the camera get lost soon after initialization // 跟踪失败，并且relocation也没有搞定，只能重新Reset if(mState==LOST) &#123; if(mpMap-&gt;KeyFramesInMap()&lt;=5) &#123; cout &lt;&lt; "Track lost soon after initialisation, reseting..." &lt;&lt; endl; mpSystem-&gt;Reset(); return; &#125; &#125; 3.10 保存上一帧图像的数据，往下传递跟踪12345if(!mCurrentFrame.mpReferenceKF) mCurrentFrame.mpReferenceKF = mpReferenceKF; // 保存上一帧的数据 mLastFrame = Frame(mCurrentFrame); 3.11 记录位姿信息，用于轨迹复现1234567891011121314151617181920// Store frame pose information to retrieve the complete camera trajectory afterwards.// 步骤3：记录位姿信息，用于轨迹复现if(!mCurrentFrame.mTcw.empty())&#123; // 计算相对姿态T_currentFrame_referenceKeyFrame cv::Mat Tcr = mCurrentFrame.mTcw*mCurrentFrame.mpReferenceKF-&gt;GetPoseInverse(); mlRelativeFramePoses.push_back(Tcr); mlpReferences.push_back(mpReferenceKF); mlFrameTimes.push_back(mCurrentFrame.mTimeStamp); mlbLost.push_back(mState==LOST);&#125;else&#123; // This can happen if tracking is lost // 如果跟踪失败，则相对位姿使用上一次值 mlRelativeFramePoses.push_back(mlRelativeFramePoses.back()); mlpReferences.push_back(mlpReferences.back()); mlFrameTimes.push_back(mlFrameTimes.back()); mlbLost.push_back(mState==LOST);&#125;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析六]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%AD%2F</url>
    <content type="text"><![CDATA[track()线程初始化过程详解Tracking.cpp里面的Track()函数可以说是 Tracking 线程最主体部分了，来看看调用Track()函数的过程： 1. 初始化初始器 mpInitializer1.1 第一次进Track()函数，处理系统的第一帧图像,此时state = NO_IMAGE_YET，赋值为state = NOT_INITIALIZED12345678910111213// track包含两部分：估计运动、跟踪局部地图// mState为tracking的状态机// SYSTME_NOT_READY, NO_IMAGE_YET, NOT_INITIALIZED, OK, LOST// 如果图像复位过、或者第一次运行，则为NO_IMAGE_YET状态if(mState==NO_IMAGES_YET)&#123; mState = NOT_INITIALIZED;&#125;// mLastProcessedState存储了Tracking最新的状态，用于FrameDrawer中的绘制mLastProcessedState=mState; 1.2 接下来 mState==NOT_INITIALIZED 执行単目初始化过程，进入MonocularInitialization();123456789101112if(mState==NOT_INITIALIZED)&#123; if(mSensor==System::STEREO || mSensor==System::RGBD) StereoInitialization(); else MonocularInitialization(); mpFrameDrawer-&gt;Update(this); if(mState!=OK) return;&#125; 1.3 此时単目初始器还没有建立，mpInitializer=nullptr,进入MonocularInitialization()函数if分支，需要保证单目初始帧的特征点数必须大于100123456789// 步骤1：得到用于初始化的第一帧，初始化需要两帧，处理第一帧时，mInitialFrame和mLastFrame都等于第一帧，也就是当前帧 mInitialFrame = Frame(mCurrentFrame); // 记录最近的一帧 mLastFrame = Frame(mCurrentFrame); // mvbPrevMatched最大的情况就是所有特征点都被跟踪上 mvbPrevMatched.resize(mCurrentFrame.mvKeysUn.size()); for(size_t i=0; i&lt;mCurrentFrame.mvKeysUn.size(); i++) mvbPrevMatched[i]=mCurrentFrame.mvKeysUn[i].pt; 1.4 构造初始器 mpInitializer12// 由当前帧构造初始器 sigma:1.0 iterations:200 mpInitializer = new Initializer(mCurrentFrame,1.0,200); 填充匹配信息为未匹配，值为-11fill(mvIniMatches.begin(),mvIniMatches.end(),-1); 此时函数返回，mState仍然为NOT_INITIALIZED 1.5 mstate = NOT_INITIALIZED,mpInitializer已经初始化，开始处理第二帧图像接下来处理第二帧图像，同样进入MonocularInitialization();此时単目初始器已经初始化，走MonocularInitialization()函数else过程。同样需要保证第二帧图像的特征点数大于1001234567891011// Try to initialize // 步骤2：如果当前帧特征点数大于100，则得到用于单目初始化的第二帧 // 如果当前帧特征点太少，重新构造初始器 // 因此只有连续两帧的特征点个数都大于100时，才能继续进行初始化过程 if((int)mCurrentFrame.mvKeys.size()&lt;=100) &#123; delete mpInitializer; mpInitializer = static_cast&lt;Initializer*&gt;(NULL); fill(mvIniMatches.begin(),mvIniMatches.end(),-1); return; &#125; 1.6 建立匹配器，在mInitialFrame帧和mCurrentFrame帧之间进行特征点匹配，返回匹配的特征点个数123456789// Find correspondences // 步骤3：在mInitialFrame与mCurrentFrame中找匹配的特征点对 // mvbPrevMatched为前一帧的特征点，存储了mInitialFrame中哪些点将进行接下来的匹配 // mvIniMatches存储mInitialFrame,mCurrentFrame之间匹配的特征点 // mvbPrevMatched为前一帧的特征点，存储了mInitialFrame中哪些点将进行接下来的匹配 // mvIniMatches存储mInitialFrame,mCurrentFrame之间匹配的特征点,键值对是两帧匹配特征点的索引 // mvbPrevMatched，mvIniMatches获得更新 ORBmatcher matcher(0.9,true); int nmatches = matcher.SearchForInitialization(mInitialFrame,mCurrentFrame,mvbPrevMatched,mvIniMatches,100); 检测匹配点的个数是否满足要求，如果初始两帧图像之间的匹配点太少，则重新初始化12345678// Check if there are enough correspondences // 步骤4：如果初始化的两帧之间的匹配点太少，重新初始化 if(nmatches&lt;100) &#123; delete mpInitializer; mpInitializer = static_cast&lt;Initializer*&gt;(NULL); return; &#125; 1.7 建立匹配器，在mInitialFrame帧和mCurrentFrame帧之间进行特征点匹配，返回匹配的特征点个数mCurrentFrame, mvIniMatches是传入的初始化参数如果初始化成功，intializer得到Rcw, tcw，mvIniP3D, vbTriangulated。12// 步骤5：通过H模型或F模型进行单目初始化，得到两帧间相对运动、初始MapPointsif(mpInitializer-&gt;Initialize(mCurrentFrame, mvIniMatches, Rcw, tcw, mvIniP3D, vbTriangulated)) 1.8 删除那些无法三角化的匹配点123456789// 步骤6：删除那些无法进行三角化的匹配点 for(size_t i=0, iend=mvIniMatches.size(); i&lt;iend;i++) &#123; if(mvIniMatches[i]&gt;=0 &amp;&amp; !vbTriangulated[i]) &#123; mvIniMatches[i]=-1; nmatches--; &#125; &#125; 1.9 设置第一帧为世界坐标系，并建立第二帧的位姿12345678// Set Frame Poses // 将初始化的第一帧作为世界坐标系，因此第一帧变换矩阵为单位矩阵 mInitialFrame.SetPose(cv::Mat::eye(4,4,CV_32F)); // 由Rcw和tcw构造Tcw,并赋值给mTcw，mTcw为世界坐标系到该帧的变换矩阵 cv::Mat Tcw = cv::Mat::eye(4,4,CV_32F); Rcw.copyTo(Tcw.rowRange(0,3).colRange(0,3)); tcw.copyTo(Tcw.rowRange(0,3).col(3)); mCurrentFrame.SetPose(Tcw); 说明:Frame.SetPose函数，将相对于参考帧的Tcw复制到当前帧的Tcw,并更新世界坐标系矩阵中的一些变量12345void Frame::SetPose(cv::Mat Tcw)&#123; mTcw = Tcw.clone(); UpdatePoseMatrices();&#125; 1234567891011void Frame::UpdatePoseMatrices()&#123; // [x_camera 1] = [R|t]*[x_world 1]，坐标为齐次形式 // x_camera = R*x_world + t mRcw = mTcw.rowRange(0,3).colRange(0,3); mRwc = mRcw.t(); mtcw = mTcw.rowRange(0,3).col(3); // mtcw, 即相机坐标系下相机坐标系到世界坐标系间的向量, 向量方向由相机坐标系指向世界坐标系 // mOw, 即世界坐标系下世界坐标系到相机坐标系间的向量, 向量方向由世界坐标系指向相机坐标系 mOw = -mRcw.t()*mtcw;&#125; 2. 建立初始局部地图 CreateInitialMapMonocular()12345// 步骤6：将三角化得到的3D点包装成MapPoints // Initialize函数会得到mvIniP3D， // mvIniP3D是cv::Point3f类型的一个容器，是个存放3D点的临时变量， // CreateInitialMapMonocular将3D点包装成MapPoint类型存入KeyFrame和Map中 CreateInitialMapMonocular(); 2.1 将第一帧图像和第二帧图像都设为关键帧12KeyFrame* pKFini = new KeyFrame(mInitialFrame,mpMap,mpKeyFrameDB);KeyFrame* pKFcur = new KeyFrame(mCurrentFrame,mpMap,mpKeyFrameDB); 2.2 将第一帧和第二帧图像的描述子都转换为Bow1234// 步骤1：将初始关键帧的描述子转为BoWpKFini-&gt;ComputeBoW();// 步骤2：将当前关键帧的描述子转为BoWpKFcur-&gt;ComputeBoW(); 2.3 将关键帧插入全局地图12345// Insert KFs in the map// 步骤3：将关键帧插入到地图// 凡是关键帧，都要插入地图mpMap-&gt;AddKeyFrame(pKFini);mpMap-&gt;AddKeyFrame(pKFcur); 2.4 生成地图点并和关键帧相关联123456789101112131415161718192021222324252627282930313233343536373839// Create MapPoints and asscoiate to keyframes// 步骤4：将3D点包装成MapPointsfor(size_t i=0; i&lt;mvIniMatches.size();i++)&#123; if(mvIniMatches[i]&lt;0) continue; //Create MapPoint. cv::Mat worldPos(mvIniP3D[i]); // 步骤4.1：用3D点构造MapPoint MapPoint* pMP = new MapPoint(worldPos,pKFcur,mpMap); // 步骤4.2：为该MapPoint添加属性： // a.观测到该MapPoint的关键帧 // b.该MapPoint的描述子 // c.该MapPoint的平均观测方向和深度范围 // 步骤4.3：表示该KeyFrame的哪个特征点可以观测到哪个3D点 pKFini-&gt;AddMapPoint(pMP,i); pKFcur-&gt;AddMapPoint(pMP,mvIniMatches[i]); // a.表示该MapPoint可以被哪个KeyFrame的哪个特征点观测到 pMP-&gt;AddObservation(pKFini,i); pMP-&gt;AddObservation(pKFcur,mvIniMatches[i]); // b.从众多观测到该MapPoint的特征点中挑选区分度最高的描述子 pMP-&gt;ComputeDistinctiveDescriptors(); // c.更新该MapPoint平均观测方向以及观测距离的范围 pMP-&gt;UpdateNormalAndDepth(); //Fill Current Frame structure mCurrentFrame.mvpMapPoints[mvIniMatches[i]] = pMP; mCurrentFrame.mvbOutlier[mvIniMatches[i]] = false; //Add to Map // 步骤4.4：在地图中添加该MapPoint mpMap-&gt;AddMapPoint(pMP);&#125; Create MapPoint()说明:12345//Create MapPoint. cv::Mat worldPos(mvIniP3D[i]); // 步骤4.1：用3D点构造MapPoint MapPoint* pMP = new MapPoint(worldPos,pKFcur,mpMap); 创建地图点的过程Pos存储地图点的世界坐标系坐标，mNormalVector存储世界坐标系下相机到3D点的单位向量，dist表示相机中心到世界坐标系原点的距离// mtcw, 即相机坐标系下相机坐标系到世界坐标系间的向量, 向量方向由相机坐标系指向世界坐标系，可以理解为在相机坐标系下，世界坐标系原点的坐标// mOw, 即世界坐标系下世界坐标系到相机坐标系间的向量, 向量方向由世界坐标系指向相机坐标系，可以理解为在世界坐标系下，相机中心的坐标 12345678910// ____// Nearer /____\ level:n-1 --&gt; dmin// /______\ d/dmin = 1.2^(n-1-m)// /________\ level:m --&gt; d// /__________\ dmax/d = 1.2^m// Farther /____________\ level:0 --&gt; dmax//// log(dmax/d)// m = ceil(------------)// log(1.2) 123456789101112131415161718192021Pos.copyTo(mWorldPos);cv::Mat Ow = pFrame-&gt;GetCameraCenter();mNormalVector = mWorldPos - Ow;// 世界坐标系下相机到3D点的向量mNormalVector = mNormalVector/cv::norm(mNormalVector);// 世界坐标系下相机到3D点的单位向量cv::Mat PC = Pos - Ow;const float dist = cv::norm(PC);const int level = pFrame-&gt;mvKeysUn[idxF].octave;const float levelScaleFactor = pFrame-&gt;mvScaleFactors[level];const int nLevels = pFrame-&gt;mnScaleLevels;// 另见PredictScale函数前的注释mfMaxDistance = dist*levelScaleFactor;mfMinDistance = mfMaxDistance/pFrame-&gt;mvScaleFactors[nLevels-1];// 见mDescriptor在MapPoint.h中的注释pFrame-&gt;mDescriptors.row(idxF).copyTo(mDescriptor);// MapPoints can be created from Tracking and Local Mapping. This mutex avoid conflicts with id.unique_lock&lt;mutex&gt; lock(mpMap-&gt;mMutexPointCreation);mnId=nNextId++; 2.5 更新关键帧间的连接关系,在关键帧之间建立边，每个边有一个权重，边的权重是该关键帧与当前帧公共3D点的个数，同时更新每一个3D点能够被哪些帧观测到12pKFini-&gt;UpdateConnections();pKFcur-&gt;UpdateConnections(); 2.6 BA优化// 步骤5：BA优化 Optimizer::GlobalBundleAdjustemnt(mpMap,20); 2.7 归一化3D点坐标和摄像机矩阵123456789101112131415161718192021222324252627282930// Set median depth to 1 // 步骤6：!!!将MapPoints的中值深度归一化到1，并归一化两帧之间变换 // 评估关键帧场景深度，q=2表示中值 float medianDepth = pKFini-&gt;ComputeSceneMedianDepth(2); float invMedianDepth = 1.0f/medianDepth; if(medianDepth&lt;0 || pKFcur-&gt;TrackedMapPoints(1)&lt;100) &#123; cout &lt;&lt; "Wrong initialization, reseting..." &lt;&lt; endl; Reset(); return; &#125; // Scale initial baseline cv::Mat Tc2w = pKFcur-&gt;GetPose(); // x/z y/z 将z归一化到1 Tc2w.col(3).rowRange(0,3) = Tc2w.col(3).rowRange(0,3)*invMedianDepth; pKFcur-&gt;SetPose(Tc2w); // Scale points // 把3D点的尺度也归一化到1 vector&lt;MapPoint*&gt; vpAllMapPoints = pKFini-&gt;GetMapPointMatches(); for(size_t iMP=0; iMP&lt;vpAllMapPoints.size(); iMP++) &#123; if(vpAllMapPoints[iMP]) &#123; MapPoint* pMP = vpAllMapPoints[iMP]; pMP-&gt;SetWorldPos(pMP-&gt;GetWorldPos()*invMedianDepth); &#125; &#125; 2.8 建立此时mpTracker的成员变量mpLocalMapper,mpLastKeyFrame，mnLastKeyFrameId，mvpLocalKeyFrames，mvpLocalMapPoints，mpReferenceKF，mLastFrame，mpMap的相关信息,以及当前帧的信息将初始关键帧和当前关键帧键入局部地图关键帧集合，重新设置当前帧的位姿为归一化后的位姿，设置局部地图为此时的初始地图，设置参考关键帧为当前关键帧，设置当前帧的参考关键帧为当前关键帧，设置最后一帧为当前帧，全局地图的参考地图为此时的局部地图，全局地图的初始关键帧为初始关键帧1234567891011121314151617181920mpLocalMapper-&gt;InsertKeyFrame(pKFini);mpLocalMapper-&gt;InsertKeyFrame(pKFcur);mCurrentFrame.SetPose(pKFcur-&gt;GetPose());mnLastKeyFrameId=mCurrentFrame.mnId;mpLastKeyFrame = pKFcur;mvpLocalKeyFrames.push_back(pKFcur);mvpLocalKeyFrames.push_back(pKFini);mvpLocalMapPoints=mpMap-&gt;GetAllMapPoints();mpReferenceKF = pKFcur;mCurrentFrame.mpReferenceKF = pKFcur;mLastFrame = Frame(mCurrentFrame);mpMap-&gt;SetReferenceMapPoints(mvpLocalMapPoints);mpMapDrawer-&gt;SetCurrentCameraPose(pKFcur-&gt;GetPose());mpMap-&gt;mvpKeyFrameOrigins.push_back(pKFini); 2.9 初始化完成，mState=OK1mState=OK;// 初始化成功，至此，初始化过程完成 3. 初始化结束后，对每一帧图像进行track()此时mState==OK，track()函数走else分支 op1=>operation: SLAM.TrackMonocular() op2=>operation: GrabImageMonocular() op3=>operation: Track() op1(right)->op2(right)->op3{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析五]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%94%2F</url>
    <content type="text"><![CDATA[ORBmatcher类说明单目SLAM初始化相关，双目和RGBD不会使用这个类 1. ORBmatcher 成员变量说明1.1 ORBmatcher 成员变量说明123static const int TH_LOW;static const int TH_HIGH;static const int HISTO_LENGTH; 1.2 ORBmatcher 匹配分数设置1float mfNNratio; 1.3 ORBmatcher 是否检查方向1bool mbCheckOrientation;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析四]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[初始器 Initializer类说明1. Initializer 成员变量说明1typedef pair&lt;int,int&gt; Match; 定义Match类型 1.1 特征点匹配数据类型声明 Match1typedef pair&lt;int,int&gt; Match; 定义Match类型 1.2 mvKeys1，mvKeys2参考帧和当前帧中的特征点12345// Keypoints from Reference Frame (Frame 1)vector&lt;cv::KeyPoint&gt; mvKeys1; ///&lt; 存储Reference Frame中的特征点// Keypoints from Current Frame (Frame 2)vector&lt;cv::KeyPoint&gt; mvKeys2; ///&lt; 存储Current Frame中的特征点 1.3 特征匹配结构说明1234// Current Matches from Reference to Current // Reference Frame: 1, Current Frame: 2 vector&lt;Match&gt; mvMatches12; ///&lt; Match的数据结构是pair,mvMatches12只记录Reference到Current匹配上的特征点对 vector&lt;bool&gt; mvbMatched1; ///&lt; 记录Reference Frame的每个特征点在Current Frame是否有匹配的特征点 1.4 相机内参1cv::Mat mK; ///&lt; 相机内参 1.5 测量误差12// Standard Deviation and Variance float mSigma, mSigma2; ///&lt; 测量误差 1.6 RANSAC迭代次数12// Ransac max iterations int mMaxIterations; ///&lt; 算Fundamental和Homography矩阵时RANSAC迭代次数 1.7 特征匹配结构说明12// Ransac sets vector&lt;vector&lt;size_t&gt; &gt; mvSets; ///&lt; 二维容器，外层容器的大小为迭代次数，内层容器大小为每次迭代算H或F矩阵需要的点 2. Initializer 成员函数说明2.1 Initializer构造函数说明123456789101112131415161718 // Fix the reference frame // 用reference frame来初始化，这个reference frame就是SLAM正式开始的第一帧 Initializer(const Frame &amp;ReferenceFrame, float sigma = 1.0, int iterations = 200);/** * @brief 给定参考帧构造Initializer * * 用reference frame来初始化，这个reference frame就是SLAM正式开始的第一帧 * @param ReferenceFrame 参考帧 * @param sigma 测量误差 * @param iterations RANSAC迭代次数 */ mK = ReferenceFrame.mK.clone(); mvKeys1 = ReferenceFrame.mvKeysUn; mSigma = sigma; mSigma2 = sigma*sigma; mMaxIterations = iterations; 2.1 Initialize()函数说明12345// Computes in parallel a fundamental matrix and a homography // Selects a model and tries to recover the motion and the structure from motion // 用current frame,也就是用SLAM逻辑上的第二帧来初始化整个SLAM，得到最开始两帧之间的R t,以及点云 bool Initialize(const Frame &amp;CurrentFrame, const vector&lt;int&gt; &amp;vMatches12, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated); 针对第一二帧图像进行初始化，初始化时需要确定第二帧相对于第一帧的旋转矩阵和平移向量，以及生成的三维地图点和是否能三角测量化，并行地计算基础矩阵和单应性矩阵，选取其中一个模型，恢复出最开始两帧之间的相对姿态以及点云。 初始化匹配特征点点1234567891011// Fill structures with current keypoints and matches with reference frame// Reference Frame: 1, Current Frame: 2// Frame2 特征点mvKeys2 = CurrentFrame.mvKeysUn;// mvMatches12记录匹配上的特征点对mvMatches12.clear();mvMatches12.reserve(mvKeys2.size());// mvbMatched1记录每个特征点是否有匹配的特征点，// 这个变量后面没有用到，后面只关心匹配上的特征点mvbMatched1.resize(mvKeys1.size()); 2.2 FindHomography()函数说明12// 假设场景为平面情况下通过前两帧求取Homography矩阵(current frame 2 到 reference frame 1),并得到该模型的评分 void FindHomography(vector&lt;bool&gt; &amp;vbMatchesInliers, float &amp;score, cv::Mat &amp;H21); 平面情况下评估Homograph矩阵并计算得分 2.3 FindFundamental()函数说明12// 假设场景为非平面情况下通过前两帧求取Fundamental矩阵(current frame 2 到 reference frame 1),并得到该模型的评分 void FindFundamental(vector&lt;bool&gt; &amp;vbInliers, float &amp;score, cv::Mat &amp;F21); 非平面情况下评估Fundamental矩并计算得分 2.4 ComputeH21()函数说明12// 被FindHomography函数调用具体来算Homography矩阵 cv::Mat ComputeH21(const vector&lt;cv::Point2f&gt; &amp;vP1, const vector&lt;cv::Point2f&gt; &amp;vP2); 2.5 FindHomography()函数说明12// 被FindFundamental函数调用具体来算Fundamental矩阵 cv::Mat ComputeF21(const vector&lt;cv::Point2f&gt; &amp;vP1, const vector&lt;cv::Point2f&gt; &amp;vP2); 2.6 CheckHomography()函数说明12// 被FindHomography函数调用，具体来算假设使用Homography模型的得分 float CheckHomography(const cv::Mat &amp;H21, const cv::Mat &amp;H12, vector&lt;bool&gt; &amp;vbMatchesInliers, float sigma); 2.7 FindHomography()函数说明12// 被FindFundamental函数调用，具体来算假设使用Fundamental模型的得分 float CheckFundamental(const cv::Mat &amp;F21, vector&lt;bool&gt; &amp;vbMatchesInliers, float sigma); 2.8 ReconstructF()函数说明123// 分解F矩阵，并从分解后的多个解中找出合适的R，t bool ReconstructF(vector&lt;bool&gt; &amp;vbMatchesInliers, cv::Mat &amp;F21, cv::Mat &amp;K, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated, float minParallax, int minTriangulated); 2.9 ReconstructH()函数说明123// 分解H矩阵，并从分解后的多个解中找出合适的R，t bool ReconstructH(vector&lt;bool&gt; &amp;vbMatchesInliers, cv::Mat &amp;H21, cv::Mat &amp;K, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated, float minParallax, int minTriangulated); 2.10 Triangulate()函数说明12// 通过三角化方法，利用反投影矩阵将特征点恢复为3D点 void Triangulate(const cv::KeyPoint &amp;kp1, const cv::KeyPoint &amp;kp2, const cv::Mat &amp;P1, const cv::Mat &amp;P2, cv::Mat &amp;x3D); 2.11 Normalize()函数说明12// 归一化三维空间点和帧间位移t void Normalize(const vector&lt;cv::KeyPoint&gt; &amp;vKeys, vector&lt;cv::Point2f&gt; &amp;vNormalizedPoints, cv::Mat &amp;T); 2.12 CheckRT()函数说明1234// ReconstructF调用该函数进行cheirality check，从而进一步找出F分解后最合适的解 int CheckRT(const cv::Mat &amp;R, const cv::Mat &amp;t, const vector&lt;cv::KeyPoint&gt; &amp;vKeys1, const vector&lt;cv::KeyPoint&gt; &amp;vKeys2, const vector&lt;Match&gt; &amp;vMatches12, vector&lt;bool&gt; &amp;vbInliers, const cv::Mat &amp;K, vector&lt;cv::Point3f&gt; &amp;vP3D, float th2, vector&lt;bool&gt; &amp;vbGood, float &amp;parallax); 2.13 DecomposeE()函数说明12// F矩阵通过结合内参可以得到Essential矩阵，该函数用于分解E矩阵，将得到4组解 void DecomposeE(const cv::Mat &amp;E, cv::Mat &amp;R1, cv::Mat &amp;R2, cv::Mat &amp;t);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Schmidt 正交化]]></title>
    <url>%2F2019%2F05%2F31%2FSchmidt-%E6%AD%A3%E4%BA%A4%E5%8C%96%2F</url>
    <content type="text"><![CDATA[施密特正交化(Schmidt orthogonalization)是求欧氏空间正交基的一种方法。从欧氏空间任意线性无关的向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$出发，求得正交向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_m}$，$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$与向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_m}$等价，再将正交向量组中每个向量经过单位化，就得到一个标准正交向量组，这种方法称为施密特正交化。 线性无关向量组未必是正交向量组，但正交向量组又是重要的，因此现在就有一个问题：能否从一个线性无关向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$出发，构造出一个标准正交向量组$\boldsymbol{e_1},\boldsymbol{e_2},\cdots,\boldsymbol{e_m}$，并且使向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$与向量组$\boldsymbol{e_1},\boldsymbol{e_2},\cdots,\boldsymbol{e_m}$等价呢?回答是肯定的，通过施密特正交化方法就可以实现。下面就来介绍这个方法，由于把一个正交向量组中每个向量经过单位化，就得到一个标准正交向量组，所以，上述问题的关键是如何由一个线性无关向量组来构造出一个正交向量组，我们以3个向量组成的线性无关组为例来说明这个方法。设向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\boldsymbol{\alpha_3}$线性无关，我们先来构造正交向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\boldsymbol{\beta_3}$，并且使$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_r}$与向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_r}$等价$(r=1,2,3)$。按所要求的条件，$\boldsymbol{\beta_1}$是$\boldsymbol{\alpha_1}$的线性组合，$\boldsymbol{\beta_2}$是$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2}$的线性组合，为方便起见，不妨设 \boldsymbol{\beta_1}=\boldsymbol{\alpha_1},\boldsymbol{\beta_2}=\boldsymbol{\alpha_2}-k\boldsymbol{\beta_1}其中，数值$k$的选取应满足$\boldsymbol{\beta_1}$与$\boldsymbol{\beta_2}$垂直，即 \langle\boldsymbol{\beta_2},\boldsymbol{\beta_1}\rangle=\langle\boldsymbol{\alpha_2},\boldsymbol{\beta_1}\rangle - k\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle=0注意到$\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle &gt; 0$，于是得$k={\langle\boldsymbol{\alpha_2},\boldsymbol{\beta_1}\rangle} / {\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}$，从而得 \boldsymbol{\beta_1} =\boldsymbol{\alpha_1},\boldsymbol{\beta_2}=\boldsymbol{\alpha_2}-\frac{\langle\boldsymbol{\alpha_2},\boldsymbol{\beta_1}\rangle}{\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}\boldsymbol{\beta_1}对于上面已经构造的向量$\boldsymbol{\beta_1}$和$\boldsymbol{\beta_2}$，再来构造$\boldsymbol{\beta_3}$，为满足要求，可以令 \boldsymbol{\beta_3} = \boldsymbol{\alpha_3}-k_1\boldsymbol{\beta_1}-k_2\boldsymbol{\beta_2}其中，$k_1,k_2$的选取应满足$\boldsymbol{\beta_3}$分别与向量$\boldsymbol{\beta_1}$和$\boldsymbol{\beta_2}$垂直，即 \langle\boldsymbol{\beta_3},\boldsymbol{\beta_1}\rangle=\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_1}\rangle - k_1\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle=0，\langle\boldsymbol{\beta_3},\boldsymbol{\beta_2}\rangle=\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_2}\rangle - k_2\langle\boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle=0由此解得 k_1=\frac{\langle \boldsymbol{\alpha_3},\boldsymbol{\beta_1}\rangle}{\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}, k_2=\frac{\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_2}\rangle}{\langle\boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle}于是得 \boldsymbol{\beta_3} = \boldsymbol{\alpha_3}-\frac{\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_1}\rangle}{\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}\boldsymbol{\beta_1}-\frac{\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_2}\rangle}{\langle\boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle}\boldsymbol{\beta_2}容易验证，向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_r}$是与$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_r}$等价的正交向量，若再将$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_r}$单位化，即令 \boldsymbol{e_i} = \frac{\boldsymbol{\beta_i}}{\Vert \boldsymbol{\beta_i} \Vert}(i=1,2,3)则$\boldsymbol{e}_1,\boldsymbol{e_2},\boldsymbol{e_3}$就是满足要求的标准正交向量。 数学归纳法一般性定理设$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}(m\leq n)$是$\mathbb{R}^n$中的一个线性无关向量组，若令 \begin{array}{l} \boldsymbol{\beta_1} = \boldsymbol{\alpha_1} \\[2ex] \boldsymbol{\beta_2} = \boldsymbol{\alpha_2}-\frac{\langle \boldsymbol{\alpha_2} , \boldsymbol{\beta_1} \rangle } {\langle \boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}\boldsymbol{\beta_1} \\[2ex] \boldsymbol{\beta_m} = \boldsymbol{\alpha_m} - \frac{\langle \boldsymbol{\alpha_m} , \boldsymbol{\beta_1} \rangle } {\langle \boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle }\boldsymbol{\beta_1}-\frac{\langle \boldsymbol{\alpha_m} , \boldsymbol{\beta_2} \rangle } {\langle \boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle }\boldsymbol{\beta_2}-\cdots- \frac{\langle \boldsymbol{\alpha_m} , \boldsymbol{\beta_{m-1}} \rangle } {\langle \boldsymbol{\beta_{m-1}},\boldsymbol{\beta_{m-1}}\rangle }\beta_{m-1} \end{array} 则$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_m}$就是一个正交向量组，若再令 \boldsymbol{e_i}=\frac {\boldsymbol{\beta_i}}{\Vert \boldsymbol{\beta_i} \Vert}(i=1,2,\cdots,m)就得到了一个标准正交向量组$\boldsymbol{e_1},\boldsymbol{e_2},\cdots,\boldsymbol{e_m}$， 且该向量和$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$上述所说明的利用线性无关向量组，构造出一个标准正交向量组的方法，就是施密特正交化方法。]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>矩阵分解</tag>
        <tag>Math</tag>
        <tag>正交化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵分解]]></title>
    <url>%2F2019%2F05%2F31%2F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. QR分解定理 1 对任意非奇异实矩阵$A$总可以分解为正交矩阵$Q$与上三角矩阵$R$的积,如果要求上三角阵$R$的对角元素均为正数,则分解是唯一的。]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Matrix</tag>
        <tag>矩阵分解</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小二乘问题]]></title>
    <url>%2F2019%2F05%2F30%2F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 非齐次线性最小二乘问题考虑线性系统： \begin{array}{c} Ax = b & \text(A \in R^{ m \times n }(m > n),b \in R^m,x \in R^n) \end{array}的最小二乘解。即求 $ x \in R^n $使得 \begin{Vmatrix} Ax-b \end{Vmatrix} _2 = \min \{ \begin{Vmatrix} Av-b \end{Vmatrix} _2:v \in R^n\}记 X_{LS}=\{ x\in R^n:x是 (1) 的解\}则称$X_{LS}$是最小二乘问题的解集；$X_{LS}$中范数最小者称为最小范数解，并记作$x_{LS}$，即 \begin{Vmatrix} x_{LS} \end{Vmatrix} _2 = \min \{ \begin{Vmatrix} x \end{Vmatrix} _2 : x \in X_{LS}\}命题 1 $x\in X_{LS} \iff A^T(Ax-b)=0$证明 $\forall x,y \in R^n,$有 \begin{Vmatrix} b-A(x+y) \end{Vmatrix} _2^2 = \begin{Vmatrix} b-Ax \end{Vmatrix} _2^2 - 2y^TA^T(b-Ax)+ \begin{Vmatrix} Ay \end{Vmatrix} _2^2因此， x \in X_{LS} \iff \forall y \in R^n,\begin{Vmatrix} Ay \end{Vmatrix} _2^2 - 2y^TA^T(b-Ax)\geq 0 \iff A^T(b-Ax)=0方程$A^T(b-Ax)=0$称为$Ax-b=0$的正规方程。推论 1 $X_{LS}$是凸集； $x_{LS}$是唯一的； $X_{LS} = \{ x_{LS}的充分必要条件是rank(A)=n \}$。 Moore-Pseudo 广义逆为了给出最小二乘的一般表示，需要矩阵的广义逆的概念。 定义 1 $Ａ\in R_{m \times n} $，若$X\in R_{n\times m}$满足 \begin{array}{c} AXA = A ,& XAX=X ,& (AX)^T=AX ,& (XA)^T=XA \end{array}则称$X$是$A$的广义逆，并记作$A^+$。矩阵$A$的广义逆是唯一的，并且可以利用$A$的 SVD 分解进行计算。令$A$的 SVD 分解为 A=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T不难验证： A^+=V\begin{pmatrix}\Sigma_r^{-1} & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}U^T命题 2 最小二乘问题的一般解为 x=A^+b+(I-A^+A)z,z\in R^n最小范数解是 x_{LS}=A^+b证明 由命题1，最小二乘问题(1)的解可以由它的正规方程: A^TAx=A^Tb给出。利用广义逆，可以验证x=A^+b是上式的一个解。另外，由 SVD 分解可证明$N(A^TA)=N(A)$，且 N(A) = \{(I-A^+A)z:z\in R^n\}此外，注意到 [(I-A^+A)z]^TA^+b=z^T(I-A^+A)A^+b=0所以， \begin{Vmatrix} x \end{Vmatrix} _2^2 = \begin{Vmatrix} A^+b \end{Vmatrix} _2^2 + \begin{Vmatrix} (I-A^+A)z \end{Vmatrix} _2^2 \geq \begin{Vmatrix} A^+b \end{Vmatrix} _2^2即：$x_{LS}=A^+b$ 满秩最小二乘问题如果(1)中的矩阵$A$是列满秩的，即$rank(A)=n$，则称它为满秩最小二乘问题。可以通过正规分解法，QR 分解法来求解该最小二乘问题，但最好的方法还是 SVD 分解法。 SVD分解方法由于$rank(A)=n$，所以$A$必有下述形式的 SVD 分解$A=U\begin{pmatrix}\Sigma_n \ 0\end{pmatrix}V^T$，于是，$A^+=V\begin{pmatrix}\Sigma_n^{-1},0 \end{pmatrix}U^T$。所以，问题的解为 x=A^+b=V\begin{pmatrix}\Sigma_n^{-1},0 \end{pmatrix}U^Tb=\frac{u_1^Tb}{\sigma_1}v_1+\frac{u_2^Tb}{\sigma_2}v_2+\cdots+\frac{u_n^Tb}{\sigma_n}v_n = \sum_{j=1}^{n}\frac{u_j^Tb}{\sigma_j}v_j亏秩最小二乘问题此时不可以利用其他方法来求解最小二乘问题，SVD 方法是首选。具体来说，若$rank(A)=r$，则$A$有 SVD 分解: A=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T因此， x=A^+b=V\begin{pmatrix}\Sigma_r^{-1} & \boldsymbol 0 \\ \boldsymbol 0 & 0\end{pmatrix}U^Tb = \sum_{j=1}^{r}\frac{u_j^Tb}{\sigma_j}v_j数值秩的定义和确定方法略 以后再补 2. 齐次最小二乘问题考虑齐次线性方程： \begin{array}{c} Ax = 0 & \text(A \in R^{ m \times n }(m > n),b \in R^m,x \in R^n) \end{array}对应的最小二乘问题是 \begin{Vmatrix} Ax \end{Vmatrix} _2 = \min \{ \begin{Vmatrix} Av \end{Vmatrix} _2:v \in R^n\}显然，$x=0$总是上述最小二乘问题的最小范数解。在实际中，人们通常关心的是它的非零解，而不是零解。因此，总是考虑相应的约束最小二乘问题: \begin{Vmatrix} Ax \end{Vmatrix}_2 = \min \{ \begin{Vmatrix} Av \end{Vmatrix}_2:v\in R^n,\begin{Vmatrix} v \end{Vmatrix}_2 = 1\}或者等价的写成 \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix}_2\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1 \end{array} \right.命题 3 若$rank(A)=r$，则该齐次线性方程的解为 x=\sum_{j=1}^{n-r}s_jv_{r+j},(\sum_{j=1}^{n-r}s_j^2=1)其中，$v_{r+1},v_{r+2},\cdots,v_n$是$A^TA$的零特征值的$n-r$个线性无关的单位特征向量。 证明 问题等价于: \left\{ \begin{array}{l} \min x^TA^TAx\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1 \end{array} \right.因$rank(A^TA)=rank(A)=r$，所以$A^TA$有特征值分解 A^TA=Vdiag(\lambda_1,\lambda_2,\cdots,\lambda_r,0,\cdots,0)V^T其中，$V$是正交矩阵，所以$v_{r+1},v_{r+2},\cdots,v_n$是$A^TAx=0$的$n-r$个相互正交的单位解，因此，上述问题的解为 x=\sum_{j=1}^{n-r}s_jv_{r+j},(\sum_{j=1}^{n-r}s_j^2=1)命题 4 若$rank(A)=r$，则该齐次线性方程的解为 x=\sum_{j=1}^{n-r}s_jv_{r+j},(\sum_{j=1}^{n-r}s_j^2=1)其中，$v_{r+1},v_{r+2},\cdots,v_n$是$A$的$n-r$个零奇异值的右奇异向量。 3. 约束齐次最小二乘问题零约束考虑约束最小二乘问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix} _2^2\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1,Cx=0 \end{array} \right.不妨假设$rank(C)=r &lt; n$，对$C$作 SVD 分解 C=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T令$V_{n-r}=(v_{r+1},v_{r+2},\cdots,v_{n})$为$V$的最后$n-r$个列向量所构成的矩阵，因为$Cx=0$的所有解可以表示为: x=V_{n-r}y,y \in R^{n-r}并且$\begin{Vmatrix} Ax\end{Vmatrix} _2^2=1 \iff \begin{Vmatrix} y\end{Vmatrix} _2^2=1$，这是因为$V_{n-r}$是列正交的。于是，上述问题可以转化为下述最小化问题 \left\{ \begin{array}{l} \min \begin{Vmatrix} AV_{n-r}y \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} y \end{Vmatrix}_2=1 \end{array} \right.这个问题在上一节已经解决。 值约束考虑约束最小二乘问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix} _2^2\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1,x=Cy \end{array} \right.不妨假设$rank(C)=r &lt; n$，对$C$作 SVD 分解 C=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T由 SVD 分解特性可知，矩阵$C$的值空间可以表示为 x = R(C)=\{U_rx'|x'\in R^r\}其中，$U_r$为$U$的前$r$列所构成的矩阵。于是$x=Cy$可以表示为$x=U_rx’$且有$\begin{Vmatrix}x\end{Vmatrix} _2^2=1 \iff \begin{Vmatrix} x’\end{Vmatrix} _2^2=1$，这是因为$U_r$是列正交的。于是，上述问题可以转化为下述最小化问题 \left\{ \begin{array}{l} \min \begin{Vmatrix} AU_rx' \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} x' \end{Vmatrix}_2=1 \end{array} \right.这个问题在上一节已经解决。 模约束考虑约束最小二乘问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix} _2^2\\[2ex] subject to\begin{Vmatrix} Cx \end{Vmatrix}_2=1 \end{array} \right.不妨假设$rank(C)=r &lt; n$，对$C$作 SVD 分解 C=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T令$x’=V^Tx$，则有 Cx = U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}x'且$\begin{Vmatrix} Ax \end{Vmatrix} _2=1 \iff \begin{Vmatrix}\Sigma_r &amp; \boldsymbol 0 \ \boldsymbol 0 &amp; 0\end{Vmatrix}x’=1 \iff \begin{Vmatrix} \Sigma_rx’\end{Vmatrix} _2=1 $，其中， x'= \begin{pmatrix} x'_r \\ x'_{n-r}\end{pmatrix}所以， Ax=AVx'=A'x'= \begin{pmatrix} A'_r & A'_{n-r}\end{pmatrix}\begin{pmatrix} x'_r \\ x'_{n-r}\end{pmatrix}=A'_rx'_r+A'_{n-r}x'_{n-r}取$y=\Sigma_rx’_r$，则$x’_r=\Sigma_r^{-1}y$，于是，上述问题可以转化为下述最小化问题 \left\{ \begin{array}{l} \min \begin{Vmatrix} A'_r\Sigma_r^{-1}y+A'_{n-r}x'_{n-r} \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} y \end{Vmatrix}_2=1 \end{array} \right.由于$\min_{x’_{n-r}} \begin{Vmatrix} A’_r\Sigma_r^{-1}y+A’_{n-r}x’_{n-r} \end{Vmatrix}_2^2$的最小化问题的解为$x’_{n-r} = -{A’}_{n-r}^+A’_r\Sigma_r^{-1}y$于是该求解问题转化为如下最小化问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} (A'_r-A'_{n-r}{A'}_{n-r}^+A'_r)\Sigma_r^{-1}y \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} y \end{Vmatrix}_2=1 \end{array} \right.这个问题在上一节已经解决。注意，$A^+A$不等于$I$，除非A满秩。]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>最小二乘问题</tag>
        <tag>SVD分解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PnP算法]]></title>
    <url>%2F2019%2F05%2F30%2FPnP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[PnP算法求解相机位姿PnP(Perspective-n-Point) 是求解3D到2D点对运动的方法。它描述了当我们知道n个3D空间点以及它们的投影位置时,如何估计相机所在的位姿。 1. 直接线性变换(DLT)考虑某个空间点$P$,它的齐次坐标为$P=(X, Y, Z, 1)^T$。在图像$I_1$中,投影到特征点$x_1=(u_1,v_1,1)^T$(以归一化平面齐次坐标表示)。此时相机的位姿$R,t$是未知的。与单应矩阵的求解类似,我们定义增广矩阵$[R|t]$为一个$3\times4$的矩阵,包含了旋转与平移信息。我们把它的展开形式列写如下: s\begin{pmatrix} u_1 \\ v_1 \\ 1 \\ \end{pmatrix} = \begin{pmatrix} t_1 & t_2 & t_3 & t_4 \\ t_5 & t_6 & t_7 & t_8 \\ t_9 & t_{10} & t_{11} & t_{12} \\ \end{pmatrix} \begin{pmatrix} X \\ Y \\ Z \\ \end{pmatrix}用最后一行把$s$消去,得到两个约束:\begin{align*}u_1 &amp;= \frac{t_1X+t_2Y+t_3Z+t_4}{t_9X+t_{10}Y+t_{11}Z+t_{12}}\\v_1 &amp;= \frac{t_5X+t_6Y+t_7Z+t_8}{t_9X+t_{10}Y+t_{11}Z+t_{12}}\end{align*} 为了简化表示,定义$T$的行向量: \begin{array}{c} t_1 = (t_1, t_2, t_3, t_4)^T,& t_2 = (t_5, t_6, t_7, t_8)^T,& t_3 = (t_9, t_{10}, t_{11}, t_{12})^T \end{array}于是有: \left\{ \begin{array}{c} t_1^TP-t_3^TPu_1=0 \\ t_2^TP-t_3^TPv_1=0 \end{array} \right.请注意$t$是待求的变量,可以看到每个特征点提供了两个关于$t$的线性约束。假设一共有$N$个特征点,可以列出线性方程组: \begin{pmatrix} P_1^T & 0 & -u_1P_1^T \\ 0 & P_1^T & -v_1P_1^T \\ \vdots & \vdots & \vdots \\ P_N^T & 0 & -u_NP_N^T \\ 0 & P_N^T & -v_NP_N^T \end{pmatrix}由于 $t$ 一共有12维，考虑到齐次性，共有11个自由度，而每一对特征点对都提供了两个约束方程。因此最少通过6对匹配点，即可实现矩阵$T$的线性求解，这种方法(也)称为直接线性变换(Direct Linear Transform，DLT)。当匹配点大于6对时，(又)可以使用 SVD 等方法对超定方程求最小二乘解。在 DLT 求解中，我们直接将$T$矩阵看成了12个未知数，忽略了它们之间的联系。因为旋转矩阵$R∈SO(3)$，用 DLT 求出的解不一定满足该约束，它是一个一般矩阵。平移向量比较好办，它属于向量空间。对于旋转矩阵$R$，我们必须针对 DLT 估计的$T$的左边$3\times3$的矩阵块，寻找一个最好的旋转矩阵对它进行近似。这可以由QR分解完成，相当于把结果从矩阵空间重新投影到$SE(3)$流形上，转换成旋转和平移两部分。需要解释的是,我们这里的$x_1$使用了归一化平面坐标，去掉了内参矩阵$K$的影响——这是因为内参$K$在SLAM中通常假设为已知。如果内参未知,那么我们也能用 PnP 去估计$K,R,t$三个量。然而由于未知量的增多，效果会差一些。]]></content>
      <categories>
        <category>SFM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>SFM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析三]]></title>
    <url>%2F2019%2F05%2F29%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89%2F</url>
    <content type="text"><![CDATA[ORBextractor特征提取器1. ORBextractor 成员变量1.1 SCORE 得分常量1enum &#123;HARRIS_SCORE=0, FAST_SCORE=1 &#125;; 定义常量，HARRIS_SCORE=0, FAST_SCORE=1，特征点提取后通过得分进行筛选得到最佳的特征点 1.2 描述子的筛选模式1std::vector&lt;cv::Point&gt; pattern; 用来快速建立256维的描述子 1.3 ORBextractor 特征提取器的控制变量12345int nfeatures; // 需要提取的特征点数目double scaleFactor; // ORB 金字塔相邻两层之间的尺度因子int nlevels; // 金字塔的层数int iniThFAST; // FAST角点提取的初始阈值int minThFAST; // FAST角点用初始阈值提取失败后放松阈值再次提取 1.4 每层金字塔对应的特征点数目Vector mnFeaturesPerLevel1std::vector&lt;int&gt; mnFeaturesPerLevel; 定义常量，HARRIS_SCORE=0, FAST_SCORE=1，特征点提取后通过得分进行筛选得到最佳的特征点 1.5 每层金字塔对应的尺度因子Vector mvScaleFactor1std::vector&lt;float&gt; mvScaleFactor; 1.6 每层金字塔对应的尺度因子的倒数Vector mvInvScaleFactor1std::vector&lt;float&gt; mvInvScaleFactor; 1.7 每层金字塔对应的Sigma的平方Vector mvLevelSigma21std::vector&lt;float&gt; mvLevelSigma2; 1.8 每层金字塔对应的Sigma的平方的倒数Vector mvLevelSigma21std::vector&lt;float&gt; mvInvLevelSigma2; 1.9 每层金字塔对应图像Mat mvImagePyramid1std::vector&lt;cv::Mat&gt; mvImagePyramid; 1.10 半径为31时所对应的截半径vector1std::vector&lt;int&gt; umax; 2. ORBextractor 成员函数2.1 ORBextractor 构造函数12ORBextractor(int nfeatures, float scaleFactor, int nlevels, int iniThFAST, int minThFAST); 第一步：算金字塔每层的尺度，然后根据尺度计算每层应该提取多少特征点，这里面涉及了一个等比数列，唤起高中的记忆，还挺有意思的。最后，保证提取总特征点数≥ nfeatures。1234567891011121314151617mvScaleFactor.resize(nlevels);mvLevelSigma2.resize(nlevels);mvScaleFactor[0]=1.0f;mvLevelSigma2[0]=1.0f;for(int i=1; i&lt;nlevels; i++)&#123; mvScaleFactor[i]=mvScaleFactor[i-1]*scaleFactor; mvLevelSigma2[i]=mvScaleFactor[i]*mvScaleFactor[i];&#125;mvInvScaleFactor.resize(nlevels);mvInvLevelSigma2.resize(nlevels);for(int i=0; i&lt;nlevels; i++)&#123; mvInvScaleFactor[i]=1.0f/mvScaleFactor[i]; mvInvLevelSigma2[i]=1.0f/mvLevelSigma2[i];&#125; nfeatures: 1000nleves: 8scaleFactor: 1.2iniThFAST: 20minThFAST: 8 mvScaleFactor: 1.0, 1.2, 1.2^2, 1.2^3, 1.2^4, 1.2^5, 1.2^6, 1.2^7mvLevelSigma2: mvScaleFactor[i]*mvScaleFactor[i];mvInvScaleFactor[i]=1.0f/mvScaleFactor[i];mvInvLevelSigma2[i]=1.0f/mvLevelSigma2[i]; 12345678910float factor = 1.0f / scaleFactor;float nDesiredFeaturesPerScale = nfeatures*(1 - factor)/(1 - (float)pow((double)factor, (double)nlevels));int sumFeatures = 0;for( int level = 0; level &lt; nlevels-1; level++ )&#123; mnFeaturesPerLevel[level] = cvRound(nDesiredFeaturesPerScale); sumFeatures += mnFeaturesPerLevel[level]; nDesiredFeaturesPerScale *= factor;&#125;mnFeaturesPerLevel[nlevels-1] = std::max(nfeatures - sumFeatures, 0); 通过等比数列求和来计算初始第0层nDesiredFeaturesPerScale的特征点数 {\rm nfeatures} = \frac{ {\rm nDesiredFeaturesPerScale}\times(1-{\rm factor}^{\rm nlevels})}{1-{\rm factor}}{\rm nDesiredFeaturesPerScale} = \frac{ {\rm nfeatures}\times(1-{\rm factor})}{1-{\rm factor}^{\rm nlevels}}并通过四舍五入来选取每一层的特征点数，最后一层的特征点数要保证总特征点数大于阈值1000 构造描述符生成器12345678910111213141516171819202122const int npoints = 512; const Point* pattern0 = (const Point*)bit_pattern_31_; std::copy(pattern0, pattern0 + npoints, std::back_inserter(pattern)); //This is for orientation // pre-compute the end of a row in a circular patch umax.resize(HALF_PATCH_SIZE + 1); int v, v0, vmax = cvFloor(HALF_PATCH_SIZE * sqrt(2.f) / 2 + 1); int vmin = cvCeil(HALF_PATCH_SIZE * sqrt(2.f) / 2); const double hp2 = HALF_PATCH_SIZE*HALF_PATCH_SIZE; for (v = 0; v &lt;= vmax; ++v) umax[v] = cvRound(sqrt(hp2 - v * v)); // Make sure we are symmetric for (v = HALF_PATCH_SIZE, v0 = 0; v &gt;= vmin; --v) &#123; while (umax[v0] == umax[v0 + 1]) ++v0; umax[v] = v0; ++v0; &#125; 2.2 void operator() 操作符提取关键点和描述符123456// Compute the ORB features and descriptors on an image.// ORB are dispersed on the image using an octree.// Mask is ignored in the current implementation.void operator()( cv::InputArray image, cv::InputArray mask, std::vector&lt;cv::KeyPoint&gt;&amp; keypoints, cv::OutputArray descriptors); 最终生成的特征点列表和描述符都放在引用参数里，ORBextractor没有成员变量保存特征点和描述符，只保存了每一层金字塔的图像 构建图像金字塔123456Mat image = _image.getMat(); assert(image.type() == CV_8UC1 ); // Pre-compute the scale pyramid // 构建图像金字塔 ComputePyramid(image); 计算每层图像的特征点1234// 计算每层图像的兴趣点 vector &lt; vector&lt;KeyPoint&gt; &gt; allKeypoints; // vector&lt;vector&lt;KeyPoint&gt;&gt; ComputeKeyPointsOctTree(allKeypoints); //ComputeKeyPointsOld(allKeypoints); 累加每一层的特征点数量，计算总的特征点数，并建立一个空的描述符矩阵，待填充123456789101112131415Mat descriptors; int nkeypoints = 0; for (int level = 0; level &lt; nlevels; ++level) nkeypoints += (int)allKeypoints[level].size(); if( nkeypoints == 0 ) _descriptors.release(); else &#123; _descriptors.create(nkeypoints, 32, CV_8U); descriptors = _descriptors.getMat(); &#125; _keypoints.clear(); _keypoints.reserve(nkeypoints); 分别统计每一层图像的特征点数，并对每一层金字塔进行高斯滤波，对滤波完的图像和关键点，匹配模式进行描述符的提取，对不同层的特征点的坐标重新定位。描述符的提取，半径为PATCH_SIZE=31123456789101112131415161718192021222324252627282930int offset = 0; for (int level = 0; level &lt; nlevels; ++level) &#123; vector&lt;KeyPoint&gt;&amp; keypoints = allKeypoints[level]; int nkeypointsLevel = (int)keypoints.size(); if(nkeypointsLevel==0) continue; // preprocess the resized image 对图像进行高斯模糊 Mat workingMat = mvImagePyramid[level].clone(); GaussianBlur(workingMat, workingMat, Size(7, 7), 2, 2, BORDER_REFLECT_101); // Compute the descriptors 计算描述子 Mat desc = descriptors.rowRange(offset, offset + nkeypointsLevel); computeDescriptors(workingMat, keypoints, desc, pattern); offset += nkeypointsLevel; // Scale keypoint coordinates if (level != 0) &#123; float scale = mvScaleFactor[level]; //getScale(level, firstLevel, scaleFactor); for (vector&lt;KeyPoint&gt;::iterator keypoint = keypoints.begin(), keypointEnd = keypoints.end(); keypoint != keypointEnd; ++keypoint) keypoint-&gt;pt *= scale; &#125; // And add the keypoints to the output _keypoints.insert(_keypoints.end(), keypoints.begin(), keypoints.end()); &#125; 2.3 ComputePyramid(cv::Mat image) 计算图像金字塔对图像进行线性插值缩小，每次图像的宽和高都缩小1.2倍1234567891011121314151617181920212223242526void ORBextractor::ComputePyramid(cv::Mat image)&#123; for (int level = 0; level &lt; nlevels; ++level) &#123; float scale = mvInvScaleFactor[level]; Size sz(cvRound((float)image.cols*scale), cvRound((float)image.rows*scale)); Size wholeSize(sz.width + EDGE_THRESHOLD*2, sz.height + EDGE_THRESHOLD*2); Mat temp(wholeSize, image.type()), masktemp; mvImagePyramid[level] = temp(Rect(EDGE_THRESHOLD, EDGE_THRESHOLD, sz.width, sz.height)); // Compute the resized image if( level != 0 ) &#123; resize(mvImagePyramid[level-1], mvImagePyramid[level], sz, 0, 0, cv::INTER_LINEAR); copyMakeBorder(mvImagePyramid[level], temp, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, BORDER_REFLECT_101+BORDER_ISOLATED); &#125; else &#123; copyMakeBorder(image, temp, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, BORDER_REFLECT_101); &#125; &#125;&#125; 2.4 ORBextractor::ComputeKeyPointsOctTree 计算特征点四叉树先对每一个网格进行FAST特征点的提取，然后再进行四叉树的建立，根据每一层要求的特征点数量，返回最大响应的特征点，最后计算每一个特征点的主方向，依据灰度质心法。定义常量，HARRIS_SCORE=0, FAST_SCORE=1，特征点提取后通过得分进行筛选得到最佳的特征点 2.5 computeOrientation 计算特征点描述符的方向1keypoint-&gt;angle = IC_Angle(image, keypoint-&gt;pt, umax); 每个特征点的方向采用灰度质心法，即统计半径为15的圆内的灰度质心方向，主要调用IC_Angle函数。 2.6 IC_Angle 计算特征点的主方向12345678910111213141516171819202122232425int m_01 = 0, m_10 = 0; const uchar* center = &amp;image.at&lt;uchar&gt; (cvRound(pt.y), cvRound(pt.x)); // Treat the center line differently, v=0 for (int u = -HALF_PATCH_SIZE; u &lt;= HALF_PATCH_SIZE; ++u) m_10 += u * center[u]; // Go line by line in the circuI853lar patch int step = (int)image.step1(); for (int v = 1; v &lt;= HALF_PATCH_SIZE; ++v) &#123; // Proceed over the two lines int v_sum = 0; int d = u_max[v]; for (int u = -d; u &lt;= d; ++u) &#123; int val_plus = center[u + v*step], val_minus = center[u - v*step]; v_sum += (val_plus - val_minus); m_10 += u * (val_plus + val_minus); &#125; m_01 += v * v_sum; &#125; return fastAtan2((float)m_01, (float)m_10); 先令m_01和m_10都为0，然后计算水平中间线上的均值，注意算均值时，在中心点的左边或者下边距离值是负数。然后依次统计每一条截半径上的水平均值和（累和），以及垂直线上的均值（相减），最后返回tan(m_01,m_10)作为主方向。 3. 初始化Current帧 st=>start: operator()生成特征点和描述符 e=>end: 得到m特征点Vector和描述符Matrix op1=>operation: 参数输入，输入图片矩阵，引用关键点列表，引用描述符矩阵 op2=>operation: 调用ComputePyramid(image)计算图像金字塔，对每一层的图像进行resize(),宽度和高度每次除以1.2然后进行线性插值，保存在成员变量mvImagePyramid里 op3=>operation: 调用ComputeKeyPointsOctTree(allKeypoints)计算特征点四叉树，构建初始网格图，对每一个网格进行特征点的提取 op4=>operation: 调用FAST函数对每一个网格来计算特征点坐标，并进行网格坐标矫正 op5=>operation: 对每一个特征点computeDescriptors(workingMat, keypoints, desc, pattern)计算描述符 st->op1->op2->op3->op4->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: Frame()构造函数生成当前帧 e=>end: 得到mCurrent帧 op1=>operation: 帧ID自加 op2=>operation: 特征提取尺度信息初始化 op3=>operation: 调用ExtractORB(0,imGray)来计算特征点Vector和描述符matrix，0表示単目，1表示双目，提取完的特征点存在mvKeys里面,描述符存在mDescriptors里面，这一步调用了特征提取器的括号运算符 op4=>operation: 对提取的每一个特征点进行矫正去畸变UndistortKeyPoints op5=>operation: ComputeImageBounds(imGray)，计算图片边界，方便进行网格划分 op6=>operation: 将特征点分配到对应的网格AssignFeaturesToGrid() st->op1->op2->op3->op4->op5->op6->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown flowchart.js画流程图]]></title>
    <url>%2F2019%2F05%2F28%2FMarkdown-flowchart.js%E7%94%BB%E6%B5%81%E7%A8%8B%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Markdown笔记：如何画流程图 Flowchart.js 仅需几行代码即可在 Web 上完成流程图的构建。可以从文字表述中画出简单的 SVG 流程图，也可以画出彩色的图表。 1. 先来看一段入门案例流程图代码在 Markdown 编辑中应该是下面这样的(由于渲染的问题，请把,,,改成三个点号)1234567891011,,,flowst=&gt;start: Starte=&gt;end: Endop1=&gt;operation: My Operationsub1=&gt;subroutine: My Subroutinecond=&gt;condition: Yes or No?io=&gt;inputoutput: catch something...st-&gt;op1-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op1,,, 输出结果如图所示: 在markdown语法中，流程图的画法和代码段类似，也就是说，流程图是写在两个,,,之间的。比如说php代码，会是这样一种格式: ,,,php代码段,,, 那么流程图就是这样的: ,,,flow代码段,,,` 2. 语法流程图的语法大体分为两部分: 前面部分用来定义流程图元素； 后面部分用来连接流程图元素，指定流程图的执行走向。 2.1 定义元素阶段的语法1tag=&gt;type: content:&gt;url 上例中下面部分代码都是定义元素部分123456st=&gt;start: Starte=&gt;end: Endop1=&gt;operation: My Operationsub1=&gt;subroutine: My Subroutinecond=&gt;condition: Yes or No?io=&gt;inputoutput: catch something... 说明： tag 是流程图中的标签，在第二段连接元素时会用到。名称可以任意，一般为流程的英文缩写和数字的组合。 type 用来确定标签的类型，=&gt;后面表示类型。由于标签的名称可以任意指定，所以要依赖type来确定标签的类型。 标签有6种类型：start end operation subroutine condition inputoutput。 content 是流程图文本框中的描述内容，: 后面表示内容，中英文均可。特别注意，冒号与文本之间一定要有个空格。 url是一个连接，与框框中的文本相绑定，:&gt;后面就是对应的 url 链接，点击文本时可以通过链接跳转到 url 指定页面。 开始1st=&gt;start: 开始 操作1op1=&gt;operation: 操作、执行说明 条件1cond=&gt;condition: 确认？ 结束1e=&gt;end: 结束 URL（貌似 SF 的编辑器不支持）1e=&gt;点击本结束跳转:&gt;http://https://segmentfault.com/blog/ingood 2.2 连接流程图元素的语法示例代码后面部分 st-&gt;op1-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op1 连接流程图元素阶段的语法就简单多了，直接用-&gt;来连接两个元素，几点说明如下：说明： 使用 -&gt; 来连接两个元素 对于condition类型，有yes和no两个分支，如示例中的cond(yes)和cond(no) 每个元素可以制定分支走向，默认向下，也可以用right指向右边，如示例中sub1(right)。 转载声明：本文转载自:https://segmentfault.com/a/1190000006247465?utm_source=tag-newest]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析二]]></title>
    <url>%2F2019%2F05%2F28%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[Tracking 线程分析1. Tracking流程图 2. Tracking 成员变量说明2.1 Tracking 状态枚举123456789enum eTrackingState&#123; SYSTEM_NOT_READY=-1, NO_IMAGES_YET=0, NOT_INITIALIZED=1, OK=2, LOST=3 &#125;;eTrackingState mState;eTrackingState mLastProcessedState; 2.2 Tracking 状态枚举 mSensor12// Input sensor:MONOCULAR, STEREO, RGBD int mSensor; mSeneor 传感器类型 2.3 当前帧和当前帧灰度图 mCurrentFrame,mImGray123// Current Frame Frame mCurrentFrame; cv::Mat mImGray; mSeneor 传感器类型 2.4 初始化的时候两帧图像之间的相关变量1234567// Initialization Variables (Monocular)// 初始化时前两帧相关变量std::vector&lt;int&gt; mvIniLastMatches;std::vector&lt;int&gt; mvIniMatches;// 跟踪初始化时前两帧之间的匹配std::vector&lt;cv::Point2f&gt; mvbPrevMatched;std::vector&lt;cv::Point3f&gt; mvIniP3D;Frame mInitialFrame; mSeneor 传感器类型 2.5 Tracking 结果关键帧列表和相对关键帧的位姿列表123456// Lists used to recover the full camera trajectory at the end of the execution.// Basically we store the reference keyframe for each frame and its relative transformationlist&lt;cv::Mat&gt; mlRelativeFramePoses;list&lt;KeyFrame*&gt; mlpReferences;list&lt;double&gt; mlFrameTimes;list&lt;bool&gt; mlbLost; mSeneor 传感器类型 2.6 是否开启地图变量 mbOnlyTracking12// True if local mapping is deactivated and we are performing only localizationbool mbOnlyTracking; mSeneor 传感器类型 2.7 只定位时0地图点是否VO变量12345// In case of performing only localization, this flag is true when there are no matches to// points in the map. Still tracking will continue if there are enough matches with temporal points.// In that case we are doing visual odometry. The system will try to do relocalization to recover// "zero-drift" localization to the map.bool mbVO; mSeneor 传感器类型 2.8 另外两个线程的指针 mpLocalMapper，mpLoopClosing123//Other Thread PointersLocalMapping* mpLocalMapper;LoopClosing* mpLoopClosing; mSeneor 传感器类型 2.9 ORB特征提取器1234567//ORB// orb特征提取器，不管单目还是双目，mpORBextractorLeft都要用到// 如果是双目，则要用到mpORBextractorRight// 如果是单目，在初始化的时候使用mpIniORBextractor而不是mpORBextractorLeft，// mpIniORBextractor属性中提取的特征点个数是mpORBextractorLeft的两倍ORBextractor* mpORBextractorLeft, *mpORBextractorRight;ORBextractor* mpIniORBextractor; mSeneor 传感器类型 2.10 Bow123//BoWORBVocabulary* mpORBVocabulary;KeyFrameDatabase* mpKeyFrameDB; mSeneor 传感器类型 2.11 単目初始器 mpInitializer123// Initalization (only for monocular)// 单目初始器Initializer* mpInitializer; mSeneor 传感器类型 2.12 局部地图1234//Local Map KeyFrame* mpReferenceKF;// 当前关键帧就是参考帧 std::vector&lt;KeyFrame*&gt; mvpLocalKeyFrames; std::vector&lt;MapPoint*&gt; mvpLocalMapPoints; mSeneor 传感器类型 2.13 SLAM 系统指针1System* mpSystem; mSeneor 传感器类型 2.14 显示相关1234//Drawers Viewer* mpViewer; FrameDrawer* mpFrameDrawer; MapDrawer* mpMapDrawer; mSeneor 传感器类型 2.15 系统地图 mpMap12//Map Map* mpMap; mSeneor 传感器类型 2.16 相机参数1234//Calibration matrix cv::Mat mK; cv::Mat mDistCoef; float mbf; mSeneor 传感器类型 2.17 新关键帧插入规则123//New KeyFrame rules (according to fps)int mMinFrames;int mMaxFrames; mSeneor 传感器类型 2.18 深度截断值1234// Threshold close/far points // Points seen as close by the stereo/RGBD sensor are considered reliable // and inserted from just one frame. Far points requiere a match in two keyframes. float mThDepth; mSeneor 传感器类型 2.19 深度图因子12// For RGB-D inputs only. For some datasets (e.g. TUM) the depthmap values are scaled.float mDepthMapFactor;; mSeneor 传感器类型 2.20 当前帧有多少特征点成功匹配12//Current matches in frame int mnMatchesInliers; mSeneor 传感器类型 2.21 上一关键帧、上一帧和重定位信息12345//Last Frame, KeyFrame and Relocalisation Info KeyFrame* mpLastKeyFrame; Frame mLastFrame; unsigned int mnLastKeyFrameId; unsigned int mnLastRelocFrameId; mSeneor 传感器类型 2.22 匀速模型匀速变换矩阵12//Motion Model cv::Mat mVelocity; mSeneor 传感器类型 2.23 相机RGB信息12//Color order (true RGB, false BGR, ignored if grayscale)bool mbRGB; mSeneor 传感器类型 2.24 临时地图点列表1list&lt;MapPoint*&gt; mlpTemporalPoints; mSeneor 传感器类型 共47个成员变量 3. Tracking 类成员函数3.1 构造函数 Tracking()12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 执行完构造函数后mState(NO_IMAGES_YET), mSensor(sensor), mbOnlyTracking(false), mbVO(false), mpORBVocabulary(pVoc),mpKeyFrameDB(pKFDB), mpInitializer(static_cast(NULL)), mpSystem(pSys), mpViewer(NULL),mpFrameDrawer(pFrameDrawer), mpMapDrawer(pMapDrawer), mpMap(pMap), mnLastRelocFrameId(0)等13个变量获得初始值。mK，mDistCoef，mbf，mMinFrames，mMaxFrames，mbRGB，mpORBextractorLeft，mpORBextractorRight，mpIniORBextractor，mThDepth， mDepthMapFactor等12个变量获得具体值。 3.2 抓取图片函数 GrabImageMonocular()1234// Preprocess the input and call Track(). Extract features and performs stereo matching. cv::Mat GrabImageStereo(const cv::Mat &amp;imRectLeft,const cv::Mat &amp;imRectRight, const double &amp;timestamp); cv::Mat GrabImageRGBD(const cv::Mat &amp;imRGB,const cv::Mat &amp;imD, const double &amp;timestamp); cv::Mat GrabImageMonocular(const cv::Mat &amp;im, const double &amp;timestamp); 3.3 设置三个线程指针函数，关联三个线程123void SetLocalMapper(LocalMapping* pLocalMapper);void SetLoopClosing(LoopClosing* pLoopClosing);void SetViewer(Viewer* pViewer); 3.4 矫正相机 ChangeCalibration1234// Load new settings// The focal length should be similar or scale prediction will fail when projecting points// TODO: Modify MapPoint::PredictScale to take into account focal lenghtvoid ChangeCalibration(const string &amp;strSettingPath); 3.5 构造函数12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 3.6 InformOnlyTracking() 设置是否只跟踪12// Use this function if you have deactivated local mapping and you only want to localize the camera. void InformOnlyTracking(const bool &amp;flag); 3.7 Reset() 函数清除所有地图点1void Reset(); 3.8 Track()12// Main tracking function. It is independent of the input sensor.void Track(); 跟踪线程的主函数，独立于传感器类型 3.9 Track初始化函数12345// Map initialization for stereo and RGB-Dvoid StereoInitialization();// Map initialization for monocularvoid MonocularInitialization(); 3.10 构造函数12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 3.11 初始化単目地图1void CreateInitialMapMonocular(); 3.12 检查1void CheckReplacedInLastFrame(); 3.13 构造函数12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 3.14 通过上一参考关键帧来跟踪图像位姿 TrackReferenceKeyFrame()1bool TrackReferenceKeyFrame(); 步骤一：将当前帧的描述子转化为BoW向量123// Compute Bag of Words vector// 步骤1：将当前帧的描述子转化为BoW向量mCurrentFrame.ComputeBoW(); 步骤二：通过特征点的BoW加快当前帧与参考关键帧之间的特征点匹配12345678// We perform first an ORB matching with the reference keyframe // If enough matches are found we setup a PnP solver ORBmatcher matcher(0.7,true); vector&lt;MapPoint*&gt; vpMapPointMatches; // 步骤2：通过特征点的BoW加快当前帧与参考帧之间的特征点匹配 // 特征点的匹配关系由MapPoints进行维护 int nmatches = matcher.SearchByBoW(mpReferenceKF,mCurrentFrame,vpMapPointMatches); 步骤三：检查特征匹配个数是否达到要求12if(nmatches&lt;15) return false; 步骤四：将上一帧的位姿态作为当前帧位姿的初始值，这个时候的位姿都是相对于InitKeyFrame,初始关键帧，即世界坐标系123// 步骤3:将上一帧的位姿态作为当前帧位姿的初始值 mCurrentFrame.mvpMapPoints = vpMapPointMatches; mCurrentFrame.SetPose(mLastFrame.mTcw); // 用上一次的Tcw设置初值，在PoseOptimization可以收敛快一些 步骤五：通过通过优化3D-2D的重投影误差来获得位姿，优化的时候，3D点的坐标是相对于世界坐标系的，所以优化得到的位姿也是相对于世界坐标系的12// 步骤4:通过优化3D-2D的重投影误差来获得位姿 Optimizer::PoseOptimization(&amp;mCurrentFrame); 步骤六：剔除优化后的outlier匹配点（MapPoints）,注意最后一步要求该地图点至少被一个关键帧观察到过,才认为该点是成功匹配点，初始化的时候已经建立了初始的地图点观测信息以及帧之间的连接关系123456789101112131415161718192021// Discard outliers // 步骤5：剔除优化后的outlier匹配点（MapPoints） int nmatchesMap = 0; for(int i =0; i&lt;mCurrentFrame.N; i++) &#123; if(mCurrentFrame.mvpMapPoints[i]) &#123; if(mCurrentFrame.mvbOutlier[i]) &#123; MapPoint* pMP = mCurrentFrame.mvpMapPoints[i]; mCurrentFrame.mvpMapPoints[i]=static_cast&lt;MapPoint*&gt;(NULL); mCurrentFrame.mvbOutlier[i]=false; pMP-&gt;mbTrackInView = false; pMP-&gt;mnLastFrameSeen = mCurrentFrame.mnId; nmatches--; &#125; else if(mCurrentFrame.mvpMapPoints[i]-&gt;Observations()&gt;0) nmatchesMap++; &#125; &#125; 步骤七：返回匹配结果是否达到要求1return nmatchesMap&gt;=10; 3.15 更新上一帧1void UpdateLastFrame(); 3.16 通过上一帧来跟踪图像位姿，利用匀速模型1234567891011/** * @brief 根据匀速度模型对上一帧的MapPoints进行跟踪 * * 1. 非单目情况，需要对上一帧产生一些新的MapPoints（临时） * 2. 将上一帧的MapPoints投影到当前帧的图像平面上，在投影的位置进行区域匹配 * 3. 根据匹配对估计当前帧的姿态 * 4. 根据姿态剔除误匹配 * @return 如果匹配数大于10，返回true * @see V-B Initial Pose Estimation From Previous Frame */bool TrackWithMotionModel(); 3.17 跟踪丢失重定位1bool Relocalization(); 3.18 更新局部地图信息，更新局部地图，更新局部3D点，更新局部地图关键帧123void UpdateLocalMap(); // 先调用UpdateLocalKeyFrames()，再调用UpdateLocalPoints()，最后设置mvpReferenceMapPointsvoid UpdateLocalPoints();void UpdateLocalKeyFrames(); 更新局部关键帧信息 UpdateLocalKeyFrames() 步骤1：遍历当前帧的MapPoints，记录所有能观测到当前帧MapPoints的关键帧123456789101112131415161718192021map&lt;KeyFrame*,int&gt; keyframeCounter;//先统计能看到当前帧mappoints的各个关键帧，map的键是关键帧，值是观察到该帧地图点的个数//遍历所有的地图点，看有哪些帧能够观测到该地图点，让该帧的值加一for(int i=0; i&lt;mCurrentFrame.N; i++)&#123; if(mCurrentFrame.mvpMapPoints[i]) &#123; MapPoint* pMP = mCurrentFrame.mvpMapPoints[i]; if(!pMP-&gt;isBad()) &#123; // 能观测到当前帧MapPoints的关键帧 const map&lt;KeyFrame*,size_t&gt; observations = pMP-&gt;GetObservations(); for(map&lt;KeyFrame*,size_t&gt;::const_iterator it=observations.begin(), itend=observations.end(); it!=itend; it++) keyframeCounter[it-&gt;first]++; &#125; else &#123; mCurrentFrame.mvpMapPoints[i]=NULL; &#125; &#125;&#125; 如果keyframeCounter为空，直接结束12if(keyframeCounter.empty()) return; 寻找共视程度最高的关键帧，即和该帧拥有最多共同地图点的关键帧pKFmax12345678910111213141516171819202122232425262728int max=0;KeyFrame* pKFmax= static_cast&lt;KeyFrame*&gt;(NULL);// 步骤2：更新局部关键帧（mvpLocalKeyFrames），添加局部关键帧有三个策略// 先清空局部关键帧mvpLocalKeyFrames.clear();mvpLocalKeyFrames.reserve(3*keyframeCounter.size());// All keyframes that observe a map point are included in the local map. Also check which keyframe shares most points// V-D K1: shares the map points with current frame// 策略1：能观测到当前帧MapPoints的关键帧作为局部关键帧for(map&lt;KeyFrame*,int&gt;::const_iterator it=keyframeCounter.begin(), itEnd=keyframeCounter.end(); it!=itEnd; it++)&#123; KeyFrame* pKF = it-&gt;first; if(pKF-&gt;isBad()) continue; if(it-&gt;second&gt;max) &#123; max=it-&gt;second; pKFmax=pKF; &#125; mvpLocalKeyFrames.push_back(it-&gt;first); // mnTrackReferenceForFrame防止重复添加局部关键帧 pKF-&gt;mnTrackReferenceForFrame = mCurrentFrame.mnId;&#125; 步骤2：与策略1得到的局部关键帧共视程度很高的关键帧作为局部关键帧(最佳共视的10帧，自己的子关键帧，自己的父关键帧) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// Include also some not-already-included keyframes that are neighbors to already-included keyframes// V-D K2: neighbors to K1 in the covisibility graph// 策略2：与策略1得到的局部关键帧共视程度很高的关键帧作为局部关键帧for(vector&lt;KeyFrame*&gt;::const_iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++)&#123; // Limit the number of keyframes if(mvpLocalKeyFrames.size()&gt;80) break; KeyFrame* pKF = *itKF; // 策略2.1:最佳共视的10帧 const vector&lt;KeyFrame*&gt; vNeighs = pKF-&gt;GetBestCovisibilityKeyFrames(10); for(vector&lt;KeyFrame*&gt;::const_iterator itNeighKF=vNeighs.begin(), itEndNeighKF=vNeighs.end(); itNeighKF!=itEndNeighKF; itNeighKF++) &#123; KeyFrame* pNeighKF = *itNeighKF; if(!pNeighKF-&gt;isBad()) &#123; // mnTrackReferenceForFrame防止重复添加局部关键帧 if(pNeighKF-&gt;mnTrackReferenceForFrame!=mCurrentFrame.mnId) &#123; mvpLocalKeyFrames.push_back(pNeighKF); pNeighKF-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; break; &#125; &#125; &#125; // 策略2.2:自己的子关键帧 const set&lt;KeyFrame*&gt; spChilds = pKF-&gt;GetChilds(); for(set&lt;KeyFrame*&gt;::const_iterator sit=spChilds.begin(), send=spChilds.end(); sit!=send; sit++) &#123; KeyFrame* pChildKF = *sit; if(!pChildKF-&gt;isBad()) &#123; if(pChildKF-&gt;mnTrackReferenceForFrame!=mCurrentFrame.mnId) &#123; mvpLocalKeyFrames.push_back(pChildKF); pChildKF-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; break; &#125; &#125; &#125; // 策略2.3:自己的父关键帧 KeyFrame* pParent = pKF-&gt;GetParent(); if(pParent) &#123; // mnTrackReferenceForFrame防止重复添加局部关键帧 if(pParent-&gt;mnTrackReferenceForFrame!=mCurrentFrame.mnId) &#123; mvpLocalKeyFrames.push_back(pParent); pParent-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; break; &#125; &#125;&#125; 步骤3：更新当前帧的参考关键帧，与自己共视程度最高的关键帧作为参考关键帧 12345if(pKFmax)&#123; mpReferenceKF = pKFmax; mCurrentFrame.mpReferenceKF = mpReferenceKF;&#125; 更新局部3D点信息 UpdateLocalPoints()根据已经更新的局部关键帧mvpLocalKeyFrames的MapPoints，更新mvpLocalMapPoints 步骤一：清空局部Tracking线程mptracker的MapPoints 12// 步骤1：清空局部MapPoints mvpLocalMapPoints.clear(); 步骤二：遍历mvpLocalKeyFrames，提取每一帧的地图点 12// 步骤2：遍历局部关键帧mvpLocalKeyFramesfor(vector&lt;KeyFrame*&gt;::const_iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++) 步骤三：提取每一帧的地图点 1const vector&lt;MapPoint*&gt; vpMPs = pKF-&gt;GetMapPointMatches(); 步骤四：将每一帧的地图点都加入mvpLocalMapPoints 123456789101112131415// 步骤2：将局部关键帧的MapPoints添加到mvpLocalMapPoints for(vector&lt;MapPoint*&gt;::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end(); itMP!=itEndMP; itMP++) &#123; MapPoint* pMP = *itMP; if(!pMP) continue; // mnTrackReferenceForFrame防止重复添加局部MapPoint if(pMP-&gt;mnTrackReferenceForFrame==mCurrentFrame.mnId) continue; if(!pMP-&gt;isBad()) &#123; mvpLocalMapPoints.push_back(pMP); pMP-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; &#125; &#125; 3.19 跟踪局部地图 TrackLocalMap() 跟踪局部地图主函数1bool TrackLocalMap(); 步骤一：更新局部关键帧mvpLocalKeyFrames和局部地图点mvpLocalMapPoints UpdateLocalMap(); 步骤二：在局部地图中查找与当前帧匹配的MapPoints SearchLocalPoints(); 步骤三：更新局部所有MapPoints后对位姿再次优化 Optimizer::PoseOptimization(&amp;mCurrentFrame); 步骤四：更新当前帧的MapPoints被观测程度，并统计跟踪局部地图的效果，只有当该地图点被其他帧观测到过，才认为匹配正确 1234567891011121314151617181920212223for(int i=0; i&lt;mCurrentFrame.N; i++) &#123; if(mCurrentFrame.mvpMapPoints[i]) &#123; // 由于当前帧的MapPoints可以被当前帧观测到，其被观测统计量加1 if(!mCurrentFrame.mvbOutlier[i]) &#123; mCurrentFrame.mvpMapPoints[i]-&gt;IncreaseFound(); if(!mbOnlyTracking) &#123; // 该MapPoint被其它关键帧观测到过 if(mCurrentFrame.mvpMapPoints[i]-&gt;Observations()&gt;0) mnMatchesInliers++; &#125; else // 记录当前帧跟踪到的MapPoints，用于统计跟踪效果 mnMatchesInliers++; &#125; else if(mSensor==System::STEREO) mCurrentFrame.mvpMapPoints[i] = static_cast&lt;MapPoint*&gt;(NULL); &#125; &#125; 步骤五：决定是否跟踪成功，重定位的话特征匹配点数大于50，一般情况下特征匹配点数大于30才认为跟踪成功 1234567if(mCurrentFrame.mnId&lt;mnLastRelocFrameId+mMaxFrames &amp;&amp; mnMatchesInliers&lt;50) return false;if(mnMatchesInliers&lt;30) return false;else return true; 3.20 寻找局部地图点 SearchLocalPoints()更新完局部关键帧和局部地图后，对Local MapPoints进行跟踪，在局部地图中查找在当前帧视野范围内的点，将视野范围内的点和当前帧的特征点进行投影匹配1void SearchLocalPoints(); 步骤一： 1234567891011121314151617181920212223// Do not search map points already matched// 步骤1：遍历当前帧的mvpMapPoints，标记这些MapPoints不参与之后的搜索// 因为当前的mvpMapPoints一定在当前帧的视野中for(vector&lt;MapPoint*&gt;::iterator vit=mCurrentFrame.mvpMapPoints.begin(), vend=mCurrentFrame.mvpMapPoints.end(); vit!=vend; vit++)&#123; MapPoint* pMP = *vit; if(pMP) &#123; if(pMP-&gt;isBad()) &#123; *vit = static_cast&lt;MapPoint*&gt;(NULL); &#125; else &#123; // 更新能观测到该点的帧数加1 pMP-&gt;IncreaseVisible(); // 标记该点被当前帧观测到，为了避免重复添加 pMP-&gt;mnLastFrameSeen = mCurrentFrame.mnId; // 标记该点将来不被投影，因为已经匹配过，mbTrackInView = false 表示不进行投影 pMP-&gt;mbTrackInView = false; &#125; &#125;&#125; 步骤二： 统计哪些mapPoints需要进行重投影，nToMatch统计需要进行重投影的点的个数 123456789101112131415161718192021222324// Project points in frame and check its visibility // 步骤2：将所有局部MapPoints投影到当前帧，判断是否在视野范围内，然后进行投影匹配 for(vector&lt;MapPoint*&gt;::iterator vit=mvpLocalMapPoints.begin(), vend=mvpLocalMapPoints.end(); vit!=vend; vit++) &#123; MapPoint* pMP = *vit; // 已经被当前帧观测到MapPoint不再判断是否能被当前帧观测到 if(pMP-&gt;mnLastFrameSeen == mCurrentFrame.mnId) continue; if(pMP-&gt;isBad()) continue; // Project (this fills MapPoint variables for matching) // 步骤2.1：判断LocalMapPoints中的点是否在在视野内，如果该地图点在视野内，会将该点的mbTrackInView 设置为true，并且会预测它所对应的特征点在金字塔的哪一层，会估计出它在图像的坐标 if(mCurrentFrame.isInFrustum(pMP,0.5)) &#123; // 观测到该点的帧数加1，该MapPoint在某些帧的视野范围内 pMP-&gt;IncreaseVisible(); // 只有在视野范围内的MapPoints才参与之后的投影匹配 nToMatch++; &#125; &#125;``` #### 3.21 判定是否需要插入关键帧 bool NeedNewKeyFrame();1- 步骤一：如果只定位，那么不插入关键帧 // 步骤1：如果用户在界面上选择重定位，那么将不插入关键帧 // 由于插入关键帧过程中会生成MapPoint，因此用户选择重定位后地图上的点云和关键帧都不会再增加 if(mbOnlyTracking) return false; 1- 步骤二：如果局部地图被闭环检测使用，则不插入关键帧 // If Local Mapping is freezed by a Loop Closure do not insert keyframes // 如果局部地图被闭环检测使用，则不插入关键帧 if(mpLocalMapper-&gt;isStopped() || mpLocalMapper-&gt;stopRequested()) return false; 1- 步骤三：如果关键帧比较少，则考虑插入关键帧，或距离上一次重定位超过1s，则考虑插入关键帧 // Do not insert keyframes if not enough frames have passed from last relocalisation // 步骤2：判断是否距离上一次插入关键帧的时间太短 // mCurrentFrame.mnId是当前帧的ID // mnLastRelocFrameId是最近一次重定位帧的ID // mMaxFrames等于图像输入的帧率 // 如果关键帧比较少，则考虑插入关键帧 // 或距离上一次重定位超过1s，则考虑插入关键帧 if(mCurrentFrame.mnIdmMaxFrames) return false;12- 步骤四：查询局部地图管理器是否繁忙 // Local Mapping accept keyframes? // 步骤4：查询局部地图管理器是否繁忙 bool bLocalMappingIdle = mpLocalMapper-&gt;AcceptKeyFrames();12- 步骤五：对于双目或RGBD摄像头，统计总的可以添加的MapPoints数量和跟踪到地图中的MapPoints数量 // 步骤5：对于双目或RGBD摄像头，统计总的可以添加的MapPoints数量和跟踪到地图中的MapPoints数量 int nMap = 0; int nTotal= 0; if(mSensor!=System::MONOCULAR)// 双目或rgbd { for(int i =0; i0 &amp;&amp; mCurrentFrame.mvDepth[i]Observations()&gt;0) nMap++;// 被关键帧观测到的mappoints数，即观测到地图中的MapPoints数量 } } } else { // There are no visual odometry matches in the monocular case nMap=1; nTotal=1; } const float ratioMap = (float)nMap/(float)(std::max(1,nTotal)); 1- 步骤六：设定阈值，判定是否需要插入关键帧 // 步骤6：决策是否需要插入关键帧 // Thresholds // 设定inlier阈值，和之前帧特征点匹配的inlier比例 float thRefRatio = 0.75f; if(nKFs&lt;2) thRefRatio = 0.4f;// 关键帧只有一帧，那么插入关键帧的阈值设置很低 if(mSensor==System::MONOCULAR) thRefRatio = 0.9f; // MapPoints中和地图关联的比例阈值 float thMapRatio = 0.35f; if(mnMatchesInliers&gt;300) thMapRatio = 0.20f; // Condition 1a: More than &quot;MaxFrames&quot; have passed from last keyframe insertion // 很长时间没有插入关键帧 const bool c1a = mCurrentFrame.mnId&gt;=mnLastKeyFrameId+mMaxFrames; // Condition 1b: More than &quot;MinFrames&quot; have passed and Local Mapping is idle // localMapper处于空闲状态 const bool c1b = (mCurrentFrame.mnId&gt;=mnLastKeyFrameId+mMinFrames &amp;&amp; bLocalMappingIdle); // Condition 1c: tracking is weak // 跟踪要跪的节奏，0.25和0.3是一个比较低的阈值 const bool c1c = mSensor!=System::MONOCULAR &amp;&amp; (mnMatchesInliers&lt;nRefMatches*0.25 || ratioMap&lt;0.3f) ; // Condition 2: Few tracked points compared to reference keyframe. Lots of visual odometry compared to map matches. // 阈值比c1c要高，与之前参考帧（最近的一个关键帧）重复度不是太高 const bool c2 = ((mnMatchesInliers&lt;nRefMatches*thRefRatio || ratioMap&lt;thMapRatio) &amp;&amp; mnMatchesInliers&gt;15); if((c1a||c1b||c1c)&amp;&amp;c2) { // If the mapping accepts keyframes, insert keyframe. // Otherwise send a signal to interrupt BA if(bLocalMappingIdle) { return true; } else { mpLocalMapper-&gt;InterruptBA(); if(mSensor!=System::MONOCULAR) { // 队列里不能阻塞太多关键帧 // tracking插入关键帧不是直接插入，而且先插入到mlNewKeyFrames中， // 然后localmapper再逐个pop出来插入到mspKeyFrames if(mpLocalMapper-&gt;KeyframesInQueue()&lt;3) return true; else return false; } else return false; } } else return false; 123#### 3.22 创建新的关键帧 *CreateNewKeyFrame()* void CreateNewKeyFrame();1- 步骤一：将当前帧构造成关键帧 // 步骤1：将当前帧构造成关键帧 KeyFrame* pKF = new KeyFrame(mCurrentFrame,mpMap,mpKeyFrameDB);1- 步骤二：将当前关键帧设置为当前帧的参考关键帧 // 步骤2：将当前关键帧设置为当前帧的参考关键帧 // 在UpdateLocalKeyFrames函数中会将与当前关键帧共视程度最高的关键帧设定为当前帧的参考关键帧 mpReferenceKF = pKF; mCurrentFrame.mpReferenceKF = pKF;1- 步骤三：将该关键帧与其他信息相关联，对于双目，要重新生成地图点 mpLocalMapper-&gt;InsertKeyFrame(pKF); mpLocalMapper-&gt;SetNotStop(false); mnLastKeyFrameId = mCurrentFrame.mnId; mpLastKeyFrame = pKF; 123### 4. 实例化mpTracker在System构造函数中new一个Tracing对象指针mpTracker，方式如下所示： //Initialize the Tracking thread//(it will live in the main thread of execution, the one that called this constructor)mpTracker = new Tracking(this, mpVocabulary, mpFrameDrawer, mpMapDrawer, mpMap, mpKeyFrameDatabase, strSettingsFile, mSensor); 1#### 4.1 读取配置文件，构造相机内参 *mK* cv::FileStorage fSettings(strSettingPath, cv::FileStorage::READ); float fx = fSettings[“Camera.fx”]; float fy = fSettings[“Camera.fy”]; float cx = fSettings[“Camera.cx”]; float cy = fSettings[“Camera.cy”]; // |fx 0 cx| // K = |0 fy cy| // |0 0 1 | cv::Mat K = cv::Mat::eye(3,3,CV_32F); K.at&lt;float&gt;(0,0) = fx; K.at&lt;float&gt;(1,1) = fy; K.at&lt;float&gt;(0,2) = cx; K.at&lt;float&gt;(1,2) = cy; K.copyTo(mK); 1#### 4.2 读取配置文件，构造相机矫正向量 *mDistCoef* // 图像矫正系数 // [k1 k2 p1 p2 k3] cv::Mat DistCoef(4,1,CV_32F); DistCoef.at&lt;float&gt;(0) = fSettings[&quot;Camera.k1&quot;]; DistCoef.at&lt;float&gt;(1) = fSettings[&quot;Camera.k2&quot;]; DistCoef.at&lt;float&gt;(2) = fSettings[&quot;Camera.p1&quot;]; DistCoef.at&lt;float&gt;(3) = fSettings[&quot;Camera.p2&quot;]; const float k3 = fSettings[&quot;Camera.k3&quot;]; if(k3!=0) { DistCoef.resize(5); DistCoef.at&lt;float&gt;(4) = k3; } DistCoef.copyTo(mDistCoef); 1#### 4.3 读取配置文件，构造相机RGB参数 *mbRGB* // 1:RGB 0:BGR int nRGB = fSettings[“Camera.RGB”]; mbRGB = nRGB; if(mbRGB) cout &lt;&lt; &quot;- color order: RGB (ignored if grayscale)&quot; &lt;&lt; endl; else cout &lt;&lt; &quot;- color order: BGR (ignored if grayscale)&quot; &lt;&lt; endl; 1#### 4.4 读取配置文件，加载ORB参数 *nFeatures*、*fScaleFactor*、*nLevels*、*fIniThFAST*、*fMinThFAST* // Load ORB parameters // 每一帧提取的特征点数 1000 int nFeatures = fSettings[&quot;ORBextractor.nFeatures&quot;]; // 图像建立金字塔时的变化尺度 1.2 float fScaleFactor = fSettings[&quot;ORBextractor.scaleFactor&quot;]; // 尺度金字塔的层数 8 int nLevels = fSettings[&quot;ORBextractor.nLevels&quot;]; // 提取fast特征点的默认阈值 20 int fIniThFAST = fSettings[&quot;ORBextractor.iniThFAST&quot;]; // 如果默认阈值提取不出足够fast特征点，则使用最小阈值 8 int fMinThFAST = fSettings[&quot;ORBextractor.minThFAST&quot;]; 1#### 4.5 通过*nFeatures*、*fScaleFactor*、*nLevels*、*fIniThFAST*、*fMinThFAST*构造特征提取器 // tracking过程都会用到mpORBextractorLeft作为特征点提取器 mpORBextractorLeft = new ORBextractor(nFeatures,fScaleFactor,nLevels,fIniThFAST,fMinThFAST); // 如果是双目，tracking过程中还会用用到mpORBextractorRight作为右目特征点提取器 if(sensor==System::STEREO) mpORBextractorRight = new ORBextractor(nFeatures,fScaleFactor,nLevels,fIniThFAST,fMinThFAST); // 在单目初始化的时候，会用mpIniORBextractor来作为特征点提取器 if(sensor==System::MONOCULAR) mpIniORBextractor = new ORBextractor(2*nFeatures,fScaleFactor,nLevels,fIniThFAST,fMinThFAST); 1#### 4.6 读取配置文件，构造相机深度截断阈值和视差因子 *mThDepth*,*mDepthMapFactor* if(sensor==System::STEREO || sensor==System::RGBD) { // 判断一个3D点远/近的阈值 mbf * 35 / fx mThDepth = mbf*(float)fSettings[&quot;ThDepth&quot;]/fx; cout &lt;&lt; endl &lt;&lt; &quot;Depth Threshold (Close/Far Points): &quot; &lt;&lt; mThDepth &lt;&lt; endl; } if(sensor==System::RGBD) { // 深度相机disparity转化为depth时的因子 mDepthMapFactor = fSettings[&quot;DepthMapFactor&quot;]; if(fabs(mDepthMapFactor)&lt;1e-5) mDepthMapFactor=1; else mDepthMapFactor = 1.0f/mDepthMapFactor; } ``` st=>start: 调用构造函数实例化mpTracker e=>end: 得到mCurrentFrame.mTcw op1=>operation: 循环SLAM.TrackMonocular(im,tframe)，对每一帧图像进行tracking op2=>operation: 调用mpTracker->GrabImageMonocular(im,timestamp),抓取每一帧图像 op3=>operation: 将RGB图转换为灰度图mImGray op4=>operation: 利用灰度图构造当前帧mCurrentFrame op5=>operation: 调用track()函数 st->op1->op2->op3->op4->op5->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: 调用构造函数实例化 mpTracker e=>end: 得到mpTracker op1=>operation: 读取配置文件，构造相机内参 mK,相机矫正向量 mDistCoef ,相机RGB参数 mbRGB op2=>operation: 读取配置文件，加载ORB参数 nFeatures、fScaleFactor、nLevels、fIniThFAST、fMinThFAST op3=>operation: 构造特征提取器 mpORBextractorLeft、mpIniORBextractor op4=>operation: 读取配置文件, 构造相机深度截断阈值和视差因子 mThDepth、mDepthMapFactor st->op1->op2->op3->op4->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析一]]></title>
    <url>%2F2019%2F05%2F28%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%80%2F</url>
    <content type="text"><![CDATA[主函数说明 mono_kitty.cc1. main 入口函数，读取3个文件参数，初始化系统 strVocFile: 字典词包的路径 strSettingFile: 系统中装有一些如相机参数、view窗口的配置文件，格式为YAML strSequence: 数据集路径 2. LoadImages 函数 LoadImages(const string &amp;strPathToSequence, vector &amp;vstrImageFilenames, vector &amp;vTimestamps) 12345678910111213141516171819202122232425262728293031 void LoadImages(const string &amp;strPathToSequence, vector&lt;string&gt; &amp;vstrImageFilenames, vector&lt;double&gt; &amp;vTimestamps)&#123; ifstream fTimes; string strPathTimeFile = strPathToSequence + "/times.txt"; fTimes.open(strPathTimeFile.c_str()); while(!fTimes.eof()) &#123; string s; getline(fTimes,s); if(!s.empty()) &#123; stringstream ss; ss &lt;&lt; s; double t; ss &gt;&gt; t; vTimestamps.push_back(t); &#125; &#125; string strPrefixLeft = strPathToSequence + "/image_0/"; const int nTimes = vTimestamps.size(); vstrImageFilenames.resize(nTimes); for(int i=0; i&lt;nTimes; i++) &#123; stringstream ss; ss &lt;&lt; setfill('0') &lt;&lt; setw(6) &lt;&lt; i; vstrImageFilenames[i] = strPrefixLeft + ss.str() + ".png"; &#125;&#125; 加载数据集函数，函数执行完vstrImageFileNames是一个存有图片具体位置的vector，位置形式如xxx/xxx/000xxx.png，vTimestamps是存有图片时间戳的vector 3. 实例化 SLAM 系统加载图片路径完成后，需要实例化一个SLAM系统对象ORB_SLAM2::System SLAM(argv[1],argv[2],ORB_SLAM2::System::MONOCULAR,true); System.h 包含了7个类，分别是Viewer， FrameDrawer, Map, Tracking, LocalMapping, LoopClosing 的声明，和System 类的定义， 就像描述的那样，这些类组成了一个系统。 1234567class Viewer;class FrameDrawer;class Map;class Tracking;class LocalMapping;class LoopClosing;class System; 成员变量说明3.1 sensor 枚举12345enum eSensor&#123; MONOCULAR=0, STEREO=1, RGBD=2&#125;; 0,1,2 分别代表传感器的类型 3.2 System 构造函数System(const string &amp;strVocFile, const string &amp;strSettingsFile, const eSensor sensor, const bool bUseViewer = true); Monocular System 构造时，读入词包路径，YAML配置文件，设置eSensor类型为Monocular，并启用Viewer线程 3.3 Tracking 函数123456789101112131415// Proccess the given stereo frame. Images must be synchronized and rectified.// Input images: RGB (CV_8UC3) or grayscale (CV_8U). RGB is converted to grayscale.// Returns the camera pose (empty if tracking fails).cv::Mat TrackStereo(const cv::Mat &amp;imLeft, const cv::Mat &amp;imRight, const double &amp;timestamp);// Process the given rgbd frame. Depthmap must be registered to the RGB frame.// Input image: RGB (CV_8UC3) or grayscale (CV_8U). RGB is converted to grayscale.// Input depthmap: Float (CV_32F).// Returns the camera pose (empty if tracking fails).cv::Mat TrackRGBD(const cv::Mat &amp;im, const cv::Mat &amp;depthmap, const double &amp;timestamp);// Proccess the given monocular frame// Input images: RGB (CV_8UC3) or grayscale (CV_8U). RGB is converted to grayscale.// Returns the camera pose (empty if tracking fails).cv::Mat TrackMonocular(const cv::Mat &amp;im, const double &amp;timestamp); 针对不同传感器不同的Tracking。输入图像可以使rgb的也可以是grayscale的（最终读进去都会转化为grayscale的），函数返回值为camera的位姿pose。Tracking 过程是对针对每一幅图像，通过先初始化然后track和优化过程来估计相机误差。 3.4 定位模式函数1234// This stops local mapping thread (map building) and performs only camera tracking.void ActivateLocalizationMode();// This resumes local mapping thread and performs SLAM again.void DeactivateLocalizationMode(); 调用ActivateLocalizationMode()将终止mapping线程，开启定位模式，调用后者重启mapping线程。 3.5 重启与终止函数1234567// Reset the system (clear map)void Reset();// All threads will be requested to finish.// It waits until all threads have finished.// This function must be called before saving the trajectory.void Shutdown(); Reset()函数将清空map，Shutdown()函数可以终止所有线程，在保存相机轨迹之前需要调用此函数。 3.6 SaveTrajectory 函数123456789101112131415161718192021// Save camera trajectory in the TUM RGB-D dataset format.// Only for stereo and RGB-D. This method does not work for monocular.// Call first Shutdown()// See format details at: http://vision.in.tum.de/data/datasets/rgbd-datasetvoid SaveTrajectoryTUM(const string &amp;filename);// Save keyframe poses in the TUM RGB-D dataset format.// This method works for all sensor input.// Call first Shutdown()// See format details at: http://vision.in.tum.de/data/datasets/rgbd-datasetvoid SaveKeyFrameTrajectoryTUM(const string &amp;filename);// Save camera trajectory in the KITTI dataset format.// Only for stereo and RGB-D. This method does not work for monocular.// Call first Shutdown()// See format details at: http://www.cvlibs.net/datasets/kitti/eval_odometry.phpvoid SaveTrajectoryKITTI(const string &amp;filename);// TODO: Save/Load functions// SaveMap(const string &amp;filename);// LoadMap(const string &amp;filename); 把相机轨迹保存成相应数据集的格式，系统调用此函数时先shutdown SLAM系统，mono_kittti中save函数用的是SaveKeyFrameTrajectoryTUM，这个函数看起来像是只能用于TUM数据集，但三种传感器均适合。 4 System private 成员变量说明4.1 eSensor12// Input sensoreSensor mSensor; 输入的传感器类型 4.2 mpVocabulary12// ORB vocabulary used for place recognition and feature matching.ORBVocabulary* mpVocabulary; 用于位置识别和特征匹配的系统词包 4.3 mpKeyFrameDatabase12// KeyFrame database for place recognition (relocalization and loop detection).KeyFrameDatabase* mpKeyFrameDatabase; 用于位置识别，重定位，回环检测的关键帧数据集 4.4 mpMap12// Map structure that stores the pointers to all KeyFrames and MapPoints.Map* mpMap; 存储系统关键帧的指针和地图点的指针 4.5 mpTracker1234// Tracker. It receives a frame and computes the associated camera pose.// It also decides when to insert a new keyframe, create some new MapPoints and// performs relocalization if tracking fails.Tracking* mpTracker; Tracker 接受一帧图像并计算相机位姿，决定什么时候需要插入关键帧，创建地图点并且执行重定位如果跟踪失败。 4.6 mpLocalMapper12// Local Mapper. It manages the local map and performs local bundle adjustment.LocalMapping* mpLocalMapper; 局部地图管理器，mpLocalMapper，管理局部地图并进行局部BA。 4.7 mpLoopCloser123// Loop Closer. It searches loops with every new keyframe. If there is a loop it performs// a pose graph optimization and full bundle adjustment (in a new thread) afterwards.LoopClosing* mpLoopCloser; 回环检测器，每次获取关键帧后都会进行回环检测，如果存在回环的话就执行位姿图的优化并且进行全局BA优化 4.8 mpViewer,mpFrameDrawer,mpMapDrawer12345// The viewer draws the map and the current camera pose. It uses Pangolin.Viewer* mpViewer;FrameDrawer* mpFrameDrawer;MapDrawer* mpMapDrawer; 视图显示 4.9 系统线程12345// System threads: Local Mapping, Loop Closing, Viewer.// The Tracking thread "lives" in the main execution thread that creates the System object.std::thread* mptLocalMapping;std::thread* mptLoopClosing;std::thread* mptViewer; 4.10 Reset flag123// Reset flagstd::mutex mMutexReset;bool mbReset; 4.11 Change mode flags1234// Change mode flagsstd::mutex mMutexMode;bool mbActivateLocalizationMode;bool mbDeactivateLocalizationMode; 5. 实例化SLAM-System构造函数123System::System(const string &amp;strVocFile, const string &amp;strSettingsFile, const eSensor sensor, const bool bUseViewer):mSensor(sensor),mbReset(false),mbActivateLocalizationMode(false), mbDeactivateLocalizationMode(false) System 构造函数用于实例化一个SALM系统，开启相机跟踪(Tracking)，局部建图(Local Mapping)，回环检测(Loop Closing)，和可视化界面(Viewer)的线程。 5.1 初始形参传递1234mSensor(sensor),mbReset(false),mbActivateLocalizationMode(false),mbDeactivateLocalizationMode(false) sensor是传进来的形参，是前面枚举体中三种传感器的一个，这里为MONOCULAR，它传递给了mSensor，这是一个System类的隐含成员变量，两种变量类型一样。mpViewer是System类的隐含成员变量，Viewer类指针，这里赋空。mbReset，mbActivateLocalizationMode，mbDeactivateLocalizationMode均为bool型，赋false。 5.2 初始化数据库 1 初始化词包 mpVocabulary 123456789101112131415161718//Load ORB Vocabulary cout &lt;&lt; endl &lt;&lt; "Loading ORB Vocabulary. This could take a while..." &lt;&lt; endl; mpVocabulary = new ORBVocabulary(); bool bVocLoad = false; // chose loading method based on file extension if (has_suffix(strVocFile, ".txt")) bVocLoad = mpVocabulary-&gt;loadFromTextFile(strVocFile); else if(has_suffix(strVocFile, ".bin")) bVocLoad = mpVocabulary-&gt;loadFromBinaryFile(strVocFile); else bVocLoad = false; if(!bVocLoad) &#123; cerr &lt;&lt; "Wrong path to vocabulary. " &lt;&lt; endl; cerr &lt;&lt; "Failed to open at: " &lt;&lt; strVocFile &lt;&lt; endl; exit(-1); &#125; cout &lt;&lt; "Vocabulary loaded!" &lt;&lt; endl &lt;&lt; endl; 2 用词包数据库来初始化关键帧数据库（用于重定位和回环检测）mpKeyFrameDatabase 12//Create KeyFrame Database mpKeyFrameDatabase = new KeyFrameDatabase(*mpVocabulary); 3 初始化一个Map类对象 ，该类用于存储指向所有关键帧和地图点的指针 mpMap 12//Create the Map mpMap = new Map(); 4 初始化画图工具，用于可视化 mpFrameDrawer、mpMapDrawer 123//Create Drawers. These are used by the Viewer mpFrameDrawer = new FrameDrawer(mpMap); mpMapDrawer = new MapDrawer(mpMap, strSettingsFile); 5 初始化Tracking线程，主线程，使用this指针（只初始化不启动，启动在main函数里TrackMonocular()启动） 1234//Initialize the Tracking thread//(it will live in the main thread of execution, the one that called this constructor) mpTracker = new Tracking(this, mpVocabulary, mpFrameDrawer, mpMapDrawer, mpMap, mpKeyFrameDatabase, strSettingsFile, mSensor); 6 初始化Local Mapping线程并启动（这里mSensor传入MONOCULAR）mpLocalMapper 123//Initialize the Local Mapping thread and launch mpLocalMapper = new LocalMapping(mpMap, mSensor==MONOCULAR); mptLocalMapping = new thread(&amp;ORB_SLAM2::LocalMapping::Run,mpLocalMapper); 7 初始化Loop Closing线程并启动（这里mSensor传入的不是MONOCULAR）mptLoopClosing 123//Initialize the Loop Closing thread and launchmpLoopCloser = new LoopClosing(mpMap, mpKeyFrameDatabase, mpVocabulary, mSensor!=MONOCULAR);mptLoopClosing = new thread(&amp;ORB_SLAM2::LoopClosing::Run, mpLoopCloser); 8 初始化Viewer线程并启动，也使用了this指针；给Tracking线程设置Viewer 123456//Initialize the Viewer thread and launch mpViewer = new Viewer(this, mpFrameDrawer,mpMapDrawer,mpTracker,strSettingsFile); if(bUseViewer) mptViewer = new thread(&amp;Viewer::Run, mpViewer); mpTracker-&gt;SetViewer(mpViewer); 9 mpTracker，mpLocalMapper，mptLoopClosing三个线程每两个线程之间设置指针相互关联 123456789//Set pointers between threads mpTracker-&gt;SetLocalMapper(mpLocalMapper); mpTracker-&gt;SetLoopClosing(mpLoopCloser); mpLocalMapper-&gt;SetTracker(mpTracker); mpLocalMapper-&gt;SetLoopCloser(mpLoopCloser); mpLoopCloser-&gt;SetTracker(mpTracker); mpLoopCloser-&gt;SetLocalMapper(mpLocalMapper); 6. 循环Tracking12345678910111213141516171819202122232425262728293031323334353637383940414243444546 // Main loop cv::Mat im; for(int ni=0; ni&lt;nImages; ni++) &#123; // Read image from file im = cv::imread(vstrImageFilenames[ni],CV_LOAD_IMAGE_UNCHANGED); double tframe = vTimestamps[ni]; if(im.empty()) &#123; cerr &lt;&lt; endl &lt;&lt; "Failed to load image at: " &lt;&lt; vstrImageFilenames[ni] &lt;&lt; endl; return 1; &#125;#ifdef COMPILEDWITHC11 std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();#else std::chrono::monotonic_clock::time_point t1 = std::chrono::monotonic_clock::now();#endif // Pass the image to the SLAM system SLAM.TrackMonocular(im,tframe);#ifdef COMPILEDWITHC11 std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();#else std::chrono::monotonic_clock::time_point t2 = std::chrono::monotonic_clock::now();#endif double ttrack= std::chrono::duration_cast&lt;std::chrono::duration&lt;double&gt; &gt;(t2 - t1).count(); vTimesTrack[ni]=ttrack; // Wait to load the next frame double T=0; if(ni&lt;nImages-1) T = vTimestamps[ni+1]-tframe; else if(ni&gt;0) T = tframe-vTimestamps[ni-1]; if(ttrack&lt;T) this_thread::sleep_for(std::chrono::microseconds((int)((T-ttrack)*1e6))); &#125; // Stop all threads SLAM.Shutdown(); 上述分为两步：读图、Tracking，其中有一部分代码（注释 //Wait to load the next frame 后）目的是为了模拟真实时间状况，如果tracking过快，则下一帧可能还没来，所以要“睡” T-ttrack 秒等待装载下一帧图片。每次tracking只处理一帧图片。]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git简单攻略]]></title>
    <url>%2F2019%2F05%2F24%2Fgit%E7%AE%80%E5%8D%95%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[git全局用户申明12git config --global user.name "Your Name"git config --global user.email "email@example.com" 创建管理库1git init 添加文件1git add reamde.md 提交文件到仓库1git commit -m "message" 为什么git提交文件需要add和commit两步呢，因为commit可以一次提交很多次add不同的文件，比如123git add file1.txtgit add file2.txt file3.txtgit commit -m "add 3files" 查看仓库状态1git status 查看文件修改内容1git diff readme.txt 查看提交历史1git log 以便确定回退到哪个版本。 查看命令历史1git reflog 以便确定回到未来的哪个版本。 版本指针HEAD指向的版本就是当前的版本，HEAD^指向前一个版本，HEAD^^指向前前版本，HEAD~100指向第前100个版本。因此，git允许我们在历史之间穿梭。 版本穿梭1git reset --hard commit_id 丢弃工作区的修改1git checkout -- file 命令git checkout -- readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 从暂存区回到工作区1git reset HEAD readme.txt 如果你把文件git add到暂存区，但是还没有git commit到仓库，可以使用git reset HEAD file 将暂存区的修改撤销掉，重新放回到工作区。git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。 小结场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout — file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。 删除文件12git rm test.txtgit commit -m "remove test.txt" 一般情况下，你通常直接在文件管理器中把没用的文件删了，或者用rm命令删了：rm test.txt。这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了。 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit，现在，文件就从版本库中被删除了。 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本：1git checkout -- test.txt 小提示：先手动删除文件，然后使用git rm 和git add效果是一样的。注意：从来没有被添加到版本库就被删除的文件，是无法恢复的！ 添加远程仓库1git remote add origin git@github.com:hahaha/hahaha.git 把本地库的所有内容推送到远程库上1git push -u origin master 把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 提交仓库到到远程1git push origin master 从远程库克隆1git clone git@github.com:hahaha/gitskills.git 现在，假设我们从零开发，那么最好的方式是先创建远程库，然后，从远程库克隆。首先，登陆GitHub，创建一个新的仓库，名字叫gitskills：我们勾选Initialize this repository with a README，这样GitHub会自动为我们创建一个README.md文件。创建完毕后，可以看到README.md文件：现在，远程库已经准备好了，下一步是用命令git clone克隆一个本地库： 注意把Git库的地址换成你自己的，然后进入gitskills目录看看，已经有README.md文件了： 转载申明本文转载自廖雪峰的博客：[https://www.liaoxuefeng.com]]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EKF详解]]></title>
    <url>%2F2019%2F05%2F23%2FEKF%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. 高斯函数\begin{equation}p(x) = \det(2\pi\Sigma)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \}}\label{eq:Gaussion}\end{equation} 所有的高斯技术都共享了基本思想，即置信度用多元正态分布来表示。$x$的密度用两个参数来表示，均值$\mu$和协方差$\Sigma$，均值$\mu$是一个向量，它与状态$x$的维数相同。协方差是对称半正定的二次型。其维数等于状态$x$的维数的二次方。高斯滤波中的参数均值和方差称为矩参数，这是因为均值和方差是概率分布的一阶矩和二阶矩；正态分布的其他矩都是零。 2. 线性高斯系统KF是由Swerling（1950）和Kalman（1960）作为线性高斯系统中的预测和滤波技术而发明的，是用矩来定义的。KF用矩参数来表示置信度：在时刻$t$，置信度用均值$\mu_t$和方差$\Sigma_t$表示、如果除了贝叶斯滤波的马尔科夫假设以外，还具有如下的三个特性，则后验就是高斯的。 状态转移概率$p(x_t | u_t, x_{t-1})$必须是带有随机高斯噪声的参数的线性函数，可有下式表示:\begin{equation}x_t = A_tx_{t-1} + B_tu_t + \varepsilon_t\label{eq:motion}\end{equation}式中，$x_t$和$x_{t-1}$都是状态向量，它们都是$n$维列向量；$u_t$为时刻$t$的控制向量。式(2)中，$A_t$为$n \times n$的矩阵，$B_t$为$n \times m$的矩阵，$n$为状态向量$x_t$的维数，$m$为控制向量$u_t$的维数。式(2)中的随机变量\varepsilon_t是一个高斯随机向量，表示由状态转移引入的不确定性。其维数与状态向量维数相同，均值为0，方差用$R_t$表示。式(2)中的状态转移概率称为线性高斯，反映了它与带有附加高斯噪声的自变量呈线性关系。式(2)定义了状态转移概率$p(x_t | u_t, x_{t-1})$。这个概率可由公式(2)带入到多元正态分布的定义式(1)来得到。后验状态的均值由$A_tx_{t-1} + B_tu_t$给定，方差由$R_t$给定：\begin{equation}p(x_t | u_t, x_{t-1}) = \det(2\pi R_t)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(x_t-A_tx_{t-1} - B_tu_t)^TR_t^{-1}(x_t-A_tx_{t-1} - B_tu_t) \}}\label{eq:status}\end{equation} 观测概率$p(z_t | x_t)$也与带有高斯噪声的自变量呈线性关系：\begin{equation}z_t = C_tx_t + \delta _t\label{eq:project}\end{equation}式中，$C_t$为$k \times n$的矩阵，$k$为观测向量$z_t$的维数；向量$\delta _t$为观测噪声。$\delta _t$服从均值为0、方差为$Q_t$的多变量高斯分布。因此观测概率由下面的多元正态分布给定：\begin{equation}p(z_t | x_t) = \det(2\pi Q_t)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(z_t - C_tx_t)^TQ_t^{-1}(z_t-C_tx_t) \}}\label{eq:measure}\end{equation} 最后，初始置信度必须${\rm bel}(x_0)$必须是正态分布的。这里用$\mu_0$表示初始置信度的均值，用$\Sigma_0$表示协方差：\begin{equation}{\rm bel}(x_0) = p (x_0)= \det(2\pi \Sigma_0)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(x_0 - \mu_0)^T\Sigma_0^{-1}(x_0-\mu_0) \}}\label{eq:initial}\end{equation} 这三个假设足以保证后验${\rm bel}(x_t)$在任何时刻$t$总符合高斯分布。 3. KF算法(Kalman fliter algorithm)KF算法如图所示，KF表示均值为$\mu_t$、方差为$\Sigma_t$的状态量在时刻$t$的置信度{\rm bel}(x_t)。KF的输入是$t-1$时刻的置信度，其均值和方差分别用$\mu_{t-1}$和$\Sigma_{t-1}$表示。为了更新这些参数，KF需要控制向量$u_t$和测量向量$z_t$。输出的是时刻$t$的置信度，均值为$\mu_t$，方差为$\Sigma_t$。 Algorithm Kalman_filter($\mu_{t-1}$,$\Sigma_{t-1}$,$u_t$,$z_t$): 3.3 线性高斯系统]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>EKF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
</search>
