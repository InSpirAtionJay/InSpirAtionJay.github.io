<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[相机矩阵的Tips]]></title>
    <url>%2F2019%2F06%2F13%2F%E7%9B%B8%E6%9C%BA%E7%9F%A9%E9%98%B5%E7%9A%84tips%2F</url>
    <content type="text"><![CDATA[关于相机矩阵以及坐标系变换的Tips绕原点逆时针旋转$\theta$角度的旋转方程为 x' = \begin{bmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta \end{bmatrix} x设$T_{cw}$表示从世界坐标系到摄像机坐标系的变换矩阵，也就是说，对于齐次坐标： \begin{array}{c} \begin{bmatrix} P_c & 1 \end{bmatrix} = T_{cw} * { \begin{bmatrix} P_w & 1 \end{bmatrix} } \\ \begin{bmatrix} P_w & 1 \end{bmatrix} = T_{wc} * { \begin{bmatrix} P_c & 1 \end{bmatrix} } \end{array}对标准坐标来说，分解成旋转矩阵和平移向量也就是 \begin{array}{c} P_c = R_{cw} * P_w + t_{cw} \\ P_w = R_{wc} * P_c + t_{wc} \end{array}$R_{cw}$表示$T_{cw}$的左上角$3 \times 3$的矩阵，即1Rcw = Tcw.rowRange(0,3).colRange(0,3); $t_{cw}$表示$T_{cw}$的右上角$3 \times 1$的矩阵，即1tcw = Tcw.rowRange(0,3).col(3); 平移向量$T_{cw}$表示在相机坐标系中，相机坐标系到世界坐标系间的向量，向量方向由相机坐标系指向世界坐标系，即相机坐标系中，世界坐标系的原点坐标。旋转矩阵R中的角度表示平移完成后，从相机坐标系逆时针旋转至世界坐标系的角度。以二维坐标旋转为例，如下图所示 反过来，$R_{wc}$表示坐标从相机坐标系转换世界坐标系的旋转矩阵，$R_{wc}=R_{cw}^T=R_{cw}^{-1}$1Rwc = Rcw.t(); $t_{wc}$也记作$Ow$，表示世界坐标系下世界坐标系到相机坐标系间的向量, 向量方向由世界坐标系指向相机坐标系，即世界坐标系下，相机中心点的坐标。$t_{wc} = R_{cw}^T \ast t_{cw}$ 1Ow = -Rcw.t()*tcw;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>Camera</tag>
        <tag>Matrix</tag>
        <tag>System Transform</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析七]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%83%2F</url>
    <content type="text"><![CDATA[track()线程主函数详解此时track()函数已经完成了初始化，mstate = OK，mpInitializer已经初始化track()函数进入else分支 1. 正常初始化 mState==OK1.1 检查并更新上一帧被替换的MapPoints，Local Mapping线程可能会修改最后一帧图像的地图点1234// Local Mapping might have changed some MapPoints tracked in last frame // 检查并更新上一帧被替换的MapPoints // 更新Fuse函数和SearchAndFuse函数替换的MapPoints CheckReplacedInLastFrame(); 1.2 根据参考关键帧进行跟踪 TrackReferenceKeyFrame()123456789101112// 运动模型是空的或刚完成重定位// mCurrentFrame.mnId&lt;mnLastRelocFrameId+2这个判断不应该有???// 源代码注释说不应该有后面这个判断，我认为有必要，如果刚进行重定位，此时mVelocity不为空// 应该只要mVelocity不为空，就优先选择TrackWithMotionModel// mnLastRelocFrameId上一次重定位的那一帧if(mVelocity.empty() || mCurrentFrame.mnId&lt;mnLastRelocFrameId+2)&#123; // 将上一帧的位姿作为当前帧的初始位姿 // 通过BoW的方式在参考帧中找当前帧特征点的匹配点 // 优化每个特征点都对应3D点重投影误差即可得到位姿 bOK = TrackReferenceKeyFrame();&#125; 1.3 根据恒速模型和最后一帧图像进行跟踪 TrackWithMotionModel()，如果恒速模型不成功，再根据参考关键帧进行跟踪1234567891011else &#123; // 根据恒速模型设定当前帧的初始位姿 // 通过投影的方式在参考帧中找当前帧特征点的匹配点 // 优化每个特征点所对应3D点的投影误差即可得到位姿 bOK = TrackWithMotionModel(); if(!bOK) // TrackReferenceKeyFrame是跟踪参考帧，不能根据固定运动速度模型预测当前帧的位姿态，通过bow加速匹配（SearchByBow） // 最后通过优化得到优化后的位姿 bOK = TrackReferenceKeyFrame(); &#125; 2. 初始化完成后，特征点跟踪丢失，mState！=OK,此时进行重定位12345else &#123; // BOW搜索，PnP求解位姿 bOK = Relocalization(); &#125; 3. 跟踪完当前帧后，需要进行更新信息3.1 更新当前帧的参考关键帧为当前参考关键帧12// 将最新的关键帧作为reference frame mCurrentFrame.mpReferenceKF = mpReferenceKF; 3.2 更新局部地图1234567891011// If we have an initial estimation of the camera pose and matching. Track the local map.// 步骤2.2：在帧间匹配得到初始的姿态后，现在对local map进行跟踪得到更多的匹配，并优化当前位姿// local map:当前帧、当前帧的MapPoints、当前关键帧与其它关键帧共视关系// 在步骤2.1中主要是两两跟踪（恒速模型跟踪上一帧、跟踪参考帧），这里搜索局部关键帧后搜集所有局部MapPoints，// 然后将局部MapPoints和当前帧进行投影匹配，得到更多匹配的MapPoints后进行Pose优化if(!mbOnlyTracking)&#123; if(bOK) bOK = TrackLocalMap();&#125;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析六]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%85%AD%2F</url>
    <content type="text"><![CDATA[track()线程初始化过程详解Tracking.cpp里面的Track()函数可以说是 Tracking 线程最主体部分了，来看看调用Track()函数的过程： 1. 初始化初始器 mpInitializer1.1 第一次进Track()函数，处理系统的第一帧图像,此时state = NO_IMAGE_YET，赋值为state = NOT_INITIALIZED12345678910111213// track包含两部分：估计运动、跟踪局部地图// mState为tracking的状态机// SYSTME_NOT_READY, NO_IMAGE_YET, NOT_INITIALIZED, OK, LOST// 如果图像复位过、或者第一次运行，则为NO_IMAGE_YET状态if(mState==NO_IMAGES_YET)&#123; mState = NOT_INITIALIZED;&#125;// mLastProcessedState存储了Tracking最新的状态，用于FrameDrawer中的绘制mLastProcessedState=mState; 1.2 接下来 mState==NOT_INITIALIZED 执行単目初始化过程，进入MonocularInitialization();123456789101112if(mState==NOT_INITIALIZED)&#123; if(mSensor==System::STEREO || mSensor==System::RGBD) StereoInitialization(); else MonocularInitialization(); mpFrameDrawer-&gt;Update(this); if(mState!=OK) return;&#125; 1.3 此时単目初始器还没有建立，mpInitializer=nullptr,进入MonocularInitialization()函数if分支，需要保证单目初始帧的特征点数必须大于100123456789// 步骤1：得到用于初始化的第一帧，初始化需要两帧，处理第一帧时，mInitialFrame和mLastFrame都等于第一帧，也就是当前帧 mInitialFrame = Frame(mCurrentFrame); // 记录最近的一帧 mLastFrame = Frame(mCurrentFrame); // mvbPrevMatched最大的情况就是所有特征点都被跟踪上 mvbPrevMatched.resize(mCurrentFrame.mvKeysUn.size()); for(size_t i=0; i&lt;mCurrentFrame.mvKeysUn.size(); i++) mvbPrevMatched[i]=mCurrentFrame.mvKeysUn[i].pt; 1.4 构造初始器 mpInitializer12// 由当前帧构造初始器 sigma:1.0 iterations:200 mpInitializer = new Initializer(mCurrentFrame,1.0,200); 填充匹配信息为未匹配，值为-11fill(mvIniMatches.begin(),mvIniMatches.end(),-1); 此时函数返回，mState仍然为NOT_INITIALIZED 1.5 mstate = NOT_INITIALIZED,mpInitializer已经初始化，开始处理第二帧图像接下来处理第二帧图像，同样进入MonocularInitialization();此时単目初始器已经初始化，走MonocularInitialization()函数else过程。同样需要保证第二帧图像的特征点数大于1001234567891011// Try to initialize // 步骤2：如果当前帧特征点数大于100，则得到用于单目初始化的第二帧 // 如果当前帧特征点太少，重新构造初始器 // 因此只有连续两帧的特征点个数都大于100时，才能继续进行初始化过程 if((int)mCurrentFrame.mvKeys.size()&lt;=100) &#123; delete mpInitializer; mpInitializer = static_cast&lt;Initializer*&gt;(NULL); fill(mvIniMatches.begin(),mvIniMatches.end(),-1); return; &#125; 1.6 建立匹配器，在mInitialFrame帧和mCurrentFrame帧之间进行特征点匹配，返回匹配的特征点个数123456789// Find correspondences // 步骤3：在mInitialFrame与mCurrentFrame中找匹配的特征点对 // mvbPrevMatched为前一帧的特征点，存储了mInitialFrame中哪些点将进行接下来的匹配 // mvIniMatches存储mInitialFrame,mCurrentFrame之间匹配的特征点 // mvbPrevMatched为前一帧的特征点，存储了mInitialFrame中哪些点将进行接下来的匹配 // mvIniMatches存储mInitialFrame,mCurrentFrame之间匹配的特征点,键值对是两帧匹配特征点的索引 // mvbPrevMatched，mvIniMatches获得更新 ORBmatcher matcher(0.9,true); int nmatches = matcher.SearchForInitialization(mInitialFrame,mCurrentFrame,mvbPrevMatched,mvIniMatches,100); 检测匹配点的个数是否满足要求，如果初始两帧图像之间的匹配点太少，则重新初始化12345678// Check if there are enough correspondences // 步骤4：如果初始化的两帧之间的匹配点太少，重新初始化 if(nmatches&lt;100) &#123; delete mpInitializer; mpInitializer = static_cast&lt;Initializer*&gt;(NULL); return; &#125; 1.7 建立匹配器，在mInitialFrame帧和mCurrentFrame帧之间进行特征点匹配，返回匹配的特征点个数mCurrentFrame, mvIniMatches是传入的初始化参数如果初始化成功，intializer得到Rcw, tcw，mvIniP3D, vbTriangulated。12// 步骤5：通过H模型或F模型进行单目初始化，得到两帧间相对运动、初始MapPointsif(mpInitializer-&gt;Initialize(mCurrentFrame, mvIniMatches, Rcw, tcw, mvIniP3D, vbTriangulated)) 1.8 删除那些无法三角化的匹配点123456789// 步骤6：删除那些无法进行三角化的匹配点 for(size_t i=0, iend=mvIniMatches.size(); i&lt;iend;i++) &#123; if(mvIniMatches[i]&gt;=0 &amp;&amp; !vbTriangulated[i]) &#123; mvIniMatches[i]=-1; nmatches--; &#125; &#125; 1.9 设置第一帧为世界坐标系，并建立第二帧的位姿12345678// Set Frame Poses // 将初始化的第一帧作为世界坐标系，因此第一帧变换矩阵为单位矩阵 mInitialFrame.SetPose(cv::Mat::eye(4,4,CV_32F)); // 由Rcw和tcw构造Tcw,并赋值给mTcw，mTcw为世界坐标系到该帧的变换矩阵 cv::Mat Tcw = cv::Mat::eye(4,4,CV_32F); Rcw.copyTo(Tcw.rowRange(0,3).colRange(0,3)); tcw.copyTo(Tcw.rowRange(0,3).col(3)); mCurrentFrame.SetPose(Tcw); 说明:Frame.SetPose函数，将相对于参考帧的Tcw复制到当前帧的Tcw,并更新世界坐标系矩阵中的一些变量12345void Frame::SetPose(cv::Mat Tcw)&#123; mTcw = Tcw.clone(); UpdatePoseMatrices();&#125; 1234567891011void Frame::UpdatePoseMatrices()&#123; // [x_camera 1] = [R|t]*[x_world 1]，坐标为齐次形式 // x_camera = R*x_world + t mRcw = mTcw.rowRange(0,3).colRange(0,3); mRwc = mRcw.t(); mtcw = mTcw.rowRange(0,3).col(3); // mtcw, 即相机坐标系下相机坐标系到世界坐标系间的向量, 向量方向由相机坐标系指向世界坐标系 // mOw, 即世界坐标系下世界坐标系到相机坐标系间的向量, 向量方向由世界坐标系指向相机坐标系 mOw = -mRcw.t()*mtcw;&#125; 2. 建立初始局部地图 CreateInitialMapMonocular()12345// 步骤6：将三角化得到的3D点包装成MapPoints // Initialize函数会得到mvIniP3D， // mvIniP3D是cv::Point3f类型的一个容器，是个存放3D点的临时变量， // CreateInitialMapMonocular将3D点包装成MapPoint类型存入KeyFrame和Map中 CreateInitialMapMonocular(); 2.1 将第一帧图像和第二帧图像都设为关键帧12KeyFrame* pKFini = new KeyFrame(mInitialFrame,mpMap,mpKeyFrameDB);KeyFrame* pKFcur = new KeyFrame(mCurrentFrame,mpMap,mpKeyFrameDB); 2.2 将第一帧和第二帧图像的描述子都转换为Bow1234// 步骤1：将初始关键帧的描述子转为BoWpKFini-&gt;ComputeBoW();// 步骤2：将当前关键帧的描述子转为BoWpKFcur-&gt;ComputeBoW(); 2.3 将关键帧插入全局地图12345// Insert KFs in the map// 步骤3：将关键帧插入到地图// 凡是关键帧，都要插入地图mpMap-&gt;AddKeyFrame(pKFini);mpMap-&gt;AddKeyFrame(pKFcur); 2.4 生成地图点并和关键帧相关联123456789101112131415161718192021222324252627282930313233343536373839// Create MapPoints and asscoiate to keyframes// 步骤4：将3D点包装成MapPointsfor(size_t i=0; i&lt;mvIniMatches.size();i++)&#123; if(mvIniMatches[i]&lt;0) continue; //Create MapPoint. cv::Mat worldPos(mvIniP3D[i]); // 步骤4.1：用3D点构造MapPoint MapPoint* pMP = new MapPoint(worldPos,pKFcur,mpMap); // 步骤4.2：为该MapPoint添加属性： // a.观测到该MapPoint的关键帧 // b.该MapPoint的描述子 // c.该MapPoint的平均观测方向和深度范围 // 步骤4.3：表示该KeyFrame的哪个特征点可以观测到哪个3D点 pKFini-&gt;AddMapPoint(pMP,i); pKFcur-&gt;AddMapPoint(pMP,mvIniMatches[i]); // a.表示该MapPoint可以被哪个KeyFrame的哪个特征点观测到 pMP-&gt;AddObservation(pKFini,i); pMP-&gt;AddObservation(pKFcur,mvIniMatches[i]); // b.从众多观测到该MapPoint的特征点中挑选区分度最高的描述子 pMP-&gt;ComputeDistinctiveDescriptors(); // c.更新该MapPoint平均观测方向以及观测距离的范围 pMP-&gt;UpdateNormalAndDepth(); //Fill Current Frame structure mCurrentFrame.mvpMapPoints[mvIniMatches[i]] = pMP; mCurrentFrame.mvbOutlier[mvIniMatches[i]] = false; //Add to Map // 步骤4.4：在地图中添加该MapPoint mpMap-&gt;AddMapPoint(pMP);&#125; Create MapPoint()说明:12345//Create MapPoint. cv::Mat worldPos(mvIniP3D[i]); // 步骤4.1：用3D点构造MapPoint MapPoint* pMP = new MapPoint(worldPos,pKFcur,mpMap); 创建地图点的过程Pos存储地图点的世界坐标系坐标，mNormalVector存储世界坐标系下相机到3D点的单位向量，dist表示相机中心到世界坐标系原点的距离// mtcw, 即相机坐标系下相机坐标系到世界坐标系间的向量, 向量方向由相机坐标系指向世界坐标系，可以理解为在相机坐标系下，世界坐标系原点的坐标// mOw, 即世界坐标系下世界坐标系到相机坐标系间的向量, 向量方向由世界坐标系指向相机坐标系，可以理解为在世界坐标系下，相机中心的坐标 12345678910// ____// Nearer /____\ level:n-1 --&gt; dmin// /______\ d/dmin = 1.2^(n-1-m)// /________\ level:m --&gt; d// /__________\ dmax/d = 1.2^m// Farther /____________\ level:0 --&gt; dmax//// log(dmax/d)// m = ceil(------------)// log(1.2) 123456789101112131415161718192021Pos.copyTo(mWorldPos);cv::Mat Ow = pFrame-&gt;GetCameraCenter();mNormalVector = mWorldPos - Ow;// 世界坐标系下相机到3D点的向量mNormalVector = mNormalVector/cv::norm(mNormalVector);// 世界坐标系下相机到3D点的单位向量cv::Mat PC = Pos - Ow;const float dist = cv::norm(PC);const int level = pFrame-&gt;mvKeysUn[idxF].octave;const float levelScaleFactor = pFrame-&gt;mvScaleFactors[level];const int nLevels = pFrame-&gt;mnScaleLevels;// 另见PredictScale函数前的注释mfMaxDistance = dist*levelScaleFactor;mfMinDistance = mfMaxDistance/pFrame-&gt;mvScaleFactors[nLevels-1];// 见mDescriptor在MapPoint.h中的注释pFrame-&gt;mDescriptors.row(idxF).copyTo(mDescriptor);// MapPoints can be created from Tracking and Local Mapping. This mutex avoid conflicts with id.unique_lock&lt;mutex&gt; lock(mpMap-&gt;mMutexPointCreation);mnId=nNextId++; 2.5 更新关键帧间的连接关系,在关键帧之间建立边，每个边有一个权重，边的权重是该关键帧与当前帧公共3D点的个数，同时更新每一个3D点能够被哪些帧观测到12pKFini-&gt;UpdateConnections();pKFcur-&gt;UpdateConnections(); 2.6 BA优化// 步骤5：BA优化 Optimizer::GlobalBundleAdjustemnt(mpMap,20); 2.7 归一化3D点坐标和摄像机矩阵123456789101112131415161718192021222324252627282930// Set median depth to 1 // 步骤6：!!!将MapPoints的中值深度归一化到1，并归一化两帧之间变换 // 评估关键帧场景深度，q=2表示中值 float medianDepth = pKFini-&gt;ComputeSceneMedianDepth(2); float invMedianDepth = 1.0f/medianDepth; if(medianDepth&lt;0 || pKFcur-&gt;TrackedMapPoints(1)&lt;100) &#123; cout &lt;&lt; "Wrong initialization, reseting..." &lt;&lt; endl; Reset(); return; &#125; // Scale initial baseline cv::Mat Tc2w = pKFcur-&gt;GetPose(); // x/z y/z 将z归一化到1 Tc2w.col(3).rowRange(0,3) = Tc2w.col(3).rowRange(0,3)*invMedianDepth; pKFcur-&gt;SetPose(Tc2w); // Scale points // 把3D点的尺度也归一化到1 vector&lt;MapPoint*&gt; vpAllMapPoints = pKFini-&gt;GetMapPointMatches(); for(size_t iMP=0; iMP&lt;vpAllMapPoints.size(); iMP++) &#123; if(vpAllMapPoints[iMP]) &#123; MapPoint* pMP = vpAllMapPoints[iMP]; pMP-&gt;SetWorldPos(pMP-&gt;GetWorldPos()*invMedianDepth); &#125; &#125; 2.8 建立此时mpTracker的成员变量mpLocalMapper,mpLastKeyFrame，mnLastKeyFrameId，mvpLocalKeyFrames，mvpLocalMapPoints，mpReferenceKF，mLastFrame，mpMap的相关信息,以及当前帧的信息将初始关键帧和当前关键帧键入局部地图关键帧集合，重新设置当前帧的位姿为归一化后的位姿，设置局部地图为此时的初始地图，设置参考关键帧为当前关键帧，设置当前帧的参考关键帧为当前关键帧，设置最后一帧为当前帧，全局地图的参考地图为此时的局部地图，全局地图的初始关键帧为初始关键帧1234567891011121314151617181920mpLocalMapper-&gt;InsertKeyFrame(pKFini);mpLocalMapper-&gt;InsertKeyFrame(pKFcur);mCurrentFrame.SetPose(pKFcur-&gt;GetPose());mnLastKeyFrameId=mCurrentFrame.mnId;mpLastKeyFrame = pKFcur;mvpLocalKeyFrames.push_back(pKFcur);mvpLocalKeyFrames.push_back(pKFini);mvpLocalMapPoints=mpMap-&gt;GetAllMapPoints();mpReferenceKF = pKFcur;mCurrentFrame.mpReferenceKF = pKFcur;mLastFrame = Frame(mCurrentFrame);mpMap-&gt;SetReferenceMapPoints(mvpLocalMapPoints);mpMapDrawer-&gt;SetCurrentCameraPose(pKFcur-&gt;GetPose());mpMap-&gt;mvpKeyFrameOrigins.push_back(pKFini); 2.9 初始化完成，mState=OK1mState=OK;// 初始化成功，至此，初始化过程完成 3. 初始化结束后，对每一帧图像进行track()此时mState==OK，track()函数走else分支 op1=>operation: SLAM.TrackMonocular() op2=>operation: GrabImageMonocular() op3=>operation: Track() op1(right)->op2(right)->op3{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析五]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%94%2F</url>
    <content type="text"><![CDATA[ORBmatcher类说明单目SLAM初始化相关，双目和RGBD不会使用这个类 1. ORBmatcher 成员变量说明1.1 ORBmatcher 成员变量说明123static const int TH_LOW;static const int TH_HIGH;static const int HISTO_LENGTH; 1.2 ORBmatcher 匹配分数设置1float mfNNratio; 1.3 ORBmatcher 是否检查方向1bool mbCheckOrientation;]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析四]]></title>
    <url>%2F2019%2F06%2F11%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[初始器 Initializer类说明1. Initializer 成员变量说明1typedef pair&lt;int,int&gt; Match; 定义Match类型 1.1 特征点匹配数据类型声明 Match1typedef pair&lt;int,int&gt; Match; 定义Match类型 1.2 mvKeys1，mvKeys2参考帧和当前帧中的特征点12345// Keypoints from Reference Frame (Frame 1)vector&lt;cv::KeyPoint&gt; mvKeys1; ///&lt; 存储Reference Frame中的特征点// Keypoints from Current Frame (Frame 2)vector&lt;cv::KeyPoint&gt; mvKeys2; ///&lt; 存储Current Frame中的特征点 1.3 特征匹配结构说明1234// Current Matches from Reference to Current // Reference Frame: 1, Current Frame: 2 vector&lt;Match&gt; mvMatches12; ///&lt; Match的数据结构是pair,mvMatches12只记录Reference到Current匹配上的特征点对 vector&lt;bool&gt; mvbMatched1; ///&lt; 记录Reference Frame的每个特征点在Current Frame是否有匹配的特征点 1.4 相机内参1cv::Mat mK; ///&lt; 相机内参 1.5 测量误差12// Standard Deviation and Variance float mSigma, mSigma2; ///&lt; 测量误差 1.6 RANSAC迭代次数12// Ransac max iterations int mMaxIterations; ///&lt; 算Fundamental和Homography矩阵时RANSAC迭代次数 1.7 特征匹配结构说明12// Ransac sets vector&lt;vector&lt;size_t&gt; &gt; mvSets; ///&lt; 二维容器，外层容器的大小为迭代次数，内层容器大小为每次迭代算H或F矩阵需要的点 2. Initializer 成员函数说明2.1 Initializer构造函数说明123456789101112131415161718 // Fix the reference frame // 用reference frame来初始化，这个reference frame就是SLAM正式开始的第一帧 Initializer(const Frame &amp;ReferenceFrame, float sigma = 1.0, int iterations = 200);/** * @brief 给定参考帧构造Initializer * * 用reference frame来初始化，这个reference frame就是SLAM正式开始的第一帧 * @param ReferenceFrame 参考帧 * @param sigma 测量误差 * @param iterations RANSAC迭代次数 */ mK = ReferenceFrame.mK.clone(); mvKeys1 = ReferenceFrame.mvKeysUn; mSigma = sigma; mSigma2 = sigma*sigma; mMaxIterations = iterations; 2.1 Initialize()函数说明12345// Computes in parallel a fundamental matrix and a homography // Selects a model and tries to recover the motion and the structure from motion // 用current frame,也就是用SLAM逻辑上的第二帧来初始化整个SLAM，得到最开始两帧之间的R t,以及点云 bool Initialize(const Frame &amp;CurrentFrame, const vector&lt;int&gt; &amp;vMatches12, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated); 针对第一二帧图像进行初始化，初始化时需要确定第二帧相对于第一帧的旋转矩阵和平移向量，以及生成的三维地图点和是否能三角测量化，并行地计算基础矩阵和单应性矩阵，选取其中一个模型，恢复出最开始两帧之间的相对姿态以及点云。 初始化匹配特征点点1234567891011// Fill structures with current keypoints and matches with reference frame// Reference Frame: 1, Current Frame: 2// Frame2 特征点mvKeys2 = CurrentFrame.mvKeysUn;// mvMatches12记录匹配上的特征点对mvMatches12.clear();mvMatches12.reserve(mvKeys2.size());// mvbMatched1记录每个特征点是否有匹配的特征点，// 这个变量后面没有用到，后面只关心匹配上的特征点mvbMatched1.resize(mvKeys1.size()); 2.2 FindHomography()函数说明12// 假设场景为平面情况下通过前两帧求取Homography矩阵(current frame 2 到 reference frame 1),并得到该模型的评分 void FindHomography(vector&lt;bool&gt; &amp;vbMatchesInliers, float &amp;score, cv::Mat &amp;H21); 平面情况下评估Homograph矩阵并计算得分 2.3 FindFundamental()函数说明12// 假设场景为非平面情况下通过前两帧求取Fundamental矩阵(current frame 2 到 reference frame 1),并得到该模型的评分 void FindFundamental(vector&lt;bool&gt; &amp;vbInliers, float &amp;score, cv::Mat &amp;F21); 非平面情况下评估Fundamental矩并计算得分 2.4 ComputeH21()函数说明12// 被FindHomography函数调用具体来算Homography矩阵 cv::Mat ComputeH21(const vector&lt;cv::Point2f&gt; &amp;vP1, const vector&lt;cv::Point2f&gt; &amp;vP2); 2.5 FindHomography()函数说明12// 被FindFundamental函数调用具体来算Fundamental矩阵 cv::Mat ComputeF21(const vector&lt;cv::Point2f&gt; &amp;vP1, const vector&lt;cv::Point2f&gt; &amp;vP2); 2.6 CheckHomography()函数说明12// 被FindHomography函数调用，具体来算假设使用Homography模型的得分 float CheckHomography(const cv::Mat &amp;H21, const cv::Mat &amp;H12, vector&lt;bool&gt; &amp;vbMatchesInliers, float sigma); 2.7 FindHomography()函数说明12// 被FindFundamental函数调用，具体来算假设使用Fundamental模型的得分 float CheckFundamental(const cv::Mat &amp;F21, vector&lt;bool&gt; &amp;vbMatchesInliers, float sigma); 2.8 ReconstructF()函数说明123// 分解F矩阵，并从分解后的多个解中找出合适的R，t bool ReconstructF(vector&lt;bool&gt; &amp;vbMatchesInliers, cv::Mat &amp;F21, cv::Mat &amp;K, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated, float minParallax, int minTriangulated); 2.9 ReconstructH()函数说明123// 分解H矩阵，并从分解后的多个解中找出合适的R，t bool ReconstructH(vector&lt;bool&gt; &amp;vbMatchesInliers, cv::Mat &amp;H21, cv::Mat &amp;K, cv::Mat &amp;R21, cv::Mat &amp;t21, vector&lt;cv::Point3f&gt; &amp;vP3D, vector&lt;bool&gt; &amp;vbTriangulated, float minParallax, int minTriangulated); 2.10 Triangulate()函数说明12// 通过三角化方法，利用反投影矩阵将特征点恢复为3D点 void Triangulate(const cv::KeyPoint &amp;kp1, const cv::KeyPoint &amp;kp2, const cv::Mat &amp;P1, const cv::Mat &amp;P2, cv::Mat &amp;x3D); 2.11 Normalize()函数说明12// 归一化三维空间点和帧间位移t void Normalize(const vector&lt;cv::KeyPoint&gt; &amp;vKeys, vector&lt;cv::Point2f&gt; &amp;vNormalizedPoints, cv::Mat &amp;T); 2.12 CheckRT()函数说明1234// ReconstructF调用该函数进行cheirality check，从而进一步找出F分解后最合适的解 int CheckRT(const cv::Mat &amp;R, const cv::Mat &amp;t, const vector&lt;cv::KeyPoint&gt; &amp;vKeys1, const vector&lt;cv::KeyPoint&gt; &amp;vKeys2, const vector&lt;Match&gt; &amp;vMatches12, vector&lt;bool&gt; &amp;vbInliers, const cv::Mat &amp;K, vector&lt;cv::Point3f&gt; &amp;vP3D, float th2, vector&lt;bool&gt; &amp;vbGood, float &amp;parallax); 2.13 DecomposeE()函数说明12// F矩阵通过结合内参可以得到Essential矩阵，该函数用于分解E矩阵，将得到4组解 void DecomposeE(const cv::Mat &amp;E, cv::Mat &amp;R1, cv::Mat &amp;R2, cv::Mat &amp;t);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Schmidt 正交化]]></title>
    <url>%2F2019%2F05%2F31%2FSchmidt-%E6%AD%A3%E4%BA%A4%E5%8C%96%2F</url>
    <content type="text"><![CDATA[施密特正交化(Schmidt orthogonalization)是求欧氏空间正交基的一种方法。从欧氏空间任意线性无关的向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$出发，求得正交向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_m}$，$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$与向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_m}$等价，再将正交向量组中每个向量经过单位化，就得到一个标准正交向量组，这种方法称为施密特正交化。 线性无关向量组未必是正交向量组，但正交向量组又是重要的，因此现在就有一个问题：能否从一个线性无关向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$出发，构造出一个标准正交向量组$\boldsymbol{e_1},\boldsymbol{e_2},\cdots,\boldsymbol{e_m}$，并且使向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$与向量组$\boldsymbol{e_1},\boldsymbol{e_2},\cdots,\boldsymbol{e_m}$等价呢?回答是肯定的，通过施密特正交化方法就可以实现。下面就来介绍这个方法，由于把一个正交向量组中每个向量经过单位化，就得到一个标准正交向量组，所以，上述问题的关键是如何由一个线性无关向量组来构造出一个正交向量组，我们以3个向量组成的线性无关组为例来说明这个方法。设向量组$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\boldsymbol{\alpha_3}$线性无关，我们先来构造正交向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\boldsymbol{\beta_3}$，并且使$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_r}$与向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_r}$等价$(r=1,2,3)$。按所要求的条件，$\boldsymbol{\beta_1}$是$\boldsymbol{\alpha_1}$的线性组合，$\boldsymbol{\beta_2}$是$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2}$的线性组合，为方便起见，不妨设 \boldsymbol{\beta_1}=\boldsymbol{\alpha_1},\boldsymbol{\beta_2}=\boldsymbol{\alpha_2}-k\boldsymbol{\beta_1}其中，数值$k$的选取应满足$\boldsymbol{\beta_1}$与$\boldsymbol{\beta_2}$垂直，即 \langle\boldsymbol{\beta_2},\boldsymbol{\beta_1}\rangle=\langle\boldsymbol{\alpha_2},\boldsymbol{\beta_1}\rangle - k\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle=0注意到$\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle &gt; 0$，于是得$k={\langle\boldsymbol{\alpha_2},\boldsymbol{\beta_1}\rangle} / {\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}$，从而得 \boldsymbol{\beta_1} =\boldsymbol{\alpha_1},\boldsymbol{\beta_2}=\boldsymbol{\alpha_2}-\frac{\langle\boldsymbol{\alpha_2},\boldsymbol{\beta_1}\rangle}{\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}\boldsymbol{\beta_1}对于上面已经构造的向量$\boldsymbol{\beta_1}$和$\boldsymbol{\beta_2}$，再来构造$\boldsymbol{\beta_3}$，为满足要求，可以令 \boldsymbol{\beta_3} = \boldsymbol{\alpha_3}-k_1\boldsymbol{\beta_1}-k_2\boldsymbol{\beta_2}其中，$k_1,k_2$的选取应满足$\boldsymbol{\beta_3}$分别与向量$\boldsymbol{\beta_1}$和$\boldsymbol{\beta_2}$垂直，即 \langle\boldsymbol{\beta_3},\boldsymbol{\beta_1}\rangle=\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_1}\rangle - k_1\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle=0，\langle\boldsymbol{\beta_3},\boldsymbol{\beta_2}\rangle=\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_2}\rangle - k_2\langle\boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle=0由此解得 k_1=\frac{\langle \boldsymbol{\alpha_3},\boldsymbol{\beta_1}\rangle}{\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}, k_2=\frac{\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_2}\rangle}{\langle\boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle}于是得 \boldsymbol{\beta_3} = \boldsymbol{\alpha_3}-\frac{\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_1}\rangle}{\langle\boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}\boldsymbol{\beta_1}-\frac{\langle\boldsymbol{\alpha_3},\boldsymbol{\beta_2}\rangle}{\langle\boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle}\boldsymbol{\beta_2}容易验证，向量组$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_r}$是与$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_r}$等价的正交向量，若再将$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_r}$单位化，即令 \boldsymbol{e_i} = \frac{\boldsymbol{\beta_i}}{\Vert \boldsymbol{\beta_i} \Vert}(i=1,2,3)则$\boldsymbol{e}_1,\boldsymbol{e_2},\boldsymbol{e_3}$就是满足要求的标准正交向量。 数学归纳法一般性定理设$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}(m\leq n)$是$\mathbb{R}^n$中的一个线性无关向量组，若令 \begin{array}{l} \boldsymbol{\beta_1} = \boldsymbol{\alpha_1} \\[2ex] \boldsymbol{\beta_2} = \boldsymbol{\alpha_2}-\frac{\langle \boldsymbol{\alpha_2} , \boldsymbol{\beta_1} \rangle } {\langle \boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle}\boldsymbol{\beta_1} \\[2ex] \boldsymbol{\beta_m} = \boldsymbol{\alpha_m} - \frac{\langle \boldsymbol{\alpha_m} , \boldsymbol{\beta_1} \rangle } {\langle \boldsymbol{\beta_1},\boldsymbol{\beta_1}\rangle }\boldsymbol{\beta_1}-\frac{\langle \boldsymbol{\alpha_m} , \boldsymbol{\beta_2} \rangle } {\langle \boldsymbol{\beta_2},\boldsymbol{\beta_2}\rangle }\boldsymbol{\beta_2}-\cdots- \frac{\langle \boldsymbol{\alpha_m} , \boldsymbol{\beta_{m-1}} \rangle } {\langle \boldsymbol{\beta_{m-1}},\boldsymbol{\beta_{m-1}}\rangle }\beta_{m-1} \end{array} 则$\boldsymbol{\beta_1},\boldsymbol{\beta_2},\cdots,\boldsymbol{\beta_m}$就是一个正交向量组，若再令 \boldsymbol{e_i}=\frac {\boldsymbol{\beta_i}}{\Vert \boldsymbol{\beta_i} \Vert}(i=1,2,\cdots,m)就得到了一个标准正交向量组$\boldsymbol{e_1},\boldsymbol{e_2},\cdots,\boldsymbol{e_m}$， 且该向量和$\boldsymbol{\alpha_1},\boldsymbol{\alpha_2},\cdots,\boldsymbol{\alpha_m}$上述所说明的利用线性无关向量组，构造出一个标准正交向量组的方法，就是施密特正交化方法。]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>矩阵分解</tag>
        <tag>Math</tag>
        <tag>正交化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵分解]]></title>
    <url>%2F2019%2F05%2F31%2F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. QR分解定理 1 对任意非奇异实矩阵$A$总可以分解为正交矩阵$Q$与上三角矩阵$R$的积,如果要求上三角阵$R$的对角元素均为正数,则分解是唯一的。]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>矩阵分解</tag>
        <tag>Math</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小二乘问题]]></title>
    <url>%2F2019%2F05%2F30%2F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1. 非齐次线性最小二乘问题考虑线性系统： \begin{array}{c} Ax = b & \text(A \in R^{ m \times n }(m > n),b \in R^m,x \in R^n) \end{array}的最小二乘解。即求 $ x \in R^n $使得 \begin{Vmatrix} Ax-b \end{Vmatrix} _2 = \min \{ \begin{Vmatrix} Av-b \end{Vmatrix} _2:v \in R^n\}记 X_{LS}=\{ x\in R^n:x是 (1) 的解\}则称$X_{LS}$是最小二乘问题的解集；$X_{LS}$中范数最小者称为最小范数解，并记作$x_{LS}$，即 \begin{Vmatrix} x_{LS} \end{Vmatrix} _2 = \min \{ \begin{Vmatrix} x \end{Vmatrix} _2 : x \in X_{LS}\}命题 1 $x\in X_{LS} \iff A^T(Ax-b)=0$证明 $\forall x,y \in R^n,$有 \begin{Vmatrix} b-A(x+y) \end{Vmatrix} _2^2 = \begin{Vmatrix} b-Ax \end{Vmatrix} _2^2 - 2y^TA^T(b-Ax)+ \begin{Vmatrix} Ay \end{Vmatrix} _2^2因此， x \in X_{LS} \iff \forall y \in R^n,\begin{Vmatrix} Ay \end{Vmatrix} _2^2 - 2y^TA^T(b-Ax)\geq 0 \iff A^T(b-Ax)=0方程$A^T(b-Ax)=0$称为$Ax-b=0$的正规方程。推论 1 $X_{LS}$是凸集； $x_{LS}$是唯一的； $X_{LS} = \{ x_{LS}的充分必要条件是rank(A)=n \}$。 Moore-Pseudo 广义逆为了给出最小二乘的一般表示，需要矩阵的广义逆的概念。 定义 1 $Ａ\in R_{m \times n} $，若$X\in R_{n\times m}$满足 \begin{array}{c} AXA = A ,& XAX=X ,& (AX)^T=AX ,& (XA)^T=XA \end{array}则称$X$是$A$的广义逆，并记作$A^+$。矩阵$A$的广义逆是唯一的，并且可以利用$A$的 SVD 分解进行计算。令$A$的 SVD 分解为 A=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T不难验证： A^+=V\begin{pmatrix}\Sigma_r^{-1} & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}U^T命题 2 最小二乘问题的一般解为 x=A^+b+(I-A^+A)z,z\in R^n最小范数解是 x_{LS}=A^+b证明 由命题1，最小二乘问题(1)的解可以由它的正规方程: A^TAx=A^Tb给出。利用广义逆，可以验证x=A^+b是上式的一个解。另外，由 SVD 分解可证明$N(A^TA)=N(A)$，且 N(A) = \{(I-A^+A)z:z\in R^n\}此外，注意到 [(I-A^+A)z]^TA^+b=z^T(I-A^+A)A^+b=0所以， \begin{Vmatrix} x \end{Vmatrix} _2^2 = \begin{Vmatrix} A^+b \end{Vmatrix} _2^2 + \begin{Vmatrix} (I-A^+A)z \end{Vmatrix} _2^2 \geq \begin{Vmatrix} A^+b \end{Vmatrix} _2^2即：$x_{LS}=A^+b$ 满秩最小二乘问题如果(1)中的矩阵$A$是列满秩的，即$rank(A)=n$，则称它为满秩最小二乘问题。可以通过正规分解法，QR 分解法来求解该最小二乘问题，但最好的方法还是 SVD 分解法。 SVD分解方法由于$rank(A)=n$，所以$A$必有下述形式的 SVD 分解$A=U\begin{pmatrix}\Sigma_n \ 0\end{pmatrix}V^T$，于是，$A^+=V\begin{pmatrix}\Sigma_n^{-1},0 \end{pmatrix}U^T$。所以，问题的解为 x=A^+b=V\begin{pmatrix}\Sigma_n^{-1},0 \end{pmatrix}U^Tb=\frac{u_1^Tb}{\sigma_1}v_1+\frac{u_2^Tb}{\sigma_2}v_2+\cdots+\frac{u_n^Tb}{\sigma_n}v_n = \sum_{j=1}^{n}\frac{u_j^Tb}{\sigma_j}v_j亏秩最小二乘问题此时不可以利用其他方法来求解最小二乘问题，SVD 方法是首选。具体来说，若$rank(A)=r$，则$A$有 SVD 分解: A=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T因此， x=A^+b=V\begin{pmatrix}\Sigma_r^{-1} & \boldsymbol 0 \\ \boldsymbol 0 & 0\end{pmatrix}U^Tb = \sum_{j=1}^{r}\frac{u_j^Tb}{\sigma_j}v_j数值秩的定义和确定方法略 以后再补 2. 齐次最小二乘问题考虑齐次线性方程： \begin{array}{c} Ax = 0 & \text(A \in R^{ m \times n }(m > n),b \in R^m,x \in R^n) \end{array}对应的最小二乘问题是 \begin{Vmatrix} Ax \end{Vmatrix} _2 = \min \{ \begin{Vmatrix} Av \end{Vmatrix} _2:v \in R^n\}显然，$x=0$总是上述最小二乘问题的最小范数解。在实际中，人们通常关心的是它的非零解，而不是零解。因此，总是考虑相应的约束最小二乘问题: \begin{Vmatrix} Ax \end{Vmatrix}_2 = \min \{ \begin{Vmatrix} Av \end{Vmatrix}_2:v\in R^n,\begin{Vmatrix} v \end{Vmatrix}_2 = 1\}或者等价的写成 \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix}_2\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1 \end{array} \right.命题 3 若$rank(A)=r$，则该齐次线性方程的解为 x=\sum_{j=1}^{n-r}s_jv_{r+j},(\sum_{j=1}^{n-r}s_j^2=1)其中，$v_{r+1},v_{r+2},\cdots,v_n$是$A^TA$的零特征值的$n-r$个线性无关的单位特征向量。 证明 问题等价于: \left\{ \begin{array}{l} \min x^TA^TAx\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1 \end{array} \right.因$rank(A^TA)=rank(A)=r$，所以$A^TA$有特征值分解 A^TA=Vdiag(\lambda_1,\lambda_2,\cdots,\lambda_r,0,\cdots,0)V^T其中，$V$是正交矩阵，所以$v_{r+1},v_{r+2},\cdots,v_n$是$A^TAx=0$的$n-r$个相互正交的单位解，因此，上述问题的解为 x=\sum_{j=1}^{n-r}s_jv_{r+j},(\sum_{j=1}^{n-r}s_j^2=1)命题 4 若$rank(A)=r$，则该齐次线性方程的解为 x=\sum_{j=1}^{n-r}s_jv_{r+j},(\sum_{j=1}^{n-r}s_j^2=1)其中，$v_{r+1},v_{r+2},\cdots,v_n$是$A$的$n-r$个零奇异值的右奇异向量。 3. 约束齐次最小二乘问题零约束考虑约束最小二乘问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix} _2^2\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1,Cx=0 \end{array} \right.不妨假设$rank(C)=r &lt; n$，对$C$作 SVD 分解 C=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T令$V_{n-r}=(v_{r+1},v_{r+2},\cdots,v_{n})$为$V$的最后$n-r$个列向量所构成的矩阵，因为$Cx=0$的所有解可以表示为: x=V_{n-r}y,y \in R^{n-r}并且$\begin{Vmatrix} Ax\end{Vmatrix} _2^2=1 \iff \begin{Vmatrix} y\end{Vmatrix} _2^2=1$，这是因为$V_{n-r}$是列正交的。于是，上述问题可以转化为下述最小化问题 \left\{ \begin{array}{l} \min \begin{Vmatrix} AV_{n-r}y \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} y \end{Vmatrix}_2=1 \end{array} \right.这个问题在上一节已经解决。 值约束考虑约束最小二乘问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix} _2^2\\[2ex] subject to\begin{Vmatrix} x \end{Vmatrix}_2=1,x=Cy \end{array} \right.不妨假设$rank(C)=r &lt; n$，对$C$作 SVD 分解 C=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T由 SVD 分解特性可知，矩阵$C$的值空间可以表示为 x = R(C)=\{U_rx'|x'\in R^r\}其中，$U_r$为$U$的前$r$列所构成的矩阵。于是$x=Cy$可以表示为$x=U_rx’$且有$\begin{Vmatrix}x\end{Vmatrix} _2^2=1 \iff \begin{Vmatrix} x’\end{Vmatrix} _2^2=1$，这是因为$U_r$是列正交的。于是，上述问题可以转化为下述最小化问题 \left\{ \begin{array}{l} \min \begin{Vmatrix} AU_rx' \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} x' \end{Vmatrix}_2=1 \end{array} \right.这个问题在上一节已经解决。 模约束考虑约束最小二乘问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} Ax \end{Vmatrix} _2^2\\[2ex] subject to\begin{Vmatrix} Cx \end{Vmatrix}_2=1 \end{array} \right.不妨假设$rank(C)=r &lt; n$，对$C$作 SVD 分解 C=U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}V^T令$x’=V^Tx$，则有 Cx = U\begin{pmatrix}\Sigma_r & \boldsymbol 0 \\ \boldsymbol 0 & 0 \end{pmatrix}x'且$\begin{Vmatrix} Ax \end{Vmatrix} _2=1 \iff \begin{Vmatrix}\Sigma_r &amp; \boldsymbol 0 \ \boldsymbol 0 &amp; 0\end{Vmatrix}x’=1 \iff \begin{Vmatrix} \Sigma_rx’\end{Vmatrix} _2=1 $，其中， x'= \begin{pmatrix} x'_r \\ x'_{n-r}\end{pmatrix}所以， Ax=AVx'=A'x'= \begin{pmatrix} A'_r & A'_{n-r}\end{pmatrix}\begin{pmatrix} x'_r \\ x'_{n-r}\end{pmatrix}=A'_rx'_r+A'_{n-r}x'_{n-r}取$y=\Sigma_rx’_r$，则$x’_r=\Sigma_r^{-1}y$，于是，上述问题可以转化为下述最小化问题 \left\{ \begin{array}{l} \min \begin{Vmatrix} A'_r\Sigma_r^{-1}y+A'_{n-r}x'_{n-r} \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} y \end{Vmatrix}_2=1 \end{array} \right.由于$\min_{x’_{n-r}} \begin{Vmatrix} A’_r\Sigma_r^{-1}y+A’_{n-r}x’_{n-r} \end{Vmatrix}_2^2$的最小化问题的解为$x’_{n-r} = -{A’}_{n-r}^+A’_r\Sigma_r^{-1}y$于是该求解问题转化为如下最小化问题: \left\{ \begin{array}{l} \min \begin{Vmatrix} (A'_r-A'_{n-r}{A'}_{n-r}^+A'_r)\Sigma_r^{-1}y \end{Vmatrix}_2^2\\[2ex] subject to\begin{Vmatrix} y \end{Vmatrix}_2=1 \end{array} \right.这个问题在上一节已经解决。注意，$A^+A$不等于$I$，除非A满秩。]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>最小二乘问题</tag>
        <tag>SVD分解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PnP算法]]></title>
    <url>%2F2019%2F05%2F30%2FPnP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[PnP算法求解相机位姿PnP(Perspective-n-Point) 是求解3D到2D点对运动的方法。它描述了当我们知道n个3D空间点以及它们的投影位置时,如何估计相机所在的位姿。 1. 直接线性变换(DLT)考虑某个空间点$P$,它的齐次坐标为$P=(X, Y, Z, 1)^T$。在图像$I_1$中,投影到特征点$x_1=(u_1,v_1,1)^T$(以归一化平面齐次坐标表示)。此时相机的位姿$R,t$是未知的。与单应矩阵的求解类似,我们定义增广矩阵$[R|t]$为一个$3\times4$的矩阵,包含了旋转与平移信息。我们把它的展开形式列写如下: s\begin{pmatrix} u_1 \\ v_1 \\ 1 \\ \end{pmatrix} = \begin{pmatrix} t_1 & t_2 & t_3 & t_4 \\ t_5 & t_6 & t_7 & t_8 \\ t_9 & t_{10} & t_{11} & t_{12} \\ \end{pmatrix} \begin{pmatrix} X \\ Y \\ Z \\ \end{pmatrix}用最后一行把$s$消去,得到两个约束:\begin{align*}u_1 &amp;= \frac{t_1X+t_2Y+t_3Z+t_4}{t_9X+t_{10}Y+t_{11}Z+t_{12}}\\v_1 &amp;= \frac{t_5X+t_6Y+t_7Z+t_8}{t_9X+t_{10}Y+t_{11}Z+t_{12}}\end{align*} 为了简化表示,定义$T$的行向量: \begin{array}{c} t_1 = (t_1, t_2, t_3, t_4)^T,& t_2 = (t_5, t_6, t_7, t_8)^T,& t_3 = (t_9, t_{10}, t_{11}, t_{12})^T \end{array}于是有: \left\{ \begin{array}{c} t_1^TP-t_3^TPu_1=0 \\ t_2^TP-t_3^TPv_1=0 \end{array} \right.请注意$t$是待求的变量,可以看到每个特征点提供了两个关于$t$的线性约束。假设一共有$N$个特征点,可以列出线性方程组: \begin{pmatrix} P_1^T & 0 & -u_1P_1^T \\ 0 & P_1^T & -v_1P_1^T \\ \vdots & \vdots & \vdots \\ P_N^T & 0 & -u_NP_N^T \\ 0 & P_N^T & -v_NP_N^T \end{pmatrix}由于 $t$ 一共有12维，考虑到齐次性，共有11个自由度，而每一对特征点对都提供了两个约束方程。因此最少通过6对匹配点，即可实现矩阵$T$的线性求解，这种方法(也)称为直接线性变换(Direct Linear Transform，DLT)。当匹配点大于6对时，(又)可以使用 SVD 等方法对超定方程求最小二乘解。在 DLT 求解中，我们直接将$T$矩阵看成了12个未知数，忽略了它们之间的联系。因为旋转矩阵$R∈SO(3)$，用 DLT 求出的解不一定满足该约束，它是一个一般矩阵。平移向量比较好办，它属于向量空间。对于旋转矩阵$R$，我们必须针对 DLT 估计的$T$的左边$3\times3$的矩阵块，寻找一个最好的旋转矩阵对它进行近似。这可以由QR分解完成，相当于把结果从矩阵空间重新投影到$SE(3)$流形上，转换成旋转和平移两部分。需要解释的是,我们这里的$x_1$使用了归一化平面坐标，去掉了内参矩阵$K$的影响——这是因为内参$K$在SLAM中通常假设为已知。如果内参未知,那么我们也能用 PnP 去估计$K,R,t$三个量。然而由于未知量的增多，效果会差一些。]]></content>
      <categories>
        <category>SFM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>SFM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析三]]></title>
    <url>%2F2019%2F05%2F29%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89%2F</url>
    <content type="text"><![CDATA[ORBextractor特征提取器1. ORBextractor 成员变量1.1 SCORE 得分常量1enum &#123;HARRIS_SCORE=0, FAST_SCORE=1 &#125;; 定义常量，HARRIS_SCORE=0, FAST_SCORE=1，特征点提取后通过得分进行筛选得到最佳的特征点 1.2 描述子的筛选模式1std::vector&lt;cv::Point&gt; pattern; 用来快速建立256维的描述子 1.3 ORBextractor 特征提取器的控制变量12345int nfeatures; // 需要提取的特征点数目double scaleFactor; // ORB 金字塔相邻两层之间的尺度因子int nlevels; // 金字塔的层数int iniThFAST; // FAST角点提取的初始阈值int minThFAST; // FAST角点用初始阈值提取失败后放松阈值再次提取 1.4 每层金字塔对应的特征点数目Vector mnFeaturesPerLevel1std::vector&lt;int&gt; mnFeaturesPerLevel; 定义常量，HARRIS_SCORE=0, FAST_SCORE=1，特征点提取后通过得分进行筛选得到最佳的特征点 1.5 每层金字塔对应的尺度因子Vector mvScaleFactor1std::vector&lt;float&gt; mvScaleFactor; 1.6 每层金字塔对应的尺度因子的倒数Vector mvInvScaleFactor1std::vector&lt;float&gt; mvInvScaleFactor; 1.7 每层金字塔对应的Sigma的平方Vector mvLevelSigma21std::vector&lt;float&gt; mvLevelSigma2; 1.8 每层金字塔对应的Sigma的平方的倒数Vector mvLevelSigma21std::vector&lt;float&gt; mvInvLevelSigma2; 1.9 每层金字塔对应图像Mat mvImagePyramid1std::vector&lt;cv::Mat&gt; mvImagePyramid; 1.10 半径为31时所对应的截半径vector1std::vector&lt;int&gt; umax; 2. ORBextractor 成员函数2.1 ORBextractor 构造函数12ORBextractor(int nfeatures, float scaleFactor, int nlevels, int iniThFAST, int minThFAST); 第一步：算金字塔每层的尺度，然后根据尺度计算每层应该提取多少特征点，这里面涉及了一个等比数列，唤起高中的记忆，还挺有意思的。最后，保证提取总特征点数≥ nfeatures。1234567891011121314151617mvScaleFactor.resize(nlevels);mvLevelSigma2.resize(nlevels);mvScaleFactor[0]=1.0f;mvLevelSigma2[0]=1.0f;for(int i=1; i&lt;nlevels; i++)&#123; mvScaleFactor[i]=mvScaleFactor[i-1]*scaleFactor; mvLevelSigma2[i]=mvScaleFactor[i]*mvScaleFactor[i];&#125;mvInvScaleFactor.resize(nlevels);mvInvLevelSigma2.resize(nlevels);for(int i=0; i&lt;nlevels; i++)&#123; mvInvScaleFactor[i]=1.0f/mvScaleFactor[i]; mvInvLevelSigma2[i]=1.0f/mvLevelSigma2[i];&#125; nfeatures: 1000nleves: 8scaleFactor: 1.2iniThFAST: 20minThFAST: 8 mvScaleFactor: 1.0, 1.2, 1.2^2, 1.2^3, 1.2^4, 1.2^5, 1.2^6, 1.2^7mvLevelSigma2: mvScaleFactor[i]*mvScaleFactor[i];mvInvScaleFactor[i]=1.0f/mvScaleFactor[i];mvInvLevelSigma2[i]=1.0f/mvLevelSigma2[i]; 12345678910float factor = 1.0f / scaleFactor;float nDesiredFeaturesPerScale = nfeatures*(1 - factor)/(1 - (float)pow((double)factor, (double)nlevels));int sumFeatures = 0;for( int level = 0; level &lt; nlevels-1; level++ )&#123; mnFeaturesPerLevel[level] = cvRound(nDesiredFeaturesPerScale); sumFeatures += mnFeaturesPerLevel[level]; nDesiredFeaturesPerScale *= factor;&#125;mnFeaturesPerLevel[nlevels-1] = std::max(nfeatures - sumFeatures, 0); 通过等比数列求和来计算初始第0层nDesiredFeaturesPerScale的特征点数 {\rm nfeatures} = \frac{ {\rm nDesiredFeaturesPerScale}\times(1-{\rm factor}^{\rm nlevels})}{1-{\rm factor}}{\rm nDesiredFeaturesPerScale} = \frac{ {\rm nfeatures}\times(1-{\rm factor})}{1-{\rm factor}^{\rm nlevels}}并通过四舍五入来选取每一层的特征点数，最后一层的特征点数要保证总特征点数大于阈值1000 构造描述符生成器12345678910111213141516171819202122const int npoints = 512; const Point* pattern0 = (const Point*)bit_pattern_31_; std::copy(pattern0, pattern0 + npoints, std::back_inserter(pattern)); //This is for orientation // pre-compute the end of a row in a circular patch umax.resize(HALF_PATCH_SIZE + 1); int v, v0, vmax = cvFloor(HALF_PATCH_SIZE * sqrt(2.f) / 2 + 1); int vmin = cvCeil(HALF_PATCH_SIZE * sqrt(2.f) / 2); const double hp2 = HALF_PATCH_SIZE*HALF_PATCH_SIZE; for (v = 0; v &lt;= vmax; ++v) umax[v] = cvRound(sqrt(hp2 - v * v)); // Make sure we are symmetric for (v = HALF_PATCH_SIZE, v0 = 0; v &gt;= vmin; --v) &#123; while (umax[v0] == umax[v0 + 1]) ++v0; umax[v] = v0; ++v0; &#125; 2.2 void operator() 操作符提取关键点和描述符123456// Compute the ORB features and descriptors on an image.// ORB are dispersed on the image using an octree.// Mask is ignored in the current implementation.void operator()( cv::InputArray image, cv::InputArray mask, std::vector&lt;cv::KeyPoint&gt;&amp; keypoints, cv::OutputArray descriptors); 最终生成的特征点列表和描述符都放在引用参数里，ORBextractor没有成员变量保存特征点和描述符，只保存了每一层金字塔的图像 构建图像金字塔123456Mat image = _image.getMat(); assert(image.type() == CV_8UC1 ); // Pre-compute the scale pyramid // 构建图像金字塔 ComputePyramid(image); 计算每层图像的特征点1234// 计算每层图像的兴趣点 vector &lt; vector&lt;KeyPoint&gt; &gt; allKeypoints; // vector&lt;vector&lt;KeyPoint&gt;&gt; ComputeKeyPointsOctTree(allKeypoints); //ComputeKeyPointsOld(allKeypoints); 累加每一层的特征点数量，计算总的特征点数，并建立一个空的描述符矩阵，待填充123456789101112131415Mat descriptors; int nkeypoints = 0; for (int level = 0; level &lt; nlevels; ++level) nkeypoints += (int)allKeypoints[level].size(); if( nkeypoints == 0 ) _descriptors.release(); else &#123; _descriptors.create(nkeypoints, 32, CV_8U); descriptors = _descriptors.getMat(); &#125; _keypoints.clear(); _keypoints.reserve(nkeypoints); 分别统计每一层图像的特征点数，并对每一层金字塔进行高斯滤波，对滤波完的图像和关键点，匹配模式进行描述符的提取，对不同层的特征点的坐标重新定位。描述符的提取，半径为PATCH_SIZE=31123456789101112131415161718192021222324252627282930int offset = 0; for (int level = 0; level &lt; nlevels; ++level) &#123; vector&lt;KeyPoint&gt;&amp; keypoints = allKeypoints[level]; int nkeypointsLevel = (int)keypoints.size(); if(nkeypointsLevel==0) continue; // preprocess the resized image 对图像进行高斯模糊 Mat workingMat = mvImagePyramid[level].clone(); GaussianBlur(workingMat, workingMat, Size(7, 7), 2, 2, BORDER_REFLECT_101); // Compute the descriptors 计算描述子 Mat desc = descriptors.rowRange(offset, offset + nkeypointsLevel); computeDescriptors(workingMat, keypoints, desc, pattern); offset += nkeypointsLevel; // Scale keypoint coordinates if (level != 0) &#123; float scale = mvScaleFactor[level]; //getScale(level, firstLevel, scaleFactor); for (vector&lt;KeyPoint&gt;::iterator keypoint = keypoints.begin(), keypointEnd = keypoints.end(); keypoint != keypointEnd; ++keypoint) keypoint-&gt;pt *= scale; &#125; // And add the keypoints to the output _keypoints.insert(_keypoints.end(), keypoints.begin(), keypoints.end()); &#125; 2.3 ComputePyramid(cv::Mat image) 计算图像金字塔对图像进行线性插值缩小，每次图像的宽和高都缩小1.2倍1234567891011121314151617181920212223242526void ORBextractor::ComputePyramid(cv::Mat image)&#123; for (int level = 0; level &lt; nlevels; ++level) &#123; float scale = mvInvScaleFactor[level]; Size sz(cvRound((float)image.cols*scale), cvRound((float)image.rows*scale)); Size wholeSize(sz.width + EDGE_THRESHOLD*2, sz.height + EDGE_THRESHOLD*2); Mat temp(wholeSize, image.type()), masktemp; mvImagePyramid[level] = temp(Rect(EDGE_THRESHOLD, EDGE_THRESHOLD, sz.width, sz.height)); // Compute the resized image if( level != 0 ) &#123; resize(mvImagePyramid[level-1], mvImagePyramid[level], sz, 0, 0, cv::INTER_LINEAR); copyMakeBorder(mvImagePyramid[level], temp, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, BORDER_REFLECT_101+BORDER_ISOLATED); &#125; else &#123; copyMakeBorder(image, temp, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, EDGE_THRESHOLD, BORDER_REFLECT_101); &#125; &#125;&#125; 2.4 ORBextractor::ComputeKeyPointsOctTree 计算特征点四叉树先对每一个网格进行FAST特征点的提取，然后再进行四叉树的建立，根据每一层要求的特征点数量，返回最大响应的特征点，最后计算每一个特征点的主方向，依据灰度质心法。定义常量，HARRIS_SCORE=0, FAST_SCORE=1，特征点提取后通过得分进行筛选得到最佳的特征点 2.5 computeOrientation 计算特征点描述符的方向1keypoint-&gt;angle = IC_Angle(image, keypoint-&gt;pt, umax); 每个特征点的方向采用灰度质心法，即统计半径为15的圆内的灰度质心方向，主要调用IC_Angle函数。 2.6 IC_Angle 计算特征点的主方向12345678910111213141516171819202122232425int m_01 = 0, m_10 = 0; const uchar* center = &amp;image.at&lt;uchar&gt; (cvRound(pt.y), cvRound(pt.x)); // Treat the center line differently, v=0 for (int u = -HALF_PATCH_SIZE; u &lt;= HALF_PATCH_SIZE; ++u) m_10 += u * center[u]; // Go line by line in the circuI853lar patch int step = (int)image.step1(); for (int v = 1; v &lt;= HALF_PATCH_SIZE; ++v) &#123; // Proceed over the two lines int v_sum = 0; int d = u_max[v]; for (int u = -d; u &lt;= d; ++u) &#123; int val_plus = center[u + v*step], val_minus = center[u - v*step]; v_sum += (val_plus - val_minus); m_10 += u * (val_plus + val_minus); &#125; m_01 += v * v_sum; &#125; return fastAtan2((float)m_01, (float)m_10); 先令m_01和m_10都为0，然后计算水平中间线上的均值，注意算均值时，在中心点的左边或者下边距离值是负数。然后依次统计每一条截半径上的水平均值和（累和），以及垂直线上的均值（相减），最后返回tan(m_01,m_10)作为主方向。 3. 初始化Current帧 st=>start: operator()生成特征点和描述符 e=>end: 得到m特征点Vector和描述符Matrix op1=>operation: 参数输入，输入图片矩阵，引用关键点列表，引用描述符矩阵 op2=>operation: 调用ComputePyramid(image)计算图像金字塔，对每一层的图像进行resize(),宽度和高度每次除以1.2然后进行线性插值，保存在成员变量mvImagePyramid里 op3=>operation: 调用ComputeKeyPointsOctTree(allKeypoints)计算特征点四叉树，构建初始网格图，对每一个网格进行特征点的提取 op4=>operation: 调用FAST函数对每一个网格来计算特征点坐标，并进行网格坐标矫正 op5=>operation: 对每一个特征点computeDescriptors(workingMat, keypoints, desc, pattern)计算描述符 st->op1->op2->op3->op4->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: Frame()构造函数生成当前帧 e=>end: 得到mCurrent帧 op1=>operation: 帧ID自加 op2=>operation: 特征提取尺度信息初始化 op3=>operation: 调用ExtractORB(0,imGray)来计算特征点Vector和描述符matrix，0表示単目，1表示双目，提取完的特征点存在mvKeys里面,描述符存在mDescriptors里面，这一步调用了特征提取器的括号运算符 op4=>operation: 对提取的每一个特征点进行矫正去畸变UndistortKeyPoints op5=>operation: ComputeImageBounds(imGray)，计算图片边界，方便进行网格划分 op6=>operation: 将特征点分配到对应的网格AssignFeaturesToGrid() st->op1->op2->op3->op4->op5->op6->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown flowchart.js画流程图]]></title>
    <url>%2F2019%2F05%2F28%2FMarkdown-flowchart.js%E7%94%BB%E6%B5%81%E7%A8%8B%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[Markdown笔记：如何画流程图 Flowchart.js 仅需几行代码即可在 Web 上完成流程图的构建。可以从文字表述中画出简单的 SVG 流程图，也可以画出彩色的图表。 1. 先来看一段入门案例流程图代码在 Markdown 编辑中应该是下面这样的(由于渲染的问题，请把,,,改成三个点号)1234567891011,,,flowst=&gt;start: Starte=&gt;end: Endop1=&gt;operation: My Operationsub1=&gt;subroutine: My Subroutinecond=&gt;condition: Yes or No?io=&gt;inputoutput: catch something...st-&gt;op1-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op1,,, 输出结果如图所示: 在markdown语法中，流程图的画法和代码段类似，也就是说，流程图是写在两个,,,之间的。比如说php代码，会是这样一种格式: ,,,php代码段,,, 那么流程图就是这样的: ,,,flow代码段,,,` 2. 语法流程图的语法大体分为两部分: 前面部分用来定义流程图元素； 后面部分用来连接流程图元素，指定流程图的执行走向。 2.1 定义元素阶段的语法1tag=&gt;type: content:&gt;url 上例中下面部分代码都是定义元素部分123456st=&gt;start: Starte=&gt;end: Endop1=&gt;operation: My Operationsub1=&gt;subroutine: My Subroutinecond=&gt;condition: Yes or No?io=&gt;inputoutput: catch something... 说明： tag 是流程图中的标签，在第二段连接元素时会用到。名称可以任意，一般为流程的英文缩写和数字的组合。 type 用来确定标签的类型，=&gt;后面表示类型。由于标签的名称可以任意指定，所以要依赖type来确定标签的类型。 标签有6种类型：start end operation subroutine condition inputoutput。 content 是流程图文本框中的描述内容，: 后面表示内容，中英文均可。特别注意，冒号与文本之间一定要有个空格。 url是一个连接，与框框中的文本相绑定，:&gt;后面就是对应的 url 链接，点击文本时可以通过链接跳转到 url 指定页面。 开始1st=&gt;start: 开始 操作1op1=&gt;operation: 操作、执行说明 条件1cond=&gt;condition: 确认？ 结束1e=&gt;end: 结束 URL（貌似 SF 的编辑器不支持）1e=&gt;点击本结束跳转:&gt;http://https://segmentfault.com/blog/ingood 2.2 连接流程图元素的语法示例代码后面部分 st-&gt;op1-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op1 连接流程图元素阶段的语法就简单多了，直接用-&gt;来连接两个元素，几点说明如下：说明： 使用 -&gt; 来连接两个元素 对于condition类型，有yes和no两个分支，如示例中的cond(yes)和cond(no) 每个元素可以制定分支走向，默认向下，也可以用right指向右边，如示例中sub1(right)。 转载声明：本文转载自:https://segmentfault.com/a/1190000006247465?utm_source=tag-newest]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
      <tags>
        <tag>Markdown笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析二]]></title>
    <url>%2F2019%2F05%2F28%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[Tracking 线程分析1. Tracking流程图 2. Tracking 成员变量说明2.1 Tracking 状态枚举123456789enum eTrackingState&#123; SYSTEM_NOT_READY=-1, NO_IMAGES_YET=0, NOT_INITIALIZED=1, OK=2, LOST=3 &#125;;eTrackingState mState;eTrackingState mLastProcessedState; 2.2 Tracking 状态枚举 mSensor12// Input sensor:MONOCULAR, STEREO, RGBD int mSensor; mSeneor 传感器类型 2.3 当前帧和当前帧灰度图 mCurrentFrame,mImGray123// Current Frame Frame mCurrentFrame; cv::Mat mImGray; mSeneor 传感器类型 2.4 初始化的时候两帧图像之间的相关变量1234567// Initialization Variables (Monocular)// 初始化时前两帧相关变量std::vector&lt;int&gt; mvIniLastMatches;std::vector&lt;int&gt; mvIniMatches;// 跟踪初始化时前两帧之间的匹配std::vector&lt;cv::Point2f&gt; mvbPrevMatched;std::vector&lt;cv::Point3f&gt; mvIniP3D;Frame mInitialFrame; mSeneor 传感器类型 2.5 Tracking 结果关键帧列表和相对关键帧的位姿列表123456// Lists used to recover the full camera trajectory at the end of the execution.// Basically we store the reference keyframe for each frame and its relative transformationlist&lt;cv::Mat&gt; mlRelativeFramePoses;list&lt;KeyFrame*&gt; mlpReferences;list&lt;double&gt; mlFrameTimes;list&lt;bool&gt; mlbLost; mSeneor 传感器类型 2.6 是否开启地图变量 mbOnlyTracking12// True if local mapping is deactivated and we are performing only localizationbool mbOnlyTracking; mSeneor 传感器类型 2.7 只定位时0地图点是否VO变量12345// In case of performing only localization, this flag is true when there are no matches to// points in the map. Still tracking will continue if there are enough matches with temporal points.// In that case we are doing visual odometry. The system will try to do relocalization to recover// "zero-drift" localization to the map.bool mbVO; mSeneor 传感器类型 2.8 另外两个线程的指针 mpLocalMapper，mpLoopClosing123//Other Thread PointersLocalMapping* mpLocalMapper;LoopClosing* mpLoopClosing; mSeneor 传感器类型 2.9 ORB特征提取器1234567//ORB// orb特征提取器，不管单目还是双目，mpORBextractorLeft都要用到// 如果是双目，则要用到mpORBextractorRight// 如果是单目，在初始化的时候使用mpIniORBextractor而不是mpORBextractorLeft，// mpIniORBextractor属性中提取的特征点个数是mpORBextractorLeft的两倍ORBextractor* mpORBextractorLeft, *mpORBextractorRight;ORBextractor* mpIniORBextractor; mSeneor 传感器类型 2.10 Bow123//BoWORBVocabulary* mpORBVocabulary;KeyFrameDatabase* mpKeyFrameDB; mSeneor 传感器类型 2.11 単目初始器 mpInitializer123// Initalization (only for monocular)// 单目初始器Initializer* mpInitializer; mSeneor 传感器类型 2.12 局部地图1234//Local Map KeyFrame* mpReferenceKF;// 当前关键帧就是参考帧 std::vector&lt;KeyFrame*&gt; mvpLocalKeyFrames; std::vector&lt;MapPoint*&gt; mvpLocalMapPoints; mSeneor 传感器类型 2.13 SLAM 系统指针1System* mpSystem; mSeneor 传感器类型 2.14 显示相关1234//Drawers Viewer* mpViewer; FrameDrawer* mpFrameDrawer; MapDrawer* mpMapDrawer; mSeneor 传感器类型 2.15 系统地图 mpMap12//Map Map* mpMap; mSeneor 传感器类型 2.16 相机参数1234//Calibration matrix cv::Mat mK; cv::Mat mDistCoef; float mbf; mSeneor 传感器类型 2.17 新关键帧插入规则123//New KeyFrame rules (according to fps)int mMinFrames;int mMaxFrames; mSeneor 传感器类型 2.18 深度截断值1234// Threshold close/far points // Points seen as close by the stereo/RGBD sensor are considered reliable // and inserted from just one frame. Far points requiere a match in two keyframes. float mThDepth; mSeneor 传感器类型 2.19 深度图因子12// For RGB-D inputs only. For some datasets (e.g. TUM) the depthmap values are scaled.float mDepthMapFactor;; mSeneor 传感器类型 2.20 当前帧有多少特征点成功匹配12//Current matches in frame int mnMatchesInliers; mSeneor 传感器类型 2.21 上一关键帧、上一帧和重定位信息12345//Last Frame, KeyFrame and Relocalisation Info KeyFrame* mpLastKeyFrame; Frame mLastFrame; unsigned int mnLastKeyFrameId; unsigned int mnLastRelocFrameId; mSeneor 传感器类型 2.22 匀速模型匀速变换矩阵12//Motion Model cv::Mat mVelocity; mSeneor 传感器类型 2.23 相机RGB信息12//Color order (true RGB, false BGR, ignored if grayscale)bool mbRGB; mSeneor 传感器类型 2.24 临时地图点列表1list&lt;MapPoint*&gt; mlpTemporalPoints; mSeneor 传感器类型 共47个成员变量 3. Tracking 类成员函数3.1 构造函数 Tracking()12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 执行完构造函数后mState(NO_IMAGES_YET), mSensor(sensor), mbOnlyTracking(false), mbVO(false), mpORBVocabulary(pVoc),mpKeyFrameDB(pKFDB), mpInitializer(static_cast(NULL)), mpSystem(pSys), mpViewer(NULL),mpFrameDrawer(pFrameDrawer), mpMapDrawer(pMapDrawer), mpMap(pMap), mnLastRelocFrameId(0)等13个变量获得初始值。mK，mDistCoef，mbf，mMinFrames，mMaxFrames，mbRGB，mpORBextractorLeft，mpORBextractorRight，mpIniORBextractor，mThDepth， mDepthMapFactor等12个变量获得具体值。 3.2 抓取图片函数 GrabImageMonocular()1234// Preprocess the input and call Track(). Extract features and performs stereo matching. cv::Mat GrabImageStereo(const cv::Mat &amp;imRectLeft,const cv::Mat &amp;imRectRight, const double &amp;timestamp); cv::Mat GrabImageRGBD(const cv::Mat &amp;imRGB,const cv::Mat &amp;imD, const double &amp;timestamp); cv::Mat GrabImageMonocular(const cv::Mat &amp;im, const double &amp;timestamp); 3.3 设置三个线程指针函数，关联三个线程123void SetLocalMapper(LocalMapping* pLocalMapper);void SetLoopClosing(LoopClosing* pLoopClosing);void SetViewer(Viewer* pViewer); 3.4 矫正相机 ChangeCalibration1234// Load new settings// The focal length should be similar or scale prediction will fail when projecting points// TODO: Modify MapPoint::PredictScale to take into account focal lenghtvoid ChangeCalibration(const string &amp;strSettingPath); 3.5 构造函数12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 3.6 InformOnlyTracking() 设置是否只跟踪12// Use this function if you have deactivated local mapping and you only want to localize the camera. void InformOnlyTracking(const bool &amp;flag); 3.7 Reset() 函数清除所有地图点1void Reset(); 3.8 Track()12// Main tracking function. It is independent of the input sensor.void Track(); 跟踪线程的主函数，独立于传感器类型 3.9 Track初始化函数12345// Map initialization for stereo and RGB-Dvoid StereoInitialization();// Map initialization for monocularvoid MonocularInitialization(); 3.10 构造函数12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 3.11 初始化単目地图1void CreateInitialMapMonocular(); 3.12 检查1void CheckReplacedInLastFrame(); 3.13 构造函数12Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Map* pMap, KeyFrameDatabase* pKFDB, const string &amp;strSettingPath, const int sensor); 3.14 通过上一参考关键帧来跟踪图像位姿 TrackReferenceKeyFrame()1bool TrackReferenceKeyFrame(); 步骤一：将当前帧的描述子转化为BoW向量123// Compute Bag of Words vector// 步骤1：将当前帧的描述子转化为BoW向量mCurrentFrame.ComputeBoW(); 步骤二：通过特征点的BoW加快当前帧与参考关键帧之间的特征点匹配12345678// We perform first an ORB matching with the reference keyframe // If enough matches are found we setup a PnP solver ORBmatcher matcher(0.7,true); vector&lt;MapPoint*&gt; vpMapPointMatches; // 步骤2：通过特征点的BoW加快当前帧与参考帧之间的特征点匹配 // 特征点的匹配关系由MapPoints进行维护 int nmatches = matcher.SearchByBoW(mpReferenceKF,mCurrentFrame,vpMapPointMatches); 步骤三：检查特征匹配个数是否达到要求12if(nmatches&lt;15) return false; 步骤四：将上一帧的位姿态作为当前帧位姿的初始值，这个时候的位姿都是相对于InitKeyFrame,初始关键帧，即世界坐标系123// 步骤3:将上一帧的位姿态作为当前帧位姿的初始值 mCurrentFrame.mvpMapPoints = vpMapPointMatches; mCurrentFrame.SetPose(mLastFrame.mTcw); // 用上一次的Tcw设置初值，在PoseOptimization可以收敛快一些 步骤五：通过通过优化3D-2D的重投影误差来获得位姿，优化的时候，3D点的坐标是相对于世界坐标系的，所以优化得到的位姿也是相对于世界坐标系的12// 步骤4:通过优化3D-2D的重投影误差来获得位姿 Optimizer::PoseOptimization(&amp;mCurrentFrame); 步骤六：剔除优化后的outlier匹配点（MapPoints）,注意最后一步要求该地图点至少被一个关键帧观察到过,才认为该点是成功匹配点，初始化的时候已经建立了初始的地图点观测信息以及帧之间的连接关系123456789101112131415161718192021// Discard outliers // 步骤5：剔除优化后的outlier匹配点（MapPoints） int nmatchesMap = 0; for(int i =0; i&lt;mCurrentFrame.N; i++) &#123; if(mCurrentFrame.mvpMapPoints[i]) &#123; if(mCurrentFrame.mvbOutlier[i]) &#123; MapPoint* pMP = mCurrentFrame.mvpMapPoints[i]; mCurrentFrame.mvpMapPoints[i]=static_cast&lt;MapPoint*&gt;(NULL); mCurrentFrame.mvbOutlier[i]=false; pMP-&gt;mbTrackInView = false; pMP-&gt;mnLastFrameSeen = mCurrentFrame.mnId; nmatches--; &#125; else if(mCurrentFrame.mvpMapPoints[i]-&gt;Observations()&gt;0) nmatchesMap++; &#125; &#125; 步骤七：返回匹配结果是否达到要求1return nmatchesMap&gt;=10; 3.15 更新上一帧1void UpdateLastFrame(); 3.16 通过上一帧来跟踪图像位姿，利用匀速模型1234567891011/** * @brief 根据匀速度模型对上一帧的MapPoints进行跟踪 * * 1. 非单目情况，需要对上一帧产生一些新的MapPoints（临时） * 2. 将上一帧的MapPoints投影到当前帧的图像平面上，在投影的位置进行区域匹配 * 3. 根据匹配对估计当前帧的姿态 * 4. 根据姿态剔除误匹配 * @return 如果匹配数大于10，返回true * @see V-B Initial Pose Estimation From Previous Frame */bool TrackWithMotionModel(); 3.17 跟踪丢失重定位1bool Relocalization(); 3.18 更新局部地图信息，更新局部地图，更新局部3D点，更新局部地图关键帧123void UpdateLocalMap(); // 先调用UpdateLocalKeyFrames()，再调用UpdateLocalPoints()，最后设置mvpReferenceMapPointsvoid UpdateLocalPoints();void UpdateLocalKeyFrames(); 更新局部关键帧信息 UpdateLocalKeyFrames() 步骤1：遍历当前帧的MapPoints，记录所有能观测到当前帧MapPoints的关键帧123456789101112131415161718192021map&lt;KeyFrame*,int&gt; keyframeCounter;//先统计能看到当前帧mappoints的各个关键帧，map的键是关键帧，值是观察到该帧地图点的个数//遍历所有的地图点，看有哪些帧能够观测到该地图点，让该帧的值加一for(int i=0; i&lt;mCurrentFrame.N; i++)&#123; if(mCurrentFrame.mvpMapPoints[i]) &#123; MapPoint* pMP = mCurrentFrame.mvpMapPoints[i]; if(!pMP-&gt;isBad()) &#123; // 能观测到当前帧MapPoints的关键帧 const map&lt;KeyFrame*,size_t&gt; observations = pMP-&gt;GetObservations(); for(map&lt;KeyFrame*,size_t&gt;::const_iterator it=observations.begin(), itend=observations.end(); it!=itend; it++) keyframeCounter[it-&gt;first]++; &#125; else &#123; mCurrentFrame.mvpMapPoints[i]=NULL; &#125; &#125;&#125; 如果keyframeCounter为空，直接结束12if(keyframeCounter.empty()) return; 寻找共视程度最高的关键帧，即和该帧拥有最多共同地图点的关键帧pKFmax12345678910111213141516171819202122232425262728int max=0;KeyFrame* pKFmax= static_cast&lt;KeyFrame*&gt;(NULL);// 步骤2：更新局部关键帧（mvpLocalKeyFrames），添加局部关键帧有三个策略// 先清空局部关键帧mvpLocalKeyFrames.clear();mvpLocalKeyFrames.reserve(3*keyframeCounter.size());// All keyframes that observe a map point are included in the local map. Also check which keyframe shares most points// V-D K1: shares the map points with current frame// 策略1：能观测到当前帧MapPoints的关键帧作为局部关键帧for(map&lt;KeyFrame*,int&gt;::const_iterator it=keyframeCounter.begin(), itEnd=keyframeCounter.end(); it!=itEnd; it++)&#123; KeyFrame* pKF = it-&gt;first; if(pKF-&gt;isBad()) continue; if(it-&gt;second&gt;max) &#123; max=it-&gt;second; pKFmax=pKF; &#125; mvpLocalKeyFrames.push_back(it-&gt;first); // mnTrackReferenceForFrame防止重复添加局部关键帧 pKF-&gt;mnTrackReferenceForFrame = mCurrentFrame.mnId;&#125; 步骤2：与策略1得到的局部关键帧共视程度很高的关键帧作为局部关键帧(最佳共视的10帧，自己的子关键帧，自己的父关键帧) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// Include also some not-already-included keyframes that are neighbors to already-included keyframes// V-D K2: neighbors to K1 in the covisibility graph// 策略2：与策略1得到的局部关键帧共视程度很高的关键帧作为局部关键帧for(vector&lt;KeyFrame*&gt;::const_iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++)&#123; // Limit the number of keyframes if(mvpLocalKeyFrames.size()&gt;80) break; KeyFrame* pKF = *itKF; // 策略2.1:最佳共视的10帧 const vector&lt;KeyFrame*&gt; vNeighs = pKF-&gt;GetBestCovisibilityKeyFrames(10); for(vector&lt;KeyFrame*&gt;::const_iterator itNeighKF=vNeighs.begin(), itEndNeighKF=vNeighs.end(); itNeighKF!=itEndNeighKF; itNeighKF++) &#123; KeyFrame* pNeighKF = *itNeighKF; if(!pNeighKF-&gt;isBad()) &#123; // mnTrackReferenceForFrame防止重复添加局部关键帧 if(pNeighKF-&gt;mnTrackReferenceForFrame!=mCurrentFrame.mnId) &#123; mvpLocalKeyFrames.push_back(pNeighKF); pNeighKF-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; break; &#125; &#125; &#125; // 策略2.2:自己的子关键帧 const set&lt;KeyFrame*&gt; spChilds = pKF-&gt;GetChilds(); for(set&lt;KeyFrame*&gt;::const_iterator sit=spChilds.begin(), send=spChilds.end(); sit!=send; sit++) &#123; KeyFrame* pChildKF = *sit; if(!pChildKF-&gt;isBad()) &#123; if(pChildKF-&gt;mnTrackReferenceForFrame!=mCurrentFrame.mnId) &#123; mvpLocalKeyFrames.push_back(pChildKF); pChildKF-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; break; &#125; &#125; &#125; // 策略2.3:自己的父关键帧 KeyFrame* pParent = pKF-&gt;GetParent(); if(pParent) &#123; // mnTrackReferenceForFrame防止重复添加局部关键帧 if(pParent-&gt;mnTrackReferenceForFrame!=mCurrentFrame.mnId) &#123; mvpLocalKeyFrames.push_back(pParent); pParent-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; break; &#125; &#125;&#125; 步骤3：更新当前帧的参考关键帧，与自己共视程度最高的关键帧作为参考关键帧 12345if(pKFmax)&#123; mpReferenceKF = pKFmax; mCurrentFrame.mpReferenceKF = mpReferenceKF;&#125; 更新局部3D点信息 UpdateLocalPoints()根据已经更新的局部关键帧mvpLocalKeyFrames的MapPoints，更新mvpLocalMapPoints 步骤一：清空局部Tracking线程mptracker的MapPoints 12// 步骤1：清空局部MapPoints mvpLocalMapPoints.clear(); 步骤二：遍历mvpLocalKeyFrames，提取每一帧的地图点 12// 步骤2：遍历局部关键帧mvpLocalKeyFramesfor(vector&lt;KeyFrame*&gt;::const_iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++) 步骤三：提取每一帧的地图点 1const vector&lt;MapPoint*&gt; vpMPs = pKF-&gt;GetMapPointMatches(); 步骤四：将每一帧的地图点都加入mvpLocalMapPoints 123456789101112131415// 步骤2：将局部关键帧的MapPoints添加到mvpLocalMapPoints for(vector&lt;MapPoint*&gt;::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end(); itMP!=itEndMP; itMP++) &#123; MapPoint* pMP = *itMP; if(!pMP) continue; // mnTrackReferenceForFrame防止重复添加局部MapPoint if(pMP-&gt;mnTrackReferenceForFrame==mCurrentFrame.mnId) continue; if(!pMP-&gt;isBad()) &#123; mvpLocalMapPoints.push_back(pMP); pMP-&gt;mnTrackReferenceForFrame=mCurrentFrame.mnId; &#125; &#125; 3.19 跟踪局部地图1bool TrackLocalMap(); 3.20 寻找局部地图点1void SearchLocalPoints(); 3.21 判定是否需要插入关键帧1bool NeedNewKeyFrame(); 3.22 创建新的关键帧1void CreateNewKeyFrame(); 4. 实例化mpTracker在System构造函数中new一个Tracing对象指针mpTracker，方式如下所示：1234//Initialize the Tracking thread//(it will live in the main thread of execution, the one that called this constructor)mpTracker = new Tracking(this, mpVocabulary, mpFrameDrawer, mpMapDrawer, mpMap, mpKeyFrameDatabase, strSettingsFile, mSensor); 4.1 读取配置文件，构造相机内参 mK123456789101112131415cv::FileStorage fSettings(strSettingPath, cv::FileStorage::READ); float fx = fSettings["Camera.fx"]; float fy = fSettings["Camera.fy"]; float cx = fSettings["Camera.cx"]; float cy = fSettings["Camera.cy"]; // |fx 0 cx| // K = |0 fy cy| // |0 0 1 | cv::Mat K = cv::Mat::eye(3,3,CV_32F); K.at&lt;float&gt;(0,0) = fx; K.at&lt;float&gt;(1,1) = fy; K.at&lt;float&gt;(0,2) = cx; K.at&lt;float&gt;(1,2) = cy; K.copyTo(mK); 4.2 读取配置文件，构造相机矫正向量 mDistCoef1234567891011121314// 图像矫正系数// [k1 k2 p1 p2 k3]cv::Mat DistCoef(4,1,CV_32F);DistCoef.at&lt;float&gt;(0) = fSettings["Camera.k1"];DistCoef.at&lt;float&gt;(1) = fSettings["Camera.k2"];DistCoef.at&lt;float&gt;(2) = fSettings["Camera.p1"];DistCoef.at&lt;float&gt;(3) = fSettings["Camera.p2"];const float k3 = fSettings["Camera.k3"];if(k3!=0)&#123; DistCoef.resize(5); DistCoef.at&lt;float&gt;(4) = k3;&#125;DistCoef.copyTo(mDistCoef); 4.3 读取配置文件，构造相机RGB参数 mbRGB12345678// 1:RGB 0:BGR int nRGB = fSettings["Camera.RGB"]; mbRGB = nRGB; if(mbRGB) cout &lt;&lt; "- color order: RGB (ignored if grayscale)" &lt;&lt; endl; else cout &lt;&lt; "- color order: BGR (ignored if grayscale)" &lt;&lt; endl; 4.4 读取配置文件，加载ORB参数 nFeatures、fScaleFactor、nLevels、fIniThFAST、fMinThFAST1234567891011// Load ORB parameters// 每一帧提取的特征点数 1000int nFeatures = fSettings["ORBextractor.nFeatures"];// 图像建立金字塔时的变化尺度 1.2float fScaleFactor = fSettings["ORBextractor.scaleFactor"];// 尺度金字塔的层数 8int nLevels = fSettings["ORBextractor.nLevels"];// 提取fast特征点的默认阈值 20int fIniThFAST = fSettings["ORBextractor.iniThFAST"];// 如果默认阈值提取不出足够fast特征点，则使用最小阈值 8int fMinThFAST = fSettings["ORBextractor.minThFAST"]; 4.5 通过nFeatures、fScaleFactor、nLevels、fIniThFAST、fMinThFAST构造特征提取器12345678910// tracking过程都会用到mpORBextractorLeft作为特征点提取器mpORBextractorLeft = new ORBextractor(nFeatures,fScaleFactor,nLevels,fIniThFAST,fMinThFAST);// 如果是双目，tracking过程中还会用用到mpORBextractorRight作为右目特征点提取器if(sensor==System::STEREO) mpORBextractorRight = new ORBextractor(nFeatures,fScaleFactor,nLevels,fIniThFAST,fMinThFAST);// 在单目初始化的时候，会用mpIniORBextractor来作为特征点提取器if(sensor==System::MONOCULAR) mpIniORBextractor = new ORBextractor(2*nFeatures,fScaleFactor,nLevels,fIniThFAST,fMinThFAST); 4.6 读取配置文件，构造相机深度截断阈值和视差因子 mThDepth,mDepthMapFactor12345678910111213141516if(sensor==System::STEREO || sensor==System::RGBD)&#123; // 判断一个3D点远/近的阈值 mbf * 35 / fx mThDepth = mbf*(float)fSettings["ThDepth"]/fx; cout &lt;&lt; endl &lt;&lt; "Depth Threshold (Close/Far Points): " &lt;&lt; mThDepth &lt;&lt; endl;&#125;if(sensor==System::RGBD)&#123; // 深度相机disparity转化为depth时的因子 mDepthMapFactor = fSettings["DepthMapFactor"]; if(fabs(mDepthMapFactor)&lt;1e-5) mDepthMapFactor=1; else mDepthMapFactor = 1.0f/mDepthMapFactor;&#125; st=>start: 调用构造函数实例化mpTracker e=>end: 得到mCurrentFrame.mTcw op1=>operation: 循环SLAM.TrackMonocular(im,tframe)，对每一帧图像进行tracking op2=>operation: 调用mpTracker->GrabImageMonocular(im,timestamp),抓取每一帧图像 op3=>operation: 将RGB图转换为灰度图mImGray op4=>operation: 利用灰度图构造当前帧mCurrentFrame op5=>operation: 调用track()函数 st->op1->op2->op3->op4->op5->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-0-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-0", options);st=>start: 调用构造函数实例化 mpTracker e=>end: 得到mpTracker op1=>operation: 读取配置文件，构造相机内参 mK,相机矫正向量 mDistCoef ,相机RGB参数 mbRGB op2=>operation: 读取配置文件，加载ORB参数 nFeatures、fScaleFactor、nLevels、fIniThFAST、fMinThFAST op3=>operation: 构造特征提取器 mpORBextractorLeft、mpIniORBextractor op4=>operation: 读取配置文件, 构造相机深度截断阈值和视差因子 mThDepth、mDepthMapFactor st->op1->op2->op3->op4->e{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12} var code = document.getElementById("flowchart-1-code").value; var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value)); var diagram = flowchart.parse(code); diagram.drawSVG("flowchart-1", options);]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORB-SLAM2源码分析一]]></title>
    <url>%2F2019%2F05%2F28%2FORB-SLAM2%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%80%2F</url>
    <content type="text"><![CDATA[主函数说明 mono_kitty.cc1. main 入口函数，读取3个文件参数，初始化系统 strVocFile: 字典词包的路径 strSettingFile: 系统中装有一些如相机参数、view窗口的配置文件，格式为YAML strSequence: 数据集路径 2. LoadImages 函数 LoadImages(const string &amp;strPathToSequence, vector &amp;vstrImageFilenames, vector &amp;vTimestamps) 12345678910111213141516171819202122232425262728293031 void LoadImages(const string &amp;strPathToSequence, vector&lt;string&gt; &amp;vstrImageFilenames, vector&lt;double&gt; &amp;vTimestamps)&#123; ifstream fTimes; string strPathTimeFile = strPathToSequence + "/times.txt"; fTimes.open(strPathTimeFile.c_str()); while(!fTimes.eof()) &#123; string s; getline(fTimes,s); if(!s.empty()) &#123; stringstream ss; ss &lt;&lt; s; double t; ss &gt;&gt; t; vTimestamps.push_back(t); &#125; &#125; string strPrefixLeft = strPathToSequence + "/image_0/"; const int nTimes = vTimestamps.size(); vstrImageFilenames.resize(nTimes); for(int i=0; i&lt;nTimes; i++) &#123; stringstream ss; ss &lt;&lt; setfill('0') &lt;&lt; setw(6) &lt;&lt; i; vstrImageFilenames[i] = strPrefixLeft + ss.str() + ".png"; &#125;&#125; 加载数据集函数，函数执行完vstrImageFileNames是一个存有图片具体位置的vector，位置形式如xxx/xxx/000xxx.png，vTimestamps是存有图片时间戳的vector 3. 实例化 SLAM 系统加载图片路径完成后，需要实例化一个SLAM系统对象ORB_SLAM2::System SLAM(argv[1],argv[2],ORB_SLAM2::System::MONOCULAR,true); System.h 包含了7个类，分别是Viewer， FrameDrawer, Map, Tracking, LocalMapping, LoopClosing 的声明，和System 类的定义， 就像描述的那样，这些类组成了一个系统。 1234567class Viewer;class FrameDrawer;class Map;class Tracking;class LocalMapping;class LoopClosing;class System; 成员变量说明3.1 sensor 枚举12345enum eSensor&#123; MONOCULAR=0, STEREO=1, RGBD=2&#125;; 0,1,2 分别代表传感器的类型 3.2 System 构造函数System(const string &amp;strVocFile, const string &amp;strSettingsFile, const eSensor sensor, const bool bUseViewer = true); Monocular System 构造时，读入词包路径，YAML配置文件，设置eSensor类型为Monocular，并启用Viewer线程 3.3 Tracking 函数123456789101112131415// Proccess the given stereo frame. Images must be synchronized and rectified.// Input images: RGB (CV_8UC3) or grayscale (CV_8U). RGB is converted to grayscale.// Returns the camera pose (empty if tracking fails).cv::Mat TrackStereo(const cv::Mat &amp;imLeft, const cv::Mat &amp;imRight, const double &amp;timestamp);// Process the given rgbd frame. Depthmap must be registered to the RGB frame.// Input image: RGB (CV_8UC3) or grayscale (CV_8U). RGB is converted to grayscale.// Input depthmap: Float (CV_32F).// Returns the camera pose (empty if tracking fails).cv::Mat TrackRGBD(const cv::Mat &amp;im, const cv::Mat &amp;depthmap, const double &amp;timestamp);// Proccess the given monocular frame// Input images: RGB (CV_8UC3) or grayscale (CV_8U). RGB is converted to grayscale.// Returns the camera pose (empty if tracking fails).cv::Mat TrackMonocular(const cv::Mat &amp;im, const double &amp;timestamp); 针对不同传感器不同的Tracking。输入图像可以使rgb的也可以是grayscale的（最终读进去都会转化为grayscale的），函数返回值为camera的位姿pose。Tracking 过程是对针对每一幅图像，通过先初始化然后track和优化过程来估计相机误差。 3.4 定位模式函数1234// This stops local mapping thread (map building) and performs only camera tracking.void ActivateLocalizationMode();// This resumes local mapping thread and performs SLAM again.void DeactivateLocalizationMode(); 调用ActivateLocalizationMode()将终止mapping线程，开启定位模式，调用后者重启mapping线程。 3.5 重启与终止函数1234567// Reset the system (clear map)void Reset();// All threads will be requested to finish.// It waits until all threads have finished.// This function must be called before saving the trajectory.void Shutdown(); Reset()函数将清空map，Shutdown()函数可以终止所有线程，在保存相机轨迹之前需要调用此函数。 3.6 SaveTrajectory 函数123456789101112131415161718192021// Save camera trajectory in the TUM RGB-D dataset format.// Only for stereo and RGB-D. This method does not work for monocular.// Call first Shutdown()// See format details at: http://vision.in.tum.de/data/datasets/rgbd-datasetvoid SaveTrajectoryTUM(const string &amp;filename);// Save keyframe poses in the TUM RGB-D dataset format.// This method works for all sensor input.// Call first Shutdown()// See format details at: http://vision.in.tum.de/data/datasets/rgbd-datasetvoid SaveKeyFrameTrajectoryTUM(const string &amp;filename);// Save camera trajectory in the KITTI dataset format.// Only for stereo and RGB-D. This method does not work for monocular.// Call first Shutdown()// See format details at: http://www.cvlibs.net/datasets/kitti/eval_odometry.phpvoid SaveTrajectoryKITTI(const string &amp;filename);// TODO: Save/Load functions// SaveMap(const string &amp;filename);// LoadMap(const string &amp;filename); 把相机轨迹保存成相应数据集的格式，系统调用此函数时先shutdown SLAM系统，mono_kittti中save函数用的是SaveKeyFrameTrajectoryTUM，这个函数看起来像是只能用于TUM数据集，但三种传感器均适合。 4 System private 成员变量说明4.1 eSensor12// Input sensoreSensor mSensor; 输入的传感器类型 4.2 mpVocabulary12// ORB vocabulary used for place recognition and feature matching.ORBVocabulary* mpVocabulary; 用于位置识别和特征匹配的系统词包 4.3 mpKeyFrameDatabase12// KeyFrame database for place recognition (relocalization and loop detection).KeyFrameDatabase* mpKeyFrameDatabase; 用于位置识别，重定位，回环检测的关键帧数据集 4.4 mpMap12// Map structure that stores the pointers to all KeyFrames and MapPoints.Map* mpMap; 存储系统关键帧的指针和地图点的指针 4.5 mpTracker1234// Tracker. It receives a frame and computes the associated camera pose.// It also decides when to insert a new keyframe, create some new MapPoints and// performs relocalization if tracking fails.Tracking* mpTracker; Tracker 接受一帧图像并计算相机位姿，决定什么时候需要插入关键帧，创建地图点并且执行重定位如果跟踪失败。 4.6 mpLocalMapper12// Local Mapper. It manages the local map and performs local bundle adjustment.LocalMapping* mpLocalMapper; 局部地图管理器，mpLocalMapper，管理局部地图并进行局部BA。 4.7 mpLoopCloser123// Loop Closer. It searches loops with every new keyframe. If there is a loop it performs// a pose graph optimization and full bundle adjustment (in a new thread) afterwards.LoopClosing* mpLoopCloser; 回环检测器，每次获取关键帧后都会进行回环检测，如果存在回环的话就执行位姿图的优化并且进行全局BA优化 4.8 mpViewer,mpFrameDrawer,mpMapDrawer12345// The viewer draws the map and the current camera pose. It uses Pangolin.Viewer* mpViewer;FrameDrawer* mpFrameDrawer;MapDrawer* mpMapDrawer; 视图显示 4.9 系统线程12345// System threads: Local Mapping, Loop Closing, Viewer.// The Tracking thread "lives" in the main execution thread that creates the System object.std::thread* mptLocalMapping;std::thread* mptLoopClosing;std::thread* mptViewer; 4.10 Reset flag123// Reset flagstd::mutex mMutexReset;bool mbReset; 4.11 Change mode flags1234// Change mode flagsstd::mutex mMutexMode;bool mbActivateLocalizationMode;bool mbDeactivateLocalizationMode; 5. 实例化SLAM-System构造函数123System::System(const string &amp;strVocFile, const string &amp;strSettingsFile, const eSensor sensor, const bool bUseViewer):mSensor(sensor),mbReset(false),mbActivateLocalizationMode(false), mbDeactivateLocalizationMode(false) System 构造函数用于实例化一个SALM系统，开启相机跟踪(Tracking)，局部建图(Local Mapping)，回环检测(Loop Closing)，和可视化界面(Viewer)的线程。 5.1 初始形参传递1234mSensor(sensor),mbReset(false),mbActivateLocalizationMode(false),mbDeactivateLocalizationMode(false) sensor是传进来的形参，是前面枚举体中三种传感器的一个，这里为MONOCULAR，它传递给了mSensor，这是一个System类的隐含成员变量，两种变量类型一样。mpViewer是System类的隐含成员变量，Viewer类指针，这里赋空。mbReset，mbActivateLocalizationMode，mbDeactivateLocalizationMode均为bool型，赋false。 5.2 初始化数据库 1 初始化词包 mpVocabulary 123456789101112131415161718//Load ORB Vocabulary cout &lt;&lt; endl &lt;&lt; "Loading ORB Vocabulary. This could take a while..." &lt;&lt; endl; mpVocabulary = new ORBVocabulary(); bool bVocLoad = false; // chose loading method based on file extension if (has_suffix(strVocFile, ".txt")) bVocLoad = mpVocabulary-&gt;loadFromTextFile(strVocFile); else if(has_suffix(strVocFile, ".bin")) bVocLoad = mpVocabulary-&gt;loadFromBinaryFile(strVocFile); else bVocLoad = false; if(!bVocLoad) &#123; cerr &lt;&lt; "Wrong path to vocabulary. " &lt;&lt; endl; cerr &lt;&lt; "Failed to open at: " &lt;&lt; strVocFile &lt;&lt; endl; exit(-1); &#125; cout &lt;&lt; "Vocabulary loaded!" &lt;&lt; endl &lt;&lt; endl; 2 用词包数据库来初始化关键帧数据库（用于重定位和回环检测）mpKeyFrameDatabase 12//Create KeyFrame Database mpKeyFrameDatabase = new KeyFrameDatabase(*mpVocabulary); 3 初始化一个Map类对象 ，该类用于存储指向所有关键帧和地图点的指针 mpMap 12//Create the Map mpMap = new Map(); 4 初始化画图工具，用于可视化 mpFrameDrawer、mpMapDrawer 123//Create Drawers. These are used by the Viewer mpFrameDrawer = new FrameDrawer(mpMap); mpMapDrawer = new MapDrawer(mpMap, strSettingsFile); 5 初始化Tracking线程，主线程，使用this指针（只初始化不启动，启动在main函数里TrackMonocular()启动） 1234//Initialize the Tracking thread//(it will live in the main thread of execution, the one that called this constructor) mpTracker = new Tracking(this, mpVocabulary, mpFrameDrawer, mpMapDrawer, mpMap, mpKeyFrameDatabase, strSettingsFile, mSensor); 6 初始化Local Mapping线程并启动（这里mSensor传入MONOCULAR）mpLocalMapper 123//Initialize the Local Mapping thread and launch mpLocalMapper = new LocalMapping(mpMap, mSensor==MONOCULAR); mptLocalMapping = new thread(&amp;ORB_SLAM2::LocalMapping::Run,mpLocalMapper); 7 初始化Loop Closing线程并启动（这里mSensor传入的不是MONOCULAR）mptLoopClosing 123//Initialize the Loop Closing thread and launchmpLoopCloser = new LoopClosing(mpMap, mpKeyFrameDatabase, mpVocabulary, mSensor!=MONOCULAR);mptLoopClosing = new thread(&amp;ORB_SLAM2::LoopClosing::Run, mpLoopCloser); 8 初始化Viewer线程并启动，也使用了this指针；给Tracking线程设置Viewer 123456//Initialize the Viewer thread and launch mpViewer = new Viewer(this, mpFrameDrawer,mpMapDrawer,mpTracker,strSettingsFile); if(bUseViewer) mptViewer = new thread(&amp;Viewer::Run, mpViewer); mpTracker-&gt;SetViewer(mpViewer); 9 mpTracker，mpLocalMapper，mptLoopClosing三个线程每两个线程之间设置指针相互关联 123456789//Set pointers between threads mpTracker-&gt;SetLocalMapper(mpLocalMapper); mpTracker-&gt;SetLoopClosing(mpLoopCloser); mpLocalMapper-&gt;SetTracker(mpTracker); mpLocalMapper-&gt;SetLoopCloser(mpLoopCloser); mpLoopCloser-&gt;SetTracker(mpTracker); mpLoopCloser-&gt;SetLocalMapper(mpLocalMapper); 6. 循环Tracking12345678910111213141516171819202122232425262728293031323334353637383940414243444546 // Main loop cv::Mat im; for(int ni=0; ni&lt;nImages; ni++) &#123; // Read image from file im = cv::imread(vstrImageFilenames[ni],CV_LOAD_IMAGE_UNCHANGED); double tframe = vTimestamps[ni]; if(im.empty()) &#123; cerr &lt;&lt; endl &lt;&lt; "Failed to load image at: " &lt;&lt; vstrImageFilenames[ni] &lt;&lt; endl; return 1; &#125;#ifdef COMPILEDWITHC11 std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();#else std::chrono::monotonic_clock::time_point t1 = std::chrono::monotonic_clock::now();#endif // Pass the image to the SLAM system SLAM.TrackMonocular(im,tframe);#ifdef COMPILEDWITHC11 std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();#else std::chrono::monotonic_clock::time_point t2 = std::chrono::monotonic_clock::now();#endif double ttrack= std::chrono::duration_cast&lt;std::chrono::duration&lt;double&gt; &gt;(t2 - t1).count(); vTimesTrack[ni]=ttrack; // Wait to load the next frame double T=0; if(ni&lt;nImages-1) T = vTimestamps[ni+1]-tframe; else if(ni&gt;0) T = tframe-vTimestamps[ni-1]; if(ttrack&lt;T) this_thread::sleep_for(std::chrono::microseconds((int)((T-ttrack)*1e6))); &#125; // Stop all threads SLAM.Shutdown(); 上述分为两步：读图、Tracking，其中有一部分代码（注释 //Wait to load the next frame 后）目的是为了模拟真实时间状况，如果tracking过快，则下一帧可能还没来，所以要“睡” T-ttrack 秒等待装载下一帧图片。每次tracking只处理一帧图片。]]></content>
      <categories>
        <category>ORB-SLAM2</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git简单攻略]]></title>
    <url>%2F2019%2F05%2F24%2Fgit%E7%AE%80%E5%8D%95%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[git全局用户申明12git config --global user.name "Your Name"git config --global user.email "email@example.com" 创建管理库1git init 添加文件1git add reamde.md 提交文件到仓库1git commit -m "message" 为什么git提交文件需要add和commit两步呢，因为commit可以一次提交很多次add不同的文件，比如123git add file1.txtgit add file2.txt file3.txtgit commit -m "add 3files" 查看仓库状态1git status 查看文件修改内容1git diff readme.txt 查看提交历史1git log 以便确定回退到哪个版本。 查看命令历史1git reflog 以便确定回到未来的哪个版本。 版本指针HEAD指向的版本就是当前的版本，HEAD^指向前一个版本，HEAD^^指向前前版本，HEAD~100指向第前100个版本。因此，git允许我们在历史之间穿梭。 版本穿梭1git reset --hard commit_id 丢弃工作区的修改1git checkout -- file 命令git checkout -- readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 从暂存区回到工作区1git reset HEAD readme.txt 如果你把文件git add到暂存区，但是还没有git commit到仓库，可以使用git reset HEAD file 将暂存区的修改撤销掉，重新放回到工作区。git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。 小结场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout — file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。 删除文件12git rm test.txtgit commit -m "remove test.txt" 一般情况下，你通常直接在文件管理器中把没用的文件删了，或者用rm命令删了：rm test.txt。这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了。 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit，现在，文件就从版本库中被删除了。 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本：1git checkout -- test.txt 小提示：先手动删除文件，然后使用git rm 和git add效果是一样的。注意：从来没有被添加到版本库就被删除的文件，是无法恢复的！ 添加远程仓库1git remote add origin git@github.com:hahaha/hahaha.git 把本地库的所有内容推送到远程库上1git push -u origin master 把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 提交仓库到到远程1git push origin master 从远程库克隆1git clone git@github.com:hahaha/gitskills.git 现在，假设我们从零开发，那么最好的方式是先创建远程库，然后，从远程库克隆。首先，登陆GitHub，创建一个新的仓库，名字叫gitskills：我们勾选Initialize this repository with a README，这样GitHub会自动为我们创建一个README.md文件。创建完毕后，可以看到README.md文件：现在，远程库已经准备好了，下一步是用命令git clone克隆一个本地库： 注意把Git库的地址换成你自己的，然后进入gitskills目录看看，已经有README.md文件了： 转载申明本文转载自廖雪峰的博客：[https://www.liaoxuefeng.com]]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EKF详解]]></title>
    <url>%2F2019%2F05%2F23%2FEKF%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. 高斯函数\begin{equation}p(x) = \det(2\pi\Sigma)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \}}\label{eq:Gaussion}\end{equation} 所有的高斯技术都共享了基本思想，即置信度用多元正态分布来表示。$x$的密度用两个参数来表示，均值$\mu$和协方差$\Sigma$，均值$\mu$是一个向量，它与状态$x$的维数相同。协方差是对称半正定的二次型。其维数等于状态$x$的维数的二次方。高斯滤波中的参数均值和方差称为矩参数，这是因为均值和方差是概率分布的一阶矩和二阶矩；正态分布的其他矩都是零。 2. 线性高斯系统KF是由Swerling（1950）和Kalman（1960）作为线性高斯系统中的预测和滤波技术而发明的，是用矩来定义的。KF用矩参数来表示置信度：在时刻$t$，置信度用均值$\mu_t$和方差$\Sigma_t$表示、如果除了贝叶斯滤波的马尔科夫假设以外，还具有如下的三个特性，则后验就是高斯的。 状态转移概率$p(x_t | u_t, x_{t-1})$必须是带有随机高斯噪声的参数的线性函数，可有下式表示:\begin{equation}x_t = A_tx_{t-1} + B_tu_t + \varepsilon_t\label{eq:motion}\end{equation}式中，$x_t$和$x_{t-1}$都是状态向量，它们都是$n$维列向量；$u_t$为时刻$t$的控制向量。式(2)中，$A_t$为$n \times n$的矩阵，$B_t$为$n \times m$的矩阵，$n$为状态向量$x_t$的维数，$m$为控制向量$u_t$的维数。式(2)中的随机变量\varepsilon_t是一个高斯随机向量，表示由状态转移引入的不确定性。其维数与状态向量维数相同，均值为0，方差用$R_t$表示。式(2)中的状态转移概率称为线性高斯，反映了它与带有附加高斯噪声的自变量呈线性关系。式(2)定义了状态转移概率$p(x_t | u_t, x_{t-1})$。这个概率可由公式(2)带入到多元正态分布的定义式(1)来得到。后验状态的均值由$A_tx_{t-1} + B_tu_t$给定，方差由$R_t$给定：\begin{equation}p(x_t | u_t, x_{t-1}) = \det(2\pi R_t)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(x_t-A_tx_{t-1} - B_tu_t)^TR_t^{-1}(x_t-A_tx_{t-1} - B_tu_t) \}}\label{eq:status}\end{equation} 观测概率$p(z_t | x_t)$也与带有高斯噪声的自变量呈线性关系：\begin{equation}z_t = C_tx_t + \delta _t\label{eq:project}\end{equation}式中，$C_t$为$k \times n$的矩阵，$k$为观测向量$z_t$的维数；向量$\delta _t$为观测噪声。$\delta _t$服从均值为0、方差为$Q_t$的多变量高斯分布。因此观测概率由下面的多元正态分布给定：\begin{equation}p(z_t | x_t) = \det(2\pi Q_t)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(z_t - C_tx_t)^TQ_t^{-1}(z_t-C_tx_t) \}}\label{eq:measure}\end{equation} 最后，初始置信度必须${\rm bel}(x_0)$必须是正态分布的。这里用$\mu_0$表示初始置信度的均值，用$\Sigma_0$表示协方差：\begin{equation}{\rm bel}(x_0) = p (x_0)= \det(2\pi \Sigma_0)^{- \frac {1}{2} }\exp{ \{ -\frac{1}{2}(x_0 - \mu_0)^T\Sigma_0^{-1}(x_0-\mu_0) \}}\label{eq:initial}\end{equation} 这三个假设足以保证后验${\rm bel}(x_t)$在任何时刻$t$总符合高斯分布。 3. KF算法(Kalman fliter algorithm)KF算法如图所示，KF表示均值为$\mu_t$、方差为$\Sigma_t$的状态量在时刻$t$的置信度{\rm bel}(x_t)。KF的输入是$t-1$时刻的置信度，其均值和方差分别用$\mu_{t-1}$和$\Sigma_{t-1}$表示。为了更新这些参数，KF需要控制向量$u_t$和测量向量$z_t$。输出的是时刻$t$的置信度，均值为$\mu_t$，方差为$\Sigma_t$。 Algorithm Kalman_filter($\mu_{t-1}$,$\Sigma_{t-1}$,$u_t$,$z_t$): 3.3 线性高斯系统]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>EKF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
</search>
